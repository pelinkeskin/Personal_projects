### [Kaggle Natural Language Processing with Disaster Tweets Competition](https://github.com/pelinkeskin/Personal_projects/tree/main/Kaggle_NLP_Disaster_Tweets)
This directory encompasses my entry for the [Kaggle Natural Language Processing with Disaster Tweets competition](https://www.kaggle.com/competitions/nlp-getting-started). The challenge revolved around constructing a machine learning model adept at distinguishing genuine disaster-related tweets from others. My primary focus throughout this competition was to enhance my proficiency in leveraging TensorFlow for training Deep Neural Networks specifically tailored for natural language processing in text classification. I accomplished a commendable accuracy surpassing 80% with my model. This notebook chronicles my meticulous dataset preprocessing, specifically tailored to suit Deep Neural Network (DNN) training. Initial experimentation involved employing Long Short-Term Memory (LSTM) and training the embedding layer within the network. However, due to the limited size of the training data, this approach led to overfitting. To address this issue, I integrated a 200-dimensional Twitter variant of Stanford's [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings. I explored diverse architectures and ultimately opted for a straightforward LSTM-CNN hybrid model. To further enhance the model's performance, I fine-tuned the hyperparameters of the LSTM-CNN hybrid using Bayesian Optimization with Keras Tuner. I'm delighted to share that this notebook is openly accessible on [Kaggle](https://www.kaggle.com/code/pelinkeskin/nlp-practice-tensorflow-lstm-cnn-glove), and I eagerly invite comments and feedback from the community.
