{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1414df7",
   "metadata": {},
   "source": [
    "# ConnectX: Deep RL with Stable_baseline3 and Gymnasium (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fed855",
   "metadata": {},
   "source": [
    "<div style=\" background-color: rgb(218, 255, 215);\n",
    "             border-style: outset;\n",
    "             padding: 10px;\n",
    "             margin: 10px;\n",
    "             text-indent: 10px;\n",
    "             text-align: justify;\n",
    "             letter-spacing: 1px;\n",
    "             line-height: 200%\">\n",
    "    <h3>&#x1F916; SUMMARY &#x1F916; </h3>\n",
    "    <ul >\n",
    "        <li style = \"margin: 10px\">I trained a vector agent in the Gymnasium environment using the Stable_baseline3 PPO algorithm.</li> \n",
    "        <li style = \"margin: 10px\">During training, I implemented learning rate decay and used a random agent for the first 500K time steps and negamax agent for the following 500K time steps, ultimately training my model for a total of 1M time steps. </li> \n",
    "        <li style = \"margin: 10px\">To generate the submission file, I passed the trained action network parameters to an equivalent pytorch deep network and created a function to predict action with it. </li> \n",
    "        <li style = \"margin: 10px\">As a beginner, I welcome your comments.&#x1F60A; But I kindly request that you provide constructive feedback in a gentle manner, as I am new to this and still learning. &#x1F605; And please rate my notebook if you find it helpful. &#x1F917; </li></ul></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4fa8a",
   "metadata": {},
   "source": [
    "## Importing Useful Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ebcb0",
   "metadata": {},
   "source": [
    "<div style=\"  font-size: 18px;  \n",
    "\n",
    "              border: 2px solid gray;\n",
    "              padding: 8px;\n",
    "              text-indent: 50px;\n",
    "              text-align: justify;\">\n",
    "<ul><li>I trained a vector agent in the Gymnasium environment using the Stable_baseline3 Actor critic network with PPO.</li>\n",
    "<li>During training, I implemented learning rate decay and used a random agent for the first 500K time steps and negamax for the following 500K time steps, ultimately training my model for a total of 1M time steps.</li>\n",
    "<li>To generate the submission file function, I passed the trained action network parameters to an equivalent pytorch deep network and submitted my results. </li>\n",
    "<li>As a beginner, I welcome your comments. Please rate my notebook if you find it helpfli. This is only the third deep RL model I have trained, so I appreciate your kindness.</li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a579a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import stable_baselines3 as sb3\n",
    "\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import os\n",
    "import torch as th\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from random import choice\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d3af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment lux_ai_s2 failed: No module named 'vec_noise'\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import evaluate, make, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f06260",
   "metadata": {},
   "source": [
    "## Checking Envorinment, converting to Gymnasium envorinment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40cf8f8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'remainingOverageTime': {'description': 'Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.',\n",
       "  'shared': False,\n",
       "  'type': 'number',\n",
       "  'minimum': 0,\n",
       "  'default': 60},\n",
       " 'step': {'description': 'Current step within the episode.',\n",
       "  'type': 'integer',\n",
       "  'shared': True,\n",
       "  'minimum': 0,\n",
       "  'default': 0},\n",
       " 'board': {'description': 'Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2',\n",
       "  'type': 'array',\n",
       "  'shared': True,\n",
       "  'default': []},\n",
       " 'mark': {'defaults': [1, 2],\n",
       "  'description': 'Which checkers are the agents.',\n",
       "  'enum': [1, 2]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'episodeSteps': {'description': 'Maximum number of steps in the episode.',\n",
       "  'type': 'integer',\n",
       "  'minimum': 1,\n",
       "  'default': 1000},\n",
       " 'actTimeout': {'description': 'Maximum runtime (seconds) to obtain an action from an agent.',\n",
       "  'type': 'number',\n",
       "  'minimum': 0,\n",
       "  'default': 2},\n",
       " 'runTimeout': {'description': 'Maximum runtime (seconds) of an episode (not necessarily DONE).',\n",
       "  'type': 'number',\n",
       "  'minimum': 0,\n",
       "  'default': 1200},\n",
       " 'columns': {'description': 'The number of columns on the board',\n",
       "  'type': 'integer',\n",
       "  'default': 7,\n",
       "  'minimum': 1},\n",
       " 'rows': {'description': 'The number of rows on the board',\n",
       "  'type': 'integer',\n",
       "  'default': 6,\n",
       "  'minimum': 1},\n",
       " 'inarow': {'description': 'The number of checkers in a row required to win.',\n",
       "  'type': 'integer',\n",
       "  'default': 4,\n",
       "  'minimum': 1},\n",
       " 'agentTimeout': {'description': 'Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.',\n",
       "  'type': 'number',\n",
       "  'minimum': 0,\n",
       "  'default': 60},\n",
       " 'timeout': {'description': 'Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.',\n",
       "  'type': 'integer',\n",
       "  'default': 2,\n",
       "  'minimum': 0}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'Column to drop a checker onto the board.',\n",
       " 'type': 'integer',\n",
       " 'minimum': 0,\n",
       " 'default': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Exploring the environment\n",
    "env = make(\"connectx\",debug=True)\n",
    "\n",
    "display(env.specification.observation)\n",
    "print('*'*66)\n",
    "display(env.specification.configuration)\n",
    "print('*'*66)\n",
    "display(env.specification.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee57e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random negamax\n"
     ]
    }
   ],
   "source": [
    "print(*env.agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13906ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: true,\n",
       "  &quot;playing&quot;: true,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: true,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;7c056d3a-1fbd-11ee-95da-7c214ac9f68a&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;columns&quot;: 7,\n",
       "      &quot;rows&quot;: 6,\n",
       "      &quot;inarow&quot;: 4,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 6,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 1,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 2,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 6,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 3,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 4,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 6,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 5,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 6,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 6,\n",
       "          &quot;reward&quot;: 1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 7,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: -1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      1,\n",
       "      -1\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;DONE&quot;,\n",
       "      &quot;DONE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.147425,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1.2e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.143307,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.113326,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 6e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ]\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"300\" height=\"300\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Playing an episode of the environment using random agent vs negamax\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.run([\"negamax\", \"random\"])\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd6b6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning envorinment stable-baselines3 compatible\n",
    "#The original code at here: https://www.kaggle.com/code/alexisbcook/deep-reinforcement-learning \n",
    "#which is compatible with gym but not with gymnasium altered to be compatible with gymnasium and stable_baselines3\n",
    "#changed reward function; imposed heavy penalty for invalid actions\n",
    "\n",
    "class ConnectFourGym(gym.Env):\n",
    "    ks_env = make(\"connectx\", debug=True)\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        \n",
    "        self.env = ConnectFourGym.ks_env.train([None, agent2])\n",
    "        self.rows = ConnectFourGym.ks_env.configuration.rows\n",
    "        self.columns = ConnectFourGym.ks_env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(1,self.rows,self.columns), dtype=int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-100, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    def reset(self,seed=0):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns) ,\\\n",
    "                ConnectFourGym.ks_env.reset()[0]['info']\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            terminated = truncated= done\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -100, True, {}\n",
    "            terminated = truncated= done\n",
    "        return np.array(self.obs['board']).reshape(1,self.rows,self.columns), reward, terminated,truncated, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684b6ffa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = ConnectFourGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ee3e1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pelin/Desktop/advancedml/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:30: UserWarning: It seems that your observation  is an image but its `dtype` is (int64) whereas it has to be `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\n",
      "  warnings.warn(\n",
      "/home/pelin/Desktop/advancedml/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:38: UserWarning: It seems that your observation space  is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
      "  warnings.warn(\n",
      "/home/pelin/Desktop/advancedml/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:51: UserWarning: The minimal resolution for an image is 36x36 for the default `CnnPolicy`. You might need to use a custom features extractor cf. https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be93e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 2, (1, 6, 7), int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c52776c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 0, 2, 2, 0, 1, 2],\n",
       "        [2, 0, 2, 0, 0, 0, 1],\n",
       "        [2, 1, 2, 0, 2, 1, 0],\n",
       "        [2, 2, 0, 2, 2, 0, 0],\n",
       "        [1, 0, 2, 1, 2, 1, 2],\n",
       "        [1, 1, 1, 1, 0, 1, 0]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e404d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7387e8b",
   "metadata": {},
   "source": [
    "## Training my vector agent with SB3  PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebff8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code ref: https://github.com/araffin/rl-baselines-zoo/blob/master/utils/utils.py#L225\n",
    "def linear_schedule(initial_value):\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: (float or str)\n",
    "    :return: (function)\n",
    "    \"\"\"\n",
    "    if isinstance(initial_value, str):\n",
    "        initial_value = float(initial_value)\n",
    "\n",
    "    def func(progress):\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0\n",
    "        :param progress: (float)\n",
    "        :return: (float)\n",
    "        \"\"\"\n",
    "        return progress * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceea2c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Creating a PPO agent using a designed vector as the observation.\n",
    "agent = sb3.PPO('MlpPolicy', \n",
    "                env, \n",
    "                n_steps=1536,\n",
    "                ent_coef = 0.001,\n",
    "                n_epochs = 8,\n",
    "                gae_lambda = 0.8365,\n",
    "                learning_rate = linear_schedule(3e-4), \n",
    "                batch_size = 512,\n",
    "                clip_range = 0.4,\n",
    "                policy_kwargs = {'log_std_init': -2, 'ortho_init':False},\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30fa9851",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=42, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=7, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Examining the agent network architectures.\n",
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02380037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an evaluation callback that is called every at regular intervals and renders the episode\n",
    "eval_log_path = './log_connectx'\n",
    "eval_env= Monitor(Monitor(ConnectFourGym()))\n",
    "\n",
    "eval_env=DummyVecEnv([lambda:eval_env])\n",
    "\n",
    "eval_callback = EvalCallback(eval_env , \n",
    "                              best_model_save_path=eval_log_path ,\n",
    "                              log_path=eval_log_path , \n",
    "                              eval_freq=1000,\n",
    "                              render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dacd570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_extractor.policy_net.0.weight 2688\n",
      "mlp_extractor.policy_net.0.bias 64\n",
      "mlp_extractor.policy_net.2.weight 4096\n",
      "mlp_extractor.policy_net.2.bias 64\n",
      "mlp_extractor.value_net.0.weight 2688\n",
      "mlp_extractor.value_net.0.bias 64\n",
      "mlp_extractor.value_net.2.weight 4096\n",
      "mlp_extractor.value_net.2.bias 64\n",
      "action_net.weight 448\n",
      "action_net.bias 7\n",
      "value_net.weight 64\n",
      "value_net.bias 1\n",
      "Total number of trainable parameters: 14344\n"
     ]
    }
   ],
   "source": [
    "for key,p in agent.get_parameters()['policy'].items():\n",
    "    print(key,p.numel())\n",
    "print(f\"Total number of trainable parameters: {sum(p.numel()for ey,p in agent.get_parameters()['policy'].items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46ccc48b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=0.73 +/- 0.78\n",
      "Episode length: 6.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 0.733    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.8     |\n",
      "|    ep_rew_mean     | -25.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 112      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 1536     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-19.04 +/- 40.37\n",
      "Episode length: 7.60 +/- 3.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.6          |\n",
      "|    mean_reward          | -19          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072183833 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.00044      |\n",
      "|    learning_rate        | 0.000299     |\n",
      "|    loss                 | 335          |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    value_loss           | 709          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-19.02 +/- 40.39\n",
      "Episode length: 8.40 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.4      |\n",
      "|    mean_reward     | -19      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.1     |\n",
      "|    ep_rew_mean     | -21.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 110      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 3072     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=0.31 +/- 0.98\n",
      "Episode length: 5.80 +/- 1.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 0.314        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022810136 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.00803      |\n",
      "|    learning_rate        | 0.000298     |\n",
      "|    loss                 | 318          |\n",
      "|    n_updates            | 16           |\n",
      "|    policy_gradient_loss | -0.0055      |\n",
      "|    value_loss           | 656          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.3     |\n",
      "|    ep_rew_mean     | -20.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 110      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 4608     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=0.77 +/- 0.78\n",
      "Episode length: 8.20 +/- 1.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8.2          |\n",
      "|    mean_reward          | 0.771        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022770008 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.0164       |\n",
      "|    learning_rate        | 0.000297     |\n",
      "|    loss                 | 271          |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.00583     |\n",
      "|    value_loss           | 584          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=0.76 +/- 0.78\n",
      "Episode length: 7.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 0.757    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.3     |\n",
      "|    ep_rew_mean     | -15.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 100      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-19.01 +/- 40.35\n",
      "Episode length: 8.80 +/- 2.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8.8          |\n",
      "|    mean_reward          | -19          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011524515 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.93        |\n",
      "|    explained_variance   | 0.0248       |\n",
      "|    learning_rate        | 0.000296     |\n",
      "|    loss                 | 238          |\n",
      "|    n_updates            | 32           |\n",
      "|    policy_gradient_loss | -0.00375     |\n",
      "|    value_loss           | 554          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.5     |\n",
      "|    ep_rew_mean     | -16.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 96       |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 7680     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=1.12 +/- 0.03\n",
      "Episode length: 6.20 +/- 1.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000991817 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.0257      |\n",
      "|    learning_rate        | 0.000295    |\n",
      "|    loss                 | 202         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00411    |\n",
      "|    value_loss           | 470         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-19.42 +/- 40.21\n",
      "Episode length: 8.40 +/- 2.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.4      |\n",
      "|    mean_reward     | -19.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10       |\n",
      "|    ep_rew_mean     | -17.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 92       |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 9216     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-0.06 +/- 0.95\n",
      "Episode length: 7.00 +/- 1.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | -0.0571     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001475826 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.0307      |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 213         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00524    |\n",
      "|    value_loss           | 520         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.4     |\n",
      "|    ep_rew_mean     | -21.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 90       |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 10752    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=1.13 +/- 0.07\n",
      "Episode length: 6.40 +/- 3.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.4         |\n",
      "|    mean_reward          | 1.13        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007513189 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0.0254      |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 253         |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 531         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=0.74 +/- 0.81\n",
      "Episode length: 6.80 +/- 3.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 0.738    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.3     |\n",
      "|    ep_rew_mean     | -15.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 88       |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.20 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052785464 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.92        |\n",
      "|    explained_variance   | 0.023        |\n",
      "|    learning_rate        | 0.000293     |\n",
      "|    loss                 | 223          |\n",
      "|    n_updates            | 64           |\n",
      "|    policy_gradient_loss | -0.00985     |\n",
      "|    value_loss           | 404          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.3     |\n",
      "|    ep_rew_mean     | -17.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 89       |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 13824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-19.40 +/- 40.15\n",
      "Episode length: 9.20 +/- 2.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.2          |\n",
      "|    mean_reward          | -19.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020644062 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.91        |\n",
      "|    explained_variance   | 0.0273       |\n",
      "|    learning_rate        | 0.000292     |\n",
      "|    loss                 | 218          |\n",
      "|    n_updates            | 72           |\n",
      "|    policy_gradient_loss | -0.00601     |\n",
      "|    value_loss           | 499          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=0.35 +/- 1.02\n",
      "Episode length: 7.40 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 0.352    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.3     |\n",
      "|    ep_rew_mean     | -12.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 95       |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 160      |\n",
      "|    total_timesteps | 15360    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-19.03 +/- 40.33\n",
      "Episode length: 8.20 +/- 3.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.2         |\n",
      "|    mean_reward          | -19         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007565055 |\n",
      "|    clip_fraction        | 0.00122     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 0.0209      |\n",
      "|    learning_rate        | 0.000291    |\n",
      "|    loss                 | 208         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00843    |\n",
      "|    value_loss           | 416         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.7      |\n",
      "|    ep_rew_mean     | -13.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 100      |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 16896    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-19.81 +/- 40.00\n",
      "Episode length: 9.00 +/- 1.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | -19.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007830113 |\n",
      "|    clip_fraction        | 0.000488    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.0169      |\n",
      "|    learning_rate        | 0.00029     |\n",
      "|    loss                 | 161         |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 363         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=0.80 +/- 0.83\n",
      "Episode length: 9.20 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | 0.795    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.8      |\n",
      "|    ep_rew_mean     | -8.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=0.75 +/- 0.82\n",
      "Episode length: 7.40 +/- 3.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.4         |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019052973 |\n",
      "|    clip_fraction        | 0.0225      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 0.0128      |\n",
      "|    learning_rate        | 0.000289    |\n",
      "|    loss                 | 135         |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 278         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.94     |\n",
      "|    ep_rew_mean     | -6.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 103      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 19968    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=20000, episode_reward=-19.04 +/- 40.37\n",
      "Episode length: 7.60 +/- 2.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | -19         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029031536 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | 0.000485    |\n",
      "|    learning_rate        | 0.000288    |\n",
      "|    loss                 | 112         |\n",
      "|    n_updates            | 104         |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 211         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=1.13 +/- 0.03\n",
      "Episode length: 6.60 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.67     |\n",
      "|    ep_rew_mean     | -7.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 105      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 21504    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=0.74 +/- 0.81\n",
      "Episode length: 6.80 +/- 2.32\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.8        |\n",
      "|    mean_reward          | 0.738      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02047919 |\n",
      "|    clip_fraction        | 0.0146     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -1.84      |\n",
      "|    explained_variance   | 0.0158     |\n",
      "|    learning_rate        | 0.000287   |\n",
      "|    loss                 | 146        |\n",
      "|    n_updates            | 112        |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    value_loss           | 278        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=0.70 +/- 0.78\n",
      "Episode length: 5.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 0.7      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.32     |\n",
      "|    ep_rew_mean     | -4.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 108      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 23040    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=1.18 +/- 0.04\n",
      "Episode length: 8.40 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.4         |\n",
      "|    mean_reward          | 1.18        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022417977 |\n",
      "|    clip_fraction        | 0.0251      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | -0.00389    |\n",
      "|    learning_rate        | 0.000286    |\n",
      "|    loss                 | 89.1        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 157         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.59     |\n",
      "|    ep_rew_mean     | -6.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 109      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 223      |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=0.75 +/- 0.77\n",
      "Episode length: 7.40 +/- 2.06\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.4        |\n",
      "|    mean_reward          | 0.752      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01840169 |\n",
      "|    clip_fraction        | 0.01       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -1.81      |\n",
      "|    explained_variance   | 0.0214     |\n",
      "|    learning_rate        | 0.000285   |\n",
      "|    loss                 | 146        |\n",
      "|    n_updates            | 128        |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    value_loss           | 226        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.43     |\n",
      "|    ep_rew_mean     | -3.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 110      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 26112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-19.84 +/- 39.96\n",
      "Episode length: 7.60 +/- 2.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.6        |\n",
      "|    mean_reward          | -19.8      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 27000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02287135 |\n",
      "|    clip_fraction        | 0.0208     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -1.78      |\n",
      "|    explained_variance   | 0.0338     |\n",
      "|    learning_rate        | 0.000284   |\n",
      "|    loss                 | 82.9       |\n",
      "|    n_updates            | 136        |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    value_loss           | 137        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.17     |\n",
      "|    ep_rew_mean     | -3.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 111      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 248      |\n",
      "|    total_timesteps | 27648    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=0.70 +/- 0.77\n",
      "Episode length: 5.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027144903 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.0474      |\n",
      "|    learning_rate        | 0.000283    |\n",
      "|    loss                 | 88.1        |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.40 +/- 2.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.62     |\n",
      "|    ep_rew_mean     | -1.19    |\n",
      "| time/              |          |\n",
      "|    fps             | 111      |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 261      |\n",
      "|    total_timesteps | 29184    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=30000, episode_reward=1.15 +/- 0.05\n",
      "Episode length: 7.20 +/- 2.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.2         |\n",
      "|    mean_reward          | 1.15        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009582014 |\n",
      "|    clip_fraction        | 0.00334     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.0266      |\n",
      "|    learning_rate        | 0.000282    |\n",
      "|    loss                 | 30.1        |\n",
      "|    n_updates            | 152         |\n",
      "|    policy_gradient_loss | -0.00879    |\n",
      "|    value_loss           | 80.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.8      |\n",
      "|    ep_rew_mean     | -9.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 114      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 268      |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-19.02 +/- 40.32\n",
      "Episode length: 8.60 +/- 3.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.6         |\n",
      "|    mean_reward          | -19         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008312802 |\n",
      "|    clip_fraction        | 0.000244    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.000282    |\n",
      "|    loss                 | 153         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00918    |\n",
      "|    value_loss           | 255         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=1.15 +/- 0.03\n",
      "Episode length: 7.20 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.84     |\n",
      "|    ep_rew_mean     | -3.27    |\n",
      "| time/              |          |\n",
      "|    fps             | 114      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 32256    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=1.14 +/- 0.03\n",
      "Episode length: 7.00 +/- 1.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | 1.14        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011407736 |\n",
      "|    clip_fraction        | 0.00195     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 0.0483      |\n",
      "|    learning_rate        | 0.000281    |\n",
      "|    loss                 | 71.6        |\n",
      "|    n_updates            | 168         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 135         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.3     |\n",
      "|    ep_rew_mean     | -5.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 115      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 33792    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-19.43 +/- 40.15\n",
      "Episode length: 8.20 +/- 2.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.2         |\n",
      "|    mean_reward          | -19.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025024181 |\n",
      "|    clip_fraction        | 0.0225      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 0.0783      |\n",
      "|    learning_rate        | 0.00028     |\n",
      "|    loss                 | 75.2        |\n",
      "|    n_updates            | 176         |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 182         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-19.04 +/- 40.35\n",
      "Episode length: 7.60 +/- 2.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | -19      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.31     |\n",
      "|    ep_rew_mean     | -1.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 117      |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 299      |\n",
      "|    total_timesteps | 35328    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=0.77 +/- 0.79\n",
      "Episode length: 8.20 +/- 1.72\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 8.2       |\n",
      "|    mean_reward          | 0.771     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 36000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0216335 |\n",
      "|    clip_fraction        | 0.0265    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -1.74     |\n",
      "|    explained_variance   | 0.0581    |\n",
      "|    learning_rate        | 0.000279  |\n",
      "|    loss                 | 52.4      |\n",
      "|    n_updates            | 184       |\n",
      "|    policy_gradient_loss | -0.0121   |\n",
      "|    value_loss           | 134       |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.94     |\n",
      "|    ep_rew_mean     | -2.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 118      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 312      |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-19.43 +/- 40.14\n",
      "Episode length: 8.00 +/- 3.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -19.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014556397 |\n",
      "|    clip_fraction        | 0.00521     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 0.0853      |\n",
      "|    learning_rate        | 0.000278    |\n",
      "|    loss                 | 39          |\n",
      "|    n_updates            | 192         |\n",
      "|    policy_gradient_loss | -0.00988    |\n",
      "|    value_loss           | 96.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=1.15 +/- 0.02\n",
      "Episode length: 7.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.91     |\n",
      "|    ep_rew_mean     | -2.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 118      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 325      |\n",
      "|    total_timesteps | 38400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=0.75 +/- 0.83\n",
      "Episode length: 7.20 +/- 1.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 0.748        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070404503 |\n",
      "|    clip_fraction        | 0.00057      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.74        |\n",
      "|    explained_variance   | 0.0473       |\n",
      "|    learning_rate        | 0.000277     |\n",
      "|    loss                 | 44.2         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00925     |\n",
      "|    value_loss           | 80           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.57     |\n",
      "|    ep_rew_mean     | -2.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 118      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 337      |\n",
      "|    total_timesteps | 39936    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=1.18 +/- 0.03\n",
      "Episode length: 8.60 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.6         |\n",
      "|    mean_reward          | 1.18        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016161198 |\n",
      "|    clip_fraction        | 0.0142      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.000276    |\n",
      "|    loss                 | 25          |\n",
      "|    n_updates            | 208         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 77.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41000, episode_reward=1.18 +/- 0.05\n",
      "Episode length: 8.60 +/- 2.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | 1.18     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.31     |\n",
      "|    ep_rew_mean     | -1.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 118      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 349      |\n",
      "|    total_timesteps | 41472    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=1.15 +/- 0.05\n",
      "Episode length: 7.20 +/- 2.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.2         |\n",
      "|    mean_reward          | 1.15        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007757383 |\n",
      "|    clip_fraction        | 0.000407    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.0682      |\n",
      "|    learning_rate        | 0.000275    |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.00851    |\n",
      "|    value_loss           | 80.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=0.75 +/- 0.76\n",
      "Episode length: 7.40 +/- 2.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 0.752    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.67     |\n",
      "|    ep_rew_mean     | -4.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 118      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=1.17 +/- 0.08\n",
      "Episode length: 8.20 +/- 3.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.2         |\n",
      "|    mean_reward          | 1.17        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027558496 |\n",
      "|    clip_fraction        | 0.0247      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.75       |\n",
      "|    explained_variance   | 0.0542      |\n",
      "|    learning_rate        | 0.000274    |\n",
      "|    loss                 | 39.6        |\n",
      "|    n_updates            | 224         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 98.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.34     |\n",
      "|    ep_rew_mean     | -1.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 118      |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 374      |\n",
      "|    total_timesteps | 44544    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=0.79 +/- 0.77\n",
      "Episode length: 9.00 +/- 2.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 45000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02611413 |\n",
      "|    clip_fraction        | 0.0301     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -1.76      |\n",
      "|    explained_variance   | 0.109      |\n",
      "|    learning_rate        | 0.000273   |\n",
      "|    loss                 | 44.2       |\n",
      "|    n_updates            | 232        |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    value_loss           | 95.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=1.16 +/- 0.05\n",
      "Episode length: 7.80 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 1.16     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.17     |\n",
      "|    ep_rew_mean     | -2.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 120      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 381      |\n",
      "|    total_timesteps | 46080    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=0.73 +/- 0.81\n",
      "Episode length: 6.40 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 0.729        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 47000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058335937 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.75        |\n",
      "|    explained_variance   | 0.0663       |\n",
      "|    learning_rate        | 0.000272     |\n",
      "|    loss                 | 37.6         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    value_loss           | 98.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.34     |\n",
      "|    ep_rew_mean     | -4.33    |\n",
      "| time/              |          |\n",
      "|    fps             | 122      |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 388      |\n",
      "|    total_timesteps | 47616    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=1.15 +/- 0.05\n",
      "Episode length: 7.40 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.4          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048761372 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.73        |\n",
      "|    explained_variance   | 0.101        |\n",
      "|    learning_rate        | 0.000271     |\n",
      "|    loss                 | 63           |\n",
      "|    n_updates            | 248          |\n",
      "|    policy_gradient_loss | -0.00768     |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=0.73 +/- 0.78\n",
      "Episode length: 6.60 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 0.733    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.4      |\n",
      "|    ep_rew_mean     | -2.37    |\n",
      "| time/              |          |\n",
      "|    fps             | 122      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 400      |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=1.14 +/- 0.03\n",
      "Episode length: 7.00 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | 1.14        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024855861 |\n",
      "|    clip_fraction        | 0.0217      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.000271    |\n",
      "|    loss                 | 22.3        |\n",
      "|    n_updates            | 256         |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 77.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.88     |\n",
      "|    ep_rew_mean     | -2.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 122      |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 412      |\n",
      "|    total_timesteps | 50688    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=0.74 +/- 0.76\n",
      "Episode length: 6.80 +/- 1.72\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 6.8       |\n",
      "|    mean_reward          | 0.738     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 51000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0223919 |\n",
      "|    clip_fraction        | 0.0268    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -1.7      |\n",
      "|    explained_variance   | 0.0893    |\n",
      "|    learning_rate        | 0.00027   |\n",
      "|    loss                 | 52.6      |\n",
      "|    n_updates            | 264       |\n",
      "|    policy_gradient_loss | -0.0183   |\n",
      "|    value_loss           | 79        |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=1.15 +/- 0.04\n",
      "Episode length: 7.40 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.47     |\n",
      "|    ep_rew_mean     | -1.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 122      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 425      |\n",
      "|    total_timesteps | 52224    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.40 +/- 2.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.4         |\n",
      "|    mean_reward          | 1.13        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 53000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021386981 |\n",
      "|    clip_fraction        | 0.0214      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.000269    |\n",
      "|    loss                 | 55.3        |\n",
      "|    n_updates            | 272         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 93.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.3      |\n",
      "|    ep_rew_mean     | 0.698    |\n",
      "| time/              |          |\n",
      "|    fps             | 122      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 437      |\n",
      "|    total_timesteps | 53760    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=0.76 +/- 0.82\n",
      "Episode length: 7.60 +/- 1.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.6          |\n",
      "|    mean_reward          | 0.757        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 54000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046366346 |\n",
      "|    clip_fraction        | 0.000814     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | -0.0324      |\n",
      "|    learning_rate        | 0.000268     |\n",
      "|    loss                 | 11.6         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00718     |\n",
      "|    value_loss           | 24.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=0.77 +/- 0.79\n",
      "Episode length: 8.20 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | 0.771    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.6      |\n",
      "|    ep_rew_mean     | -5.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 445      |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-0.04 +/- 0.96\n",
      "Episode length: 7.80 +/- 3.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.8         |\n",
      "|    mean_reward          | -0.0381     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016325962 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.000267    |\n",
      "|    loss                 | 49.8        |\n",
      "|    n_updates            | 288         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.01     |\n",
      "|    ep_rew_mean     | 0.751    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 452      |\n",
      "|    total_timesteps | 56832    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=1.14 +/- 0.04\n",
      "Episode length: 7.00 +/- 1.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7            |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 57000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072652474 |\n",
      "|    clip_fraction        | 0.00138      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.69        |\n",
      "|    explained_variance   | -0.00595     |\n",
      "|    learning_rate        | 0.000266     |\n",
      "|    loss                 | 4.29         |\n",
      "|    n_updates            | 296          |\n",
      "|    policy_gradient_loss | -0.00739     |\n",
      "|    value_loss           | 23.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=1.15 +/- 0.04\n",
      "Episode length: 7.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9        |\n",
      "|    ep_rew_mean     | -0.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 460      |\n",
      "|    total_timesteps | 58368    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=59000, episode_reward=0.74 +/- 0.78\n",
      "Episode length: 7.00 +/- 2.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | 0.743       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 59000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011589243 |\n",
      "|    clip_fraction        | 0.00749     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.0933      |\n",
      "|    learning_rate        | 0.000265    |\n",
      "|    loss                 | 23.5        |\n",
      "|    n_updates            | 304         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 58.7        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.32     |\n",
      "|    ep_rew_mean     | -0.332   |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 467      |\n",
      "|    total_timesteps | 59904    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=0.77 +/- 0.83\n",
      "Episode length: 8.20 +/- 1.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.2         |\n",
      "|    mean_reward          | 0.771       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020617155 |\n",
      "|    clip_fraction        | 0.0202      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.000264    |\n",
      "|    loss                 | 39.5        |\n",
      "|    n_updates            | 312         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 57.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=0.77 +/- 0.80\n",
      "Episode length: 8.00 +/- 1.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | 0.767    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.31     |\n",
      "|    ep_rew_mean     | -0.272   |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 480      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=1.15 +/- 0.04\n",
      "Episode length: 7.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.4          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 62000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114150075 |\n",
      "|    clip_fraction        | 0.00618      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 0.0901       |\n",
      "|    learning_rate        | 0.000263     |\n",
      "|    loss                 | 9.36         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0129      |\n",
      "|    value_loss           | 22.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.14     |\n",
      "|    ep_rew_mean     | -2.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 492      |\n",
      "|    total_timesteps | 62976    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=1.16 +/- 0.06\n",
      "Episode length: 7.80 +/- 2.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.8         |\n",
      "|    mean_reward          | 1.16        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 63000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013179134 |\n",
      "|    clip_fraction        | 0.00944     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.000262    |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 328         |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 74.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=1.15 +/- 0.02\n",
      "Episode length: 7.40 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.08     |\n",
      "|    ep_rew_mean     | -0.198   |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 505      |\n",
      "|    total_timesteps | 64512    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=1.15 +/- 0.03\n",
      "Episode length: 7.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.2         |\n",
      "|    mean_reward          | 1.15        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023205483 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0688      |\n",
      "|    learning_rate        | 0.000261    |\n",
      "|    loss                 | 36.8        |\n",
      "|    n_updates            | 336         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 40.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=1.15 +/- 0.04\n",
      "Episode length: 7.40 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.76     |\n",
      "|    ep_rew_mean     | -0.365   |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 518      |\n",
      "|    total_timesteps | 66048    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=1.15 +/- 0.02\n",
      "Episode length: 7.40 +/- 1.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.4         |\n",
      "|    mean_reward          | 1.15        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 67000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015459582 |\n",
      "|    clip_fraction        | 0.0135      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.00026     |\n",
      "|    loss                 | 22.7        |\n",
      "|    n_updates            | 344         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 39.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.71     |\n",
      "|    ep_rew_mean     | 0.624    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 530      |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=1.12 +/- 0.03\n",
      "Episode length: 6.20 +/- 1.17\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 6.2       |\n",
      "|    mean_reward          | 1.12      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 68000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0162386 |\n",
      "|    clip_fraction        | 0.0169    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -1.56     |\n",
      "|    explained_variance   | 0.147     |\n",
      "|    learning_rate        | 0.000259  |\n",
      "|    loss                 | 9.58      |\n",
      "|    n_updates            | 352       |\n",
      "|    policy_gradient_loss | -0.0159   |\n",
      "|    value_loss           | 20.3      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=69000, episode_reward=0.76 +/- 0.81\n",
      "Episode length: 7.80 +/- 2.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 0.762    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.42     |\n",
      "|    ep_rew_mean     | -1.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 543      |\n",
      "|    total_timesteps | 69120    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=0.76 +/- 0.81\n",
      "Episode length: 7.60 +/- 2.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | 0.757       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021046301 |\n",
      "|    clip_fraction        | 0.016       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.0799      |\n",
      "|    learning_rate        | 0.000259    |\n",
      "|    loss                 | 31.8        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 58.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.84     |\n",
      "|    ep_rew_mean     | -0.343   |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 555      |\n",
      "|    total_timesteps | 70656    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=1.12 +/- 0.03\n",
      "Episode length: 6.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 71000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012146416 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.000258    |\n",
      "|    loss                 | 35.9        |\n",
      "|    n_updates            | 368         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 38.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=0.35 +/- 0.98\n",
      "Episode length: 7.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 0.348    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.48     |\n",
      "|    ep_rew_mean     | -0.332   |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 568      |\n",
      "|    total_timesteps | 72192    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=1.13 +/- 0.03\n",
      "Episode length: 6.40 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.4        |\n",
      "|    mean_reward          | 1.13       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 73000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01412576 |\n",
      "|    clip_fraction        | 0.00781    |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 0.189      |\n",
      "|    learning_rate        | 0.000257   |\n",
      "|    loss                 | 49.1       |\n",
      "|    n_updates            | 376        |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    value_loss           | 55.2       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.8      |\n",
      "|    ep_rew_mean     | 0.786    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 576      |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=1.17 +/- 0.04\n",
      "Episode length: 8.00 +/- 1.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | 1.17        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011111821 |\n",
      "|    clip_fraction        | 0.0048      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.081       |\n",
      "|    learning_rate        | 0.000256    |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 384         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 21.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=0.70 +/- 0.78\n",
      "Episode length: 5.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 0.7      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.57     |\n",
      "|    ep_rew_mean     | -1.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 589      |\n",
      "|    total_timesteps | 75264    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=1.11 +/- 0.02\n",
      "Episode length: 5.80 +/- 0.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015450194 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.0581      |\n",
      "|    learning_rate        | 0.000255    |\n",
      "|    loss                 | 42.9        |\n",
      "|    n_updates            | 392         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 59.7        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.16     |\n",
      "|    ep_rew_mean     | 0.83     |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 601      |\n",
      "|    total_timesteps | 76800    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=1.16 +/- 0.07\n",
      "Episode length: 7.80 +/- 2.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.8          |\n",
      "|    mean_reward          | 1.16         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 77000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057900306 |\n",
      "|    clip_fraction        | 0.00366      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.0462       |\n",
      "|    learning_rate        | 0.000254     |\n",
      "|    loss                 | 7.19         |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00877     |\n",
      "|    value_loss           | 22           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=1.15 +/- 0.06\n",
      "Episode length: 7.40 +/- 2.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.4      |\n",
      "|    ep_rew_mean     | -0.154   |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 614      |\n",
      "|    total_timesteps | 78336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=0.31 +/- 0.96\n",
      "Episode length: 5.80 +/- 1.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 0.314       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 79000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011205167 |\n",
      "|    clip_fraction        | 0.00708     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.0518      |\n",
      "|    learning_rate        | 0.000253    |\n",
      "|    loss                 | 9.75        |\n",
      "|    n_updates            | 408         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 21.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.08     |\n",
      "|    ep_rew_mean     | -0.121   |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 627      |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=1.14 +/- 0.02\n",
      "Episode length: 7.00 +/- 0.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7            |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029021513 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 0.000252     |\n",
      "|    loss                 | 40.5         |\n",
      "|    n_updates            | 416          |\n",
      "|    policy_gradient_loss | -0.00747     |\n",
      "|    value_loss           | 57.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=1.12 +/- 0.03\n",
      "Episode length: 6.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.3      |\n",
      "|    ep_rew_mean     | 0.714    |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 635      |\n",
      "|    total_timesteps | 81408    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=0.76 +/- 0.82\n",
      "Episode length: 7.80 +/- 1.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.8         |\n",
      "|    mean_reward          | 0.762       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017270047 |\n",
      "|    clip_fraction        | 0.0111      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -1.96       |\n",
      "|    learning_rate        | 0.000251    |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 424         |\n",
      "|    policy_gradient_loss | -0.00983    |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.75     |\n",
      "|    ep_rew_mean     | -0.309   |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 648      |\n",
      "|    total_timesteps | 82944    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=1.15 +/- 0.05\n",
      "Episode length: 7.40 +/- 1.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.4         |\n",
      "|    mean_reward          | 1.15        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 83000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025380239 |\n",
      "|    clip_fraction        | 0.027       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.8         |\n",
      "|    n_updates            | 432         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 19.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=1.15 +/- 0.07\n",
      "Episode length: 7.20 +/- 2.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.99     |\n",
      "|    ep_rew_mean     | -1.11    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 661      |\n",
      "|    total_timesteps | 84480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=1.14 +/- 0.03\n",
      "Episode length: 6.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | 1.14        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 85000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012972918 |\n",
      "|    clip_fraction        | 0.0112      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.227       |\n",
      "|    learning_rate        | 0.000249    |\n",
      "|    loss                 | 20.6        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0089     |\n",
      "|    value_loss           | 36          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.60 +/- 2.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.9      |\n",
      "|    ep_rew_mean     | 0.824    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 674      |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=0.73 +/- 0.83\n",
      "Episode length: 6.60 +/- 2.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.6         |\n",
      "|    mean_reward          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 87000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008649035 |\n",
      "|    clip_fraction        | 0.00415     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | -1.67       |\n",
      "|    learning_rate        | 0.000248    |\n",
      "|    loss                 | 0.984       |\n",
      "|    n_updates            | 448         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.18     |\n",
      "|    ep_rew_mean     | 0.791    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 688      |\n",
      "|    total_timesteps | 87552    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=88000, episode_reward=1.12 +/- 0.03\n",
      "Episode length: 6.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 88000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019762391 |\n",
      "|    clip_fraction        | 0.0386      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -1.43       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | 0.326       |\n",
      "|    n_updates            | 456         |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=0.34 +/- 0.94\n",
      "Episode length: 7.00 +/- 2.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 0.343    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | 0.845    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 701      |\n",
      "|    total_timesteps | 89088    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=0.74 +/- 0.79\n",
      "Episode length: 6.80 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | 0.738       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021415785 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | -0.88       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | 0.171       |\n",
      "|    n_updates            | 464         |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.562       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | 0.907    |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 714      |\n",
      "|    total_timesteps | 90624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 91000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095180925 |\n",
      "|    clip_fraction        | 0.00439      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | 12.2         |\n",
      "|    n_updates            | 472          |\n",
      "|    policy_gradient_loss | -0.00559     |\n",
      "|    value_loss           | 38.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=1.14 +/- 0.07\n",
      "Episode length: 7.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.88     |\n",
      "|    ep_rew_mean     | 0.904    |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 727      |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.60 +/- 2.15\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 6.6       |\n",
      "|    mean_reward          | 1.13      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 93000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0180808 |\n",
      "|    clip_fraction        | 0.0208    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -1.19     |\n",
      "|    explained_variance   | -0.928    |\n",
      "|    learning_rate        | 0.000245  |\n",
      "|    loss                 | 0.263     |\n",
      "|    n_updates            | 480       |\n",
      "|    policy_gradient_loss | -0.0131   |\n",
      "|    value_loss           | 0.605     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.82     |\n",
      "|    ep_rew_mean     | -0.108   |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 735      |\n",
      "|    total_timesteps | 93696    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.40 +/- 2.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.4         |\n",
      "|    mean_reward          | 1.13        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 94000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013087426 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.0647      |\n",
      "|    learning_rate        | 0.000244    |\n",
      "|    loss                 | 23.8        |\n",
      "|    n_updates            | 488         |\n",
      "|    policy_gradient_loss | -0.00605    |\n",
      "|    value_loss           | 59.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.23     |\n",
      "|    ep_rew_mean     | 0.988    |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 743      |\n",
      "|    total_timesteps | 95232    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=0.69 +/- 0.79\n",
      "Episode length: 4.80 +/- 0.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 96000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051868376 |\n",
      "|    clip_fraction        | 0.00293      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.15        |\n",
      "|    explained_variance   | 0.0816       |\n",
      "|    learning_rate        | 0.000243     |\n",
      "|    loss                 | 14.8         |\n",
      "|    n_updates            | 496          |\n",
      "|    policy_gradient_loss | -0.00838     |\n",
      "|    value_loss           | 38.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.15     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 756      |\n",
      "|    total_timesteps | 96768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=0.70 +/- 0.82\n",
      "Episode length: 5.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 0.705       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 97000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007965664 |\n",
      "|    clip_fraction        | 0.00675     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.0711      |\n",
      "|    learning_rate        | 0.000242    |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 504         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 39.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=98000, episode_reward=1.14 +/- 0.07\n",
      "Episode length: 6.80 +/- 2.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.84     |\n",
      "|    ep_rew_mean     | 0.923    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 769      |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=1.14 +/- 0.03\n",
      "Episode length: 7.00 +/- 1.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7            |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 99000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081963735 |\n",
      "|    clip_fraction        | 0.00667      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.153        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | 5.45         |\n",
      "|    n_updates            | 512          |\n",
      "|    policy_gradient_loss | -0.00693     |\n",
      "|    value_loss           | 19.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.4      |\n",
      "|    ep_rew_mean     | 0.832    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 782      |\n",
      "|    total_timesteps | 99840    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.60 +/- 2.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110404985 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | -1.39        |\n",
      "|    learning_rate        | 0.00024      |\n",
      "|    loss                 | 0.477        |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    value_loss           | 1.22         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=1.15 +/- 0.04\n",
      "Episode length: 7.40 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.29     |\n",
      "|    ep_rew_mean     | -0.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 790      |\n",
      "|    total_timesteps | 101376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=0.33 +/- 0.96\n",
      "Episode length: 6.40 +/- 1.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 0.329        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 102000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071696136 |\n",
      "|    clip_fraction        | 0.00431      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.154        |\n",
      "|    learning_rate        | 0.000239     |\n",
      "|    loss                 | 19.9         |\n",
      "|    n_updates            | 528          |\n",
      "|    policy_gradient_loss | -0.00654     |\n",
      "|    value_loss           | 37.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.26     |\n",
      "|    ep_rew_mean     | -0.181   |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 798      |\n",
      "|    total_timesteps | 102912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=1.10 +/- 0.02\n",
      "Episode length: 5.00 +/- 0.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 103000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051082196 |\n",
      "|    clip_fraction        | 0.00269      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | 20.9         |\n",
      "|    n_updates            | 536          |\n",
      "|    policy_gradient_loss | -0.0049      |\n",
      "|    value_loss           | 56.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.56     |\n",
      "|    ep_rew_mean     | -1.08    |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 806      |\n",
      "|    total_timesteps | 104448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=1.14 +/- 0.05\n",
      "Episode length: 7.00 +/- 2.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7            |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 105000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048879497 |\n",
      "|    clip_fraction        | 0.0035       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.000237     |\n",
      "|    loss                 | 22.1         |\n",
      "|    n_updates            | 544          |\n",
      "|    policy_gradient_loss | -0.00641     |\n",
      "|    value_loss           | 38.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.42     |\n",
      "|    ep_rew_mean     | 0.873    |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 814      |\n",
      "|    total_timesteps | 105984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=0.70 +/- 0.77\n",
      "Episode length: 5.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 106000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014217962 |\n",
      "|    clip_fraction        | 0.00846     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -1.83       |\n",
      "|    learning_rate        | 0.000236    |\n",
      "|    loss                 | 0.973       |\n",
      "|    n_updates            | 552         |\n",
      "|    policy_gradient_loss | -0.00992    |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=0.71 +/- 0.79\n",
      "Episode length: 5.80 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 0.714    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.17     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 822      |\n",
      "|    total_timesteps | 107520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 108000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020367438 |\n",
      "|    clip_fraction        | 0.0201      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | -1.8        |\n",
      "|    learning_rate        | 0.000235    |\n",
      "|    loss                 | 0.596       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=0.70 +/- 0.77\n",
      "Episode length: 5.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 0.7      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.08     |\n",
      "|    ep_rew_mean     | 0.965    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 830      |\n",
      "|    total_timesteps | 109056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 2.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014756758 |\n",
      "|    clip_fraction        | 0.0246      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | -1.19       |\n",
      "|    learning_rate        | 0.000235    |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 568         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 0.388       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.04     |\n",
      "|    ep_rew_mean     | 0.944    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 838      |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.40 +/- 2.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 111000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069144764 |\n",
      "|    clip_fraction        | 0.00269      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.947       |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.000234     |\n",
      "|    loss                 | 0.658        |\n",
      "|    n_updates            | 576          |\n",
      "|    policy_gradient_loss | -0.00304     |\n",
      "|    value_loss           | 19           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=0.71 +/- 0.81\n",
      "Episode length: 5.80 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 0.714    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.08     |\n",
      "|    ep_rew_mean     | -0.145   |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 846      |\n",
      "|    total_timesteps | 112128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.00 +/- 1.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 113000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014015078 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.958       |\n",
      "|    explained_variance   | 0.118        |\n",
      "|    learning_rate        | 0.000233     |\n",
      "|    loss                 | 38           |\n",
      "|    n_updates            | 584          |\n",
      "|    policy_gradient_loss | -0.00459     |\n",
      "|    value_loss           | 75.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.49     |\n",
      "|    ep_rew_mean     | 0.991    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 854      |\n",
      "|    total_timesteps | 113664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=0.69 +/- 0.77\n",
      "Episode length: 4.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 114000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010737452 |\n",
      "|    clip_fraction        | 0.00415     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.925      |\n",
      "|    explained_variance   | -1.44       |\n",
      "|    learning_rate        | 0.000232    |\n",
      "|    loss                 | 0.364       |\n",
      "|    n_updates            | 592         |\n",
      "|    policy_gradient_loss | -0.00561    |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 115000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.69     |\n",
      "|    ep_rew_mean     | 0.935    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 869      |\n",
      "|    total_timesteps | 115200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 116000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016638292 |\n",
      "|    clip_fraction        | 0.0183      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.953      |\n",
      "|    explained_variance   | 0.0963      |\n",
      "|    learning_rate        | 0.000231    |\n",
      "|    loss                 | 24.7        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.06     |\n",
      "|    ep_rew_mean     | -0.126   |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 882      |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=117000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.20 +/- 1.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 117000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017685638 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.985       |\n",
      "|    explained_variance   | 0.181        |\n",
      "|    learning_rate        | 0.00023      |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 608          |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    value_loss           | 19           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=0.71 +/- 0.77\n",
      "Episode length: 5.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 0.714    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.05     |\n",
      "|    ep_rew_mean     | 0.944    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 891      |\n",
      "|    total_timesteps | 118272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 119000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059069227 |\n",
      "|    clip_fraction        | 0.00277      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.933       |\n",
      "|    explained_variance   | -1.37        |\n",
      "|    learning_rate        | 0.000229     |\n",
      "|    loss                 | 0.757        |\n",
      "|    n_updates            | 616          |\n",
      "|    policy_gradient_loss | -0.00707     |\n",
      "|    value_loss           | 1.33         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.01     |\n",
      "|    ep_rew_mean     | 0.923    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 898      |\n",
      "|    total_timesteps | 119808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=0.68 +/- 0.79\n",
      "Episode length: 4.20 +/- 0.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004080391 |\n",
      "|    clip_fraction        | 0.00244     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.893      |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.000228    |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 624         |\n",
      "|    policy_gradient_loss | -0.00394    |\n",
      "|    value_loss           | 19.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=0.73 +/- 0.77\n",
      "Episode length: 6.60 +/- 2.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 0.733    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.6      |\n",
      "|    ep_rew_mean     | -0.0967  |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 906      |\n",
      "|    total_timesteps | 121344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.2           |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 122000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085087254 |\n",
      "|    clip_fraction        | 0.000326      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.925        |\n",
      "|    explained_variance   | 0.239         |\n",
      "|    learning_rate        | 0.000227      |\n",
      "|    loss                 | 27            |\n",
      "|    n_updates            | 632           |\n",
      "|    policy_gradient_loss | -0.00506      |\n",
      "|    value_loss           | 34.9          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.83     |\n",
      "|    ep_rew_mean     | -0.0712  |\n",
      "| time/              |          |\n",
      "|    fps             | 134      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 914      |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 123000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017704232 |\n",
      "|    clip_fraction        | 0.0198      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.955      |\n",
      "|    explained_variance   | 0.043       |\n",
      "|    learning_rate        | 0.000226    |\n",
      "|    loss                 | 5.83        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 20.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=1.15 +/- 0.09\n",
      "Episode length: 7.20 +/- 3.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.39     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 928      |\n",
      "|    total_timesteps | 124416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=0.70 +/- 0.75\n",
      "Episode length: 5.00 +/- 2.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 125000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017827645 |\n",
      "|    clip_fraction        | 0.0175      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | -1.82       |\n",
      "|    learning_rate        | 0.000225    |\n",
      "|    loss                 | 0.514       |\n",
      "|    n_updates            | 648         |\n",
      "|    policy_gradient_loss | -0.00924    |\n",
      "|    value_loss           | 1.34        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.1      |\n",
      "|    ep_rew_mean     | -0.165   |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 942      |\n",
      "|    total_timesteps | 125952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=0.78 +/- 0.77\n",
      "Episode length: 8.40 +/- 2.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8.4          |\n",
      "|    mean_reward          | 0.776        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 126000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038315915 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.954       |\n",
      "|    explained_variance   | 0.156        |\n",
      "|    learning_rate        | 0.000224     |\n",
      "|    loss                 | 22.9         |\n",
      "|    n_updates            | 656          |\n",
      "|    policy_gradient_loss | -0.00712     |\n",
      "|    value_loss           | 38.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=127000, episode_reward=0.70 +/- 0.80\n",
      "Episode length: 5.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 0.7      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 127000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.82     |\n",
      "|    ep_rew_mean     | -1.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 956      |\n",
      "|    total_timesteps | 127488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=0.74 +/- 0.78\n",
      "Episode length: 7.00 +/- 2.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7            |\n",
      "|    mean_reward          | 0.743        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 128000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019916312 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.939       |\n",
      "|    explained_variance   | 0.127        |\n",
      "|    learning_rate        | 0.000224     |\n",
      "|    loss                 | 28.4         |\n",
      "|    n_updates            | 664          |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    value_loss           | 56.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.37     |\n",
      "|    ep_rew_mean     | 0.948    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 970      |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=0.70 +/- 0.78\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 0.705       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005921308 |\n",
      "|    clip_fraction        | 0.00309     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.95       |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.000223    |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 672         |\n",
      "|    policy_gradient_loss | -0.00882    |\n",
      "|    value_loss           | 38          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.34     |\n",
      "|    ep_rew_mean     | -0.163   |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 984      |\n",
      "|    total_timesteps | 130560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 131000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005801782 |\n",
      "|    clip_fraction        | 0.0061      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.000222    |\n",
      "|    loss                 | 9.67        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 19          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.60 +/- 2.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 132000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.51     |\n",
      "|    ep_rew_mean     | -0.0788  |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 999      |\n",
      "|    total_timesteps | 132096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=1.16 +/- 0.08\n",
      "Episode length: 7.80 +/- 3.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.8          |\n",
      "|    mean_reward          | 1.16         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 133000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063032694 |\n",
      "|    clip_fraction        | 0.00456      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.938       |\n",
      "|    explained_variance   | 0.182        |\n",
      "|    learning_rate        | 0.000221     |\n",
      "|    loss                 | 9.64         |\n",
      "|    n_updates            | 688          |\n",
      "|    policy_gradient_loss | -0.0106      |\n",
      "|    value_loss           | 19.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.01     |\n",
      "|    ep_rew_mean     | 0.843    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 1012     |\n",
      "|    total_timesteps | 133632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 7.00 +/- 2.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7            |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 134000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067747966 |\n",
      "|    clip_fraction        | 0.00545      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.968       |\n",
      "|    explained_variance   | -1.85        |\n",
      "|    learning_rate        | 0.00022      |\n",
      "|    loss                 | 0.342        |\n",
      "|    n_updates            | 696          |\n",
      "|    policy_gradient_loss | -0.00814     |\n",
      "|    value_loss           | 1.38         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=1.15 +/- 0.06\n",
      "Episode length: 7.40 +/- 2.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.82     |\n",
      "|    ep_rew_mean     | -0.0714  |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1020     |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 136000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018079752 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.945       |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.000219     |\n",
      "|    loss                 | 8.76         |\n",
      "|    n_updates            | 704          |\n",
      "|    policy_gradient_loss | -0.00375     |\n",
      "|    value_loss           | 18.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.68     |\n",
      "|    ep_rew_mean     | 0.935    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 89       |\n",
      "|    time_elapsed    | 1028     |\n",
      "|    total_timesteps | 136704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=0.71 +/- 0.79\n",
      "Episode length: 5.80 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 0.714       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 137000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007407494 |\n",
      "|    clip_fraction        | 0.00252     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.981      |\n",
      "|    explained_variance   | -1.52       |\n",
      "|    learning_rate        | 0.000218    |\n",
      "|    loss                 | 0.566       |\n",
      "|    n_updates            | 712         |\n",
      "|    policy_gradient_loss | -0.00577    |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=0.70 +/- 0.75\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.695    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 138000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.48     |\n",
      "|    ep_rew_mean     | 0.87     |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 1037     |\n",
      "|    total_timesteps | 138240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=0.37 +/- 0.90\n",
      "Episode length: 8.20 +/- 3.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.2         |\n",
      "|    mean_reward          | 0.371       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 139000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016781094 |\n",
      "|    clip_fraction        | 0.0135      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.985      |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.000217    |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    value_loss           | 18.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.51     |\n",
      "|    ep_rew_mean     | -0.0188  |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 1045     |\n",
      "|    total_timesteps | 139776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=0.74 +/- 0.73\n",
      "Episode length: 7.00 +/- 3.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7            |\n",
      "|    mean_reward          | 0.743        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051060375 |\n",
      "|    clip_fraction        | 0.00106      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.904       |\n",
      "|    explained_variance   | 0.202        |\n",
      "|    learning_rate        | 0.000216     |\n",
      "|    loss                 | 12.3         |\n",
      "|    n_updates            | 728          |\n",
      "|    policy_gradient_loss | -0.0073      |\n",
      "|    value_loss           | 36           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.40 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.7      |\n",
      "|    ep_rew_mean     | 0.916    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 1059     |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=0.75 +/- 0.82\n",
      "Episode length: 7.20 +/- 2.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.2         |\n",
      "|    mean_reward          | 0.748       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 142000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007591033 |\n",
      "|    clip_fraction        | 0.00488     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.889      |\n",
      "|    explained_variance   | -1.59       |\n",
      "|    learning_rate        | 0.000215    |\n",
      "|    loss                 | 0.582       |\n",
      "|    n_updates            | 736         |\n",
      "|    policy_gradient_loss | -0.00816    |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.53     |\n",
      "|    ep_rew_mean     | 0.952    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 1073     |\n",
      "|    total_timesteps | 142848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=0.68 +/- 0.78\n",
      "Episode length: 4.40 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 143000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011907652 |\n",
      "|    clip_fraction        | 0.00781     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.912      |\n",
      "|    explained_variance   | -1.43       |\n",
      "|    learning_rate        | 0.000214    |\n",
      "|    loss                 | 0.263       |\n",
      "|    n_updates            | 744         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.923       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.56     |\n",
      "|    ep_rew_mean     | 0.892    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 1087     |\n",
      "|    total_timesteps | 144384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=1.13 +/- 0.07\n",
      "Episode length: 6.60 +/- 2.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.6         |\n",
      "|    mean_reward          | 1.13        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 145000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012784951 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.88       |\n",
      "|    explained_variance   | -1.53       |\n",
      "|    learning_rate        | 0.000213    |\n",
      "|    loss                 | 0.206       |\n",
      "|    n_updates            | 752         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.825       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.9      |\n",
      "|    ep_rew_mean     | 0.98     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 95       |\n",
      "|    time_elapsed    | 1101     |\n",
      "|    total_timesteps | 145920   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=146000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 2.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 146000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026672522 |\n",
      "|    clip_fraction        | 0.0487      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.82       |\n",
      "|    explained_variance   | -0.603      |\n",
      "|    learning_rate        | 0.000212    |\n",
      "|    loss                 | 0.0764      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 0.251       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.49     |\n",
      "|    ep_rew_mean     | 0.971    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 1116     |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.6         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 148000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022819003 |\n",
      "|    clip_fraction        | 0.0303      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.741      |\n",
      "|    explained_variance   | -0.169      |\n",
      "|    learning_rate        | 0.000212    |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 768         |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.2      |\n",
      "|    ep_rew_mean     | -1.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 1130     |\n",
      "|    total_timesteps | 148992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=0.79 +/- 0.75\n",
      "Episode length: 8.80 +/- 2.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8.8          |\n",
      "|    mean_reward          | 0.786        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 149000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056766174 |\n",
      "|    clip_fraction        | 0.00423      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.741       |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.000211     |\n",
      "|    loss                 | 10.3         |\n",
      "|    n_updates            | 776          |\n",
      "|    policy_gradient_loss | -0.00818     |\n",
      "|    value_loss           | 38.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.40 +/- 2.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.87     |\n",
      "|    ep_rew_mean     | 0.956    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1145     |\n",
      "|    total_timesteps | 150528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 2.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 151000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059926263 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.705       |\n",
      "|    explained_variance   | -0.38        |\n",
      "|    learning_rate        | 0.00021      |\n",
      "|    loss                 | 0.137        |\n",
      "|    n_updates            | 784          |\n",
      "|    policy_gradient_loss | -0.00393     |\n",
      "|    value_loss           | 0.373        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=0.74 +/- 0.77\n",
      "Episode length: 7.00 +/- 2.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 0.743    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 152000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.32     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 99       |\n",
      "|    time_elapsed    | 1160     |\n",
      "|    total_timesteps | 152064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 1.1        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 153000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02537425 |\n",
      "|    clip_fraction        | 0.0421     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.717     |\n",
      "|    explained_variance   | -0.319     |\n",
      "|    learning_rate        | 0.000209   |\n",
      "|    loss                 | 0.0303     |\n",
      "|    n_updates            | 792        |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.21     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 1174     |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=0.73 +/- 0.77\n",
      "Episode length: 6.60 +/- 2.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.6         |\n",
      "|    mean_reward          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 154000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018448664 |\n",
      "|    clip_fraction        | 0.0243      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | 0.0505      |\n",
      "|    learning_rate        | 0.000208    |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 20.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.17     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 1188     |\n",
      "|    total_timesteps | 155136   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=156000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.40 +/- 2.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 156000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010531289 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.684       |\n",
      "|    explained_variance   | 0.118        |\n",
      "|    learning_rate        | 0.000207     |\n",
      "|    loss                 | 24.5         |\n",
      "|    n_updates            | 808          |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    value_loss           | 37.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.3      |\n",
      "|    ep_rew_mean     | 0.926    |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 1203     |\n",
      "|    total_timesteps | 156672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.40 +/- 2.58\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.4        |\n",
      "|    mean_reward          | 1.13       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 157000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00226557 |\n",
      "|    clip_fraction        | 0.000163   |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.686     |\n",
      "|    explained_variance   | 0.109      |\n",
      "|    learning_rate        | 0.000206   |\n",
      "|    loss                 | 1.59       |\n",
      "|    n_updates            | 816        |\n",
      "|    policy_gradient_loss | -0.00347   |\n",
      "|    value_loss           | 19.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.20 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 158000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6        |\n",
      "|    ep_rew_mean     | 0.00905  |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 1217     |\n",
      "|    total_timesteps | 158208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.20 +/- 1.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 159000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003776443 |\n",
      "|    clip_fraction        | 0.000732    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.715      |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.000205    |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 824         |\n",
      "|    policy_gradient_loss | -0.00475    |\n",
      "|    value_loss           | 18.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.06     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 1232     |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010410884 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.689       |\n",
      "|    explained_variance   | 0.243        |\n",
      "|    learning_rate        | 0.000204     |\n",
      "|    loss                 | 17.2         |\n",
      "|    n_updates            | 832          |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    value_loss           | 35.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=0.71 +/- 0.79\n",
      "Episode length: 5.80 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 0.714    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.23     |\n",
      "|    ep_rew_mean     | 0.985    |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 1240     |\n",
      "|    total_timesteps | 161280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=1.15 +/- 0.05\n",
      "Episode length: 7.20 +/- 1.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 162000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039599813 |\n",
      "|    clip_fraction        | 0.000651     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.666       |\n",
      "|    explained_variance   | -1.55        |\n",
      "|    learning_rate        | 0.000203     |\n",
      "|    loss                 | 0.524        |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 1.22         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.04     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 1249     |\n",
      "|    total_timesteps | 162816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=0.70 +/- 0.79\n",
      "Episode length: 5.00 +/- 1.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 163000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034183578 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.72        |\n",
      "|    explained_variance   | 0.191        |\n",
      "|    learning_rate        | 0.000202     |\n",
      "|    loss                 | 5.36         |\n",
      "|    n_updates            | 848          |\n",
      "|    policy_gradient_loss | -0.00419     |\n",
      "|    value_loss           | 18.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=0.74 +/- 0.76\n",
      "Episode length: 7.00 +/- 3.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 0.743    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 164000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.33     |\n",
      "|    ep_rew_mean     | 0.907    |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 107      |\n",
      "|    time_elapsed    | 1257     |\n",
      "|    total_timesteps | 164352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 165000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036354957 |\n",
      "|    clip_fraction        | 0.00114      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.725       |\n",
      "|    explained_variance   | -1.2         |\n",
      "|    learning_rate        | 0.000201     |\n",
      "|    loss                 | 0.842        |\n",
      "|    n_updates            | 856          |\n",
      "|    policy_gradient_loss | -0.00648     |\n",
      "|    value_loss           | 1.4          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.3      |\n",
      "|    ep_rew_mean     | 0.966    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1265     |\n",
      "|    total_timesteps | 165888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.60 +/- 2.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.6         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 166000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002521281 |\n",
      "|    clip_fraction        | 0.00122     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.69       |\n",
      "|    explained_variance   | -1.54       |\n",
      "|    learning_rate        | 0.0002      |\n",
      "|    loss                 | 0.164       |\n",
      "|    n_updates            | 864         |\n",
      "|    policy_gradient_loss | -0.00792    |\n",
      "|    value_loss           | 0.82        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=0.71 +/- 0.78\n",
      "Episode length: 5.60 +/- 2.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 0.71     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.07     |\n",
      "|    ep_rew_mean     | 0.901    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 109      |\n",
      "|    time_elapsed    | 1273     |\n",
      "|    total_timesteps | 167424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=0.74 +/- 0.76\n",
      "Episode length: 6.80 +/- 2.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.8          |\n",
      "|    mean_reward          | 0.738        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 168000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036772399 |\n",
      "|    clip_fraction        | 0.000814     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.701       |\n",
      "|    explained_variance   | 0.156        |\n",
      "|    learning_rate        | 0.0002       |\n",
      "|    loss                 | 15.5         |\n",
      "|    n_updates            | 872          |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    value_loss           | 19.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.62     |\n",
      "|    ep_rew_mean     | 0.934    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 1281     |\n",
      "|    total_timesteps | 168960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.20 +/- 1.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 169000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062433933 |\n",
      "|    clip_fraction        | 0.00138      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.692       |\n",
      "|    explained_variance   | -1.89        |\n",
      "|    learning_rate        | 0.000199     |\n",
      "|    loss                 | 0.399        |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0061      |\n",
      "|    value_loss           | 1.04         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=0.72 +/- 0.76\n",
      "Episode length: 6.20 +/- 2.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 0.724    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.43     |\n",
      "|    ep_rew_mean     | 0.989    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 111      |\n",
      "|    time_elapsed    | 1289     |\n",
      "|    total_timesteps | 170496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.00 +/- 2.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 171000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068474547 |\n",
      "|    clip_fraction        | 0.00993      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.706       |\n",
      "|    explained_variance   | 0.0234       |\n",
      "|    learning_rate        | 0.000198     |\n",
      "|    loss                 | 13.9         |\n",
      "|    n_updates            | 888          |\n",
      "|    policy_gradient_loss | -0.0055      |\n",
      "|    value_loss           | 20.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=0.75 +/- 0.83\n",
      "Episode length: 7.20 +/- 2.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 0.748    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.9      |\n",
      "|    ep_rew_mean     | 0.857    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 1298     |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 173000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076139276 |\n",
      "|    clip_fraction        | 0.00854      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.699       |\n",
      "|    explained_variance   | 0.196        |\n",
      "|    learning_rate        | 0.000197     |\n",
      "|    loss                 | 9.83         |\n",
      "|    n_updates            | 896          |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    value_loss           | 18.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.26     |\n",
      "|    ep_rew_mean     | 0.965    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 1306     |\n",
      "|    total_timesteps | 173568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=1.09 +/- 0.02\n",
      "Episode length: 4.60 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 174000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009292343 |\n",
      "|    clip_fraction        | 0.00993     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | -0.612      |\n",
      "|    learning_rate        | 0.000196    |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 904         |\n",
      "|    policy_gradient_loss | -0.00694    |\n",
      "|    value_loss           | 0.292       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 175000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.92     |\n",
      "|    ep_rew_mean     | -0.0529  |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 1314     |\n",
      "|    total_timesteps | 175104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 176000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010215124 |\n",
      "|    clip_fraction        | 0.00773     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.701      |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.000195    |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 912         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.3      |\n",
      "|    ep_rew_mean     | 0.926    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 1322     |\n",
      "|    total_timesteps | 176640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 177000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014874454 |\n",
      "|    clip_fraction        | 0.02        |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.64       |\n",
      "|    explained_variance   | -1.07       |\n",
      "|    learning_rate        | 0.000194    |\n",
      "|    loss                 | 0.174       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 0.481       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.01     |\n",
      "|    ep_rew_mean     | 0.939    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 1330     |\n",
      "|    total_timesteps | 178176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | 1.14        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 179000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014019008 |\n",
      "|    clip_fraction        | 0.0202      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.674      |\n",
      "|    explained_variance   | -0.708      |\n",
      "|    learning_rate        | 0.000193    |\n",
      "|    loss                 | 0.0752      |\n",
      "|    n_updates            | 928         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.279       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.79     |\n",
      "|    ep_rew_mean     | 0.934    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 117      |\n",
      "|    time_elapsed    | 1345     |\n",
      "|    total_timesteps | 179712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015139726 |\n",
      "|    clip_fraction        | 0.0304      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | -0.0882     |\n",
      "|    learning_rate        | 0.000192    |\n",
      "|    loss                 | 0.0395      |\n",
      "|    n_updates            | 936         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 181000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.2      |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 1360     |\n",
      "|    total_timesteps | 181248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.00 +/- 1.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 182000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017757425 |\n",
      "|    clip_fraction        | 0.0225      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | -0.767      |\n",
      "|    learning_rate        | 0.000191    |\n",
      "|    loss                 | 0.0755      |\n",
      "|    n_updates            | 944         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 0.42        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.19     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 119      |\n",
      "|    time_elapsed    | 1375     |\n",
      "|    total_timesteps | 182784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.6         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 183000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013046312 |\n",
      "|    clip_fraction        | 0.0178      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | -0.325      |\n",
      "|    learning_rate        | 0.00019     |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 952         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.58     |\n",
      "|    ep_rew_mean     | -0.0171  |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 1389     |\n",
      "|    total_timesteps | 184320   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=185000, episode_reward=0.72 +/- 0.81\n",
      "Episode length: 6.00 +/- 2.28\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6          |\n",
      "|    mean_reward          | 0.719      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 185000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00414374 |\n",
      "|    clip_fraction        | 0.00228    |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.627     |\n",
      "|    explained_variance   | 0.0276     |\n",
      "|    learning_rate        | 0.000189   |\n",
      "|    loss                 | 15.7       |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.00563   |\n",
      "|    value_loss           | 41         |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.86     |\n",
      "|    ep_rew_mean     | 0.976    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 121      |\n",
      "|    time_elapsed    | 1398     |\n",
      "|    total_timesteps | 185856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 186000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003097472 |\n",
      "|    clip_fraction        | 0.000651    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.000188    |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 968         |\n",
      "|    policy_gradient_loss | -0.00708    |\n",
      "|    value_loss           | 18.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 187000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.85     |\n",
      "|    ep_rew_mean     | 0.995    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 1407     |\n",
      "|    total_timesteps | 187392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 188000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010556222 |\n",
      "|    clip_fraction        | 0.007       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | -0.937      |\n",
      "|    learning_rate        | 0.000188    |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 976         |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    value_loss           | 0.365       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.48     |\n",
      "|    ep_rew_mean     | -1.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 1415     |\n",
      "|    total_timesteps | 188928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 189000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046028155 |\n",
      "|    clip_fraction        | 0.00342      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.583       |\n",
      "|    explained_variance   | 0.0146       |\n",
      "|    learning_rate        | 0.000187     |\n",
      "|    loss                 | 31.1         |\n",
      "|    n_updates            | 984          |\n",
      "|    policy_gradient_loss | -0.00881     |\n",
      "|    value_loss           | 61           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=1.14 +/- 0.05\n",
      "Episode length: 7.00 +/- 2.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.88     |\n",
      "|    ep_rew_mean     | 0.956    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 1423     |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 191000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013357053 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.588       |\n",
      "|    explained_variance   | 0.0408       |\n",
      "|    learning_rate        | 0.000186     |\n",
      "|    loss                 | 15.8         |\n",
      "|    n_updates            | 992          |\n",
      "|    policy_gradient_loss | -0.00396     |\n",
      "|    value_loss           | 20.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=0.72 +/- 0.76\n",
      "Episode length: 6.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 0.724    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.42     |\n",
      "|    ep_rew_mean     | 0.965    |\n",
      "| time/              |          |\n",
      "|    fps             | 134      |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 1431     |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.20 +/- 2.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.2        |\n",
      "|    mean_reward          | 1.12       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 193000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01290571 |\n",
      "|    clip_fraction        | 0.018      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.574     |\n",
      "|    explained_variance   | 0.0037     |\n",
      "|    learning_rate        | 0.000185   |\n",
      "|    loss                 | 0.199      |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.00823   |\n",
      "|    value_loss           | 20.7       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.73     |\n",
      "|    ep_rew_mean     | 0.973    |\n",
      "| time/              |          |\n",
      "|    fps             | 134      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 1440     |\n",
      "|    total_timesteps | 193536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 194000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003043035 |\n",
      "|    clip_fraction        | 0.000732    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | -0.327      |\n",
      "|    learning_rate        | 0.000184    |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 1008        |\n",
      "|    policy_gradient_loss | -0.00474    |\n",
      "|    value_loss           | 0.285       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=195000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 195000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.01     |\n",
      "|    ep_rew_mean     | -0.0107  |\n",
      "| time/              |          |\n",
      "|    fps             | 134      |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 1448     |\n",
      "|    total_timesteps | 195072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=0.69 +/- 0.76\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 196000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001895729 |\n",
      "|    clip_fraction        | 0.000163    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.0716      |\n",
      "|    learning_rate        | 0.000183    |\n",
      "|    loss                 | 11.7        |\n",
      "|    n_updates            | 1016        |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    value_loss           | 39.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.4      |\n",
      "|    ep_rew_mean     | -0.00143 |\n",
      "| time/              |          |\n",
      "|    fps             | 134      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 1462     |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 197000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063413195 |\n",
      "|    clip_fraction        | 0.00789      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.609       |\n",
      "|    explained_variance   | -0.0032      |\n",
      "|    learning_rate        | 0.000182     |\n",
      "|    loss                 | 4.51         |\n",
      "|    n_updates            | 1024         |\n",
      "|    policy_gradient_loss | -0.00804     |\n",
      "|    value_loss           | 21.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.69     |\n",
      "|    ep_rew_mean     | 0.00548  |\n",
      "| time/              |          |\n",
      "|    fps             | 134      |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 1477     |\n",
      "|    total_timesteps | 198144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.60 +/- 1.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 199000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009225374 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.62        |\n",
      "|    explained_variance   | 0.254        |\n",
      "|    learning_rate        | 0.000181     |\n",
      "|    loss                 | 5.28         |\n",
      "|    n_updates            | 1032         |\n",
      "|    policy_gradient_loss | -0.00422     |\n",
      "|    value_loss           | 17.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.03     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 1492     |\n",
      "|    total_timesteps | 199680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 1.07        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004494077 |\n",
      "|    clip_fraction        | 0.00277     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | -1.47       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.545       |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    value_loss           | 0.799       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 201000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.87     |\n",
      "|    ep_rew_mean     | 0.956    |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 1507     |\n",
      "|    total_timesteps | 201216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 202000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003291371 |\n",
      "|    clip_fraction        | 0.000326    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.000179    |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 1048        |\n",
      "|    policy_gradient_loss | -0.00564    |\n",
      "|    value_loss           | 19.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.98     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 1522     |\n",
      "|    total_timesteps | 202752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.40 +/- 1.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 203000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021279035 |\n",
      "|    clip_fraction        | 0.000895     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | -0.909       |\n",
      "|    learning_rate        | 0.000178     |\n",
      "|    loss                 | 0.377        |\n",
      "|    n_updates            | 1056         |\n",
      "|    policy_gradient_loss | -0.00554     |\n",
      "|    value_loss           | 0.715        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.31     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 133      |\n",
      "|    time_elapsed    | 1536     |\n",
      "|    total_timesteps | 204288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 205000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058192466 |\n",
      "|    clip_fraction        | 0.00236      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.565       |\n",
      "|    explained_variance   | -0.753       |\n",
      "|    learning_rate        | 0.000177     |\n",
      "|    loss                 | 0.205        |\n",
      "|    n_updates            | 1064         |\n",
      "|    policy_gradient_loss | -0.0079      |\n",
      "|    value_loss           | 0.46         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.3      |\n",
      "|    ep_rew_mean     | 0.966    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 1551     |\n",
      "|    total_timesteps | 205824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 206000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015032652 |\n",
      "|    clip_fraction        | 0.0226      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.539      |\n",
      "|    explained_variance   | -0.35       |\n",
      "|    learning_rate        | 0.000177    |\n",
      "|    loss                 | 0.0944      |\n",
      "|    n_updates            | 1072        |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 0.235       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 207000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.84     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 1566     |\n",
      "|    total_timesteps | 207360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 208000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005925369 |\n",
      "|    clip_fraction        | 0.012       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | -0.121      |\n",
      "|    learning_rate        | 0.000176    |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0095     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.93     |\n",
      "|    ep_rew_mean     | 0.0274   |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 1581     |\n",
      "|    total_timesteps | 208896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 1.07        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 209000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004117236 |\n",
      "|    clip_fraction        | 0.00366     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.000175    |\n",
      "|    loss                 | 2.14        |\n",
      "|    n_updates            | 1088        |\n",
      "|    policy_gradient_loss | -0.00407    |\n",
      "|    value_loss           | 19          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-19.08 +/- 40.30\n",
      "Episode length: 6.20 +/- 4.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -19.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.93     |\n",
      "|    ep_rew_mean     | 0.957    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 1595     |\n",
      "|    total_timesteps | 210432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.40 +/- 0.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 211000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045230906 |\n",
      "|    clip_fraction        | 0.0026       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.51        |\n",
      "|    explained_variance   | 0.0886       |\n",
      "|    learning_rate        | 0.000174     |\n",
      "|    loss                 | 6.44         |\n",
      "|    n_updates            | 1096         |\n",
      "|    policy_gradient_loss | -0.0048      |\n",
      "|    value_loss           | 19.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.8      |\n",
      "|    ep_rew_mean     | 0.994    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 1604     |\n",
      "|    total_timesteps | 211968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 212000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017824328 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.483       |\n",
      "|    explained_variance   | 0.0969       |\n",
      "|    learning_rate        | 0.000173     |\n",
      "|    loss                 | 0.8          |\n",
      "|    n_updates            | 1104         |\n",
      "|    policy_gradient_loss | -0.00309     |\n",
      "|    value_loss           | 19.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 213000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.06     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 139      |\n",
      "|    time_elapsed    | 1613     |\n",
      "|    total_timesteps | 213504   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=214000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 214000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075037964 |\n",
      "|    clip_fraction        | 0.00488      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.5         |\n",
      "|    explained_variance   | 0.0275       |\n",
      "|    learning_rate        | 0.000172     |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 1112         |\n",
      "|    policy_gradient_loss | -0.0102      |\n",
      "|    value_loss           | 20.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.20 +/- 2.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 215000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.1      |\n",
      "|    ep_rew_mean     | 0.941    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 1628     |\n",
      "|    total_timesteps | 215040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.60 +/- 1.62\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.6           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 216000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00052922504 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.49         |\n",
      "|    explained_variance   | 0.0804        |\n",
      "|    learning_rate        | 0.000171      |\n",
      "|    loss                 | 14.6          |\n",
      "|    n_updates            | 1120          |\n",
      "|    policy_gradient_loss | -0.000875     |\n",
      "|    value_loss           | 39.3          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.97     |\n",
      "|    ep_rew_mean     | 0.0283   |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 141      |\n",
      "|    time_elapsed    | 1643     |\n",
      "|    total_timesteps | 216576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=1.15 +/- 0.10\n",
      "Episode length: 7.20 +/- 4.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 217000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015839408 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.482       |\n",
      "|    explained_variance   | 0.238        |\n",
      "|    learning_rate        | 0.00017      |\n",
      "|    loss                 | 6.66         |\n",
      "|    n_updates            | 1128         |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    value_loss           | 34.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 218000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.16     |\n",
      "|    ep_rew_mean     | 0.943    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 1658     |\n",
      "|    total_timesteps | 218112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=0.69 +/- 0.76\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 0.69          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 219000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00055193255 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.495        |\n",
      "|    explained_variance   | 0.139         |\n",
      "|    learning_rate        | 0.000169      |\n",
      "|    loss                 | 12.6          |\n",
      "|    n_updates            | 1136          |\n",
      "|    policy_gradient_loss | -0.000803     |\n",
      "|    value_loss           | 19.6          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.5      |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 143      |\n",
      "|    time_elapsed    | 1666     |\n",
      "|    total_timesteps | 219648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=1.15 +/- 0.07\n",
      "Episode length: 7.20 +/- 2.99\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 220000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026825182 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.505       |\n",
      "|    explained_variance   | -1.7         |\n",
      "|    learning_rate        | 0.000168     |\n",
      "|    loss                 | 0.356        |\n",
      "|    n_updates            | 1144         |\n",
      "|    policy_gradient_loss | -0.00355     |\n",
      "|    value_loss           | 1.02         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.40 +/- 2.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 221000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.87     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 1675     |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 222000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013279401 |\n",
      "|    clip_fraction        | 0.012       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.471      |\n",
      "|    explained_variance   | -1.01       |\n",
      "|    learning_rate        | 0.000167    |\n",
      "|    loss                 | 0.153       |\n",
      "|    n_updates            | 1152        |\n",
      "|    policy_gradient_loss | -0.00995    |\n",
      "|    value_loss           | 0.515       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.99     |\n",
      "|    ep_rew_mean     | 0.959    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 1683     |\n",
      "|    total_timesteps | 222720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 223000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008834537 |\n",
      "|    clip_fraction        | 0.00765     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | -0.607      |\n",
      "|    learning_rate        | 0.000166    |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.00867    |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=224000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 224000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.96     |\n",
      "|    ep_rew_mean     | 0.998    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 1692     |\n",
      "|    total_timesteps | 224256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 225000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013381656 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.507       |\n",
      "|    explained_variance   | 0.172        |\n",
      "|    learning_rate        | 0.000165     |\n",
      "|    loss                 | 2.22         |\n",
      "|    n_updates            | 1168         |\n",
      "|    policy_gradient_loss | -0.000507    |\n",
      "|    value_loss           | 19           |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.93     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 1701     |\n",
      "|    total_timesteps | 225792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 226000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004767863 |\n",
      "|    clip_fraction        | 0.00236     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.483      |\n",
      "|    explained_variance   | -0.69       |\n",
      "|    learning_rate        | 0.000165    |\n",
      "|    loss                 | 0.135       |\n",
      "|    n_updates            | 1176        |\n",
      "|    policy_gradient_loss | -0.00468    |\n",
      "|    value_loss           | 0.304       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=227000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 227000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.85     |\n",
      "|    ep_rew_mean     | 0.935    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 1710     |\n",
      "|    total_timesteps | 227328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 228000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012462833 |\n",
      "|    clip_fraction        | 0.0128      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.47       |\n",
      "|    explained_variance   | -0.898      |\n",
      "|    learning_rate        | 0.000164    |\n",
      "|    loss                 | 0.12        |\n",
      "|    n_updates            | 1184        |\n",
      "|    policy_gradient_loss | -0.00856    |\n",
      "|    value_loss           | 0.347       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.02     |\n",
      "|    ep_rew_mean     | -0.0705  |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 1725     |\n",
      "|    total_timesteps | 228864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=1.16 +/- 0.02\n",
      "Episode length: 7.80 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.8         |\n",
      "|    mean_reward          | 1.16        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 229000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003545089 |\n",
      "|    clip_fraction        | 0.00057     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.473      |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.000163    |\n",
      "|    loss                 | 16          |\n",
      "|    n_updates            | 1192        |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    value_loss           | 18.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=0.70 +/- 0.75\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.695    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.01     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 1740     |\n",
      "|    total_timesteps | 230400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 231000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038605302 |\n",
      "|    clip_fraction        | 0.00106      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.477       |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.000162     |\n",
      "|    loss                 | 18.7         |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.00584     |\n",
      "|    value_loss           | 18.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.81     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 151      |\n",
      "|    time_elapsed    | 1755     |\n",
      "|    total_timesteps | 231936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=0.79 +/- 0.80\n",
      "Episode length: 8.80 +/- 3.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.8         |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 232000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001835944 |\n",
      "|    clip_fraction        | 0.000163    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.451      |\n",
      "|    explained_variance   | 0.0772      |\n",
      "|    learning_rate        | 0.000161    |\n",
      "|    loss                 | 6.4         |\n",
      "|    n_updates            | 1208        |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    value_loss           | 19.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=0.79 +/- 0.81\n",
      "Episode length: 8.80 +/- 2.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | 0.786    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 233000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.82     |\n",
      "|    ep_rew_mean     | 0.0648   |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 1770     |\n",
      "|    total_timesteps | 233472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 234000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001697889 |\n",
      "|    clip_fraction        | 0.000407    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.00016     |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 1216        |\n",
      "|    policy_gradient_loss | -0.00518    |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.45     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 1785     |\n",
      "|    total_timesteps | 235008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 236000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025501589 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.434       |\n",
      "|    explained_variance   | -1.25        |\n",
      "|    learning_rate        | 0.000159     |\n",
      "|    loss                 | 0.164        |\n",
      "|    n_updates            | 1224         |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    value_loss           | 0.541        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.85     |\n",
      "|    ep_rew_mean     | -0.0145  |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 1800     |\n",
      "|    total_timesteps | 236544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 237000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011859766 |\n",
      "|    clip_fraction        | 0.00057      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.463       |\n",
      "|    explained_variance   | 0.169        |\n",
      "|    learning_rate        | 0.000158     |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 1232         |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    value_loss           | 37.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 238000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.79     |\n",
      "|    ep_rew_mean     | -0.036   |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 155      |\n",
      "|    time_elapsed    | 1809     |\n",
      "|    total_timesteps | 238080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.4           |\n",
      "|    mean_reward          | 1.08          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 239000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029185065 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.448        |\n",
      "|    explained_variance   | 0.221         |\n",
      "|    learning_rate        | 0.000157      |\n",
      "|    loss                 | 19.8          |\n",
      "|    n_updates            | 1240          |\n",
      "|    policy_gradient_loss | -0.00233      |\n",
      "|    value_loss           | 36.2          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.49     |\n",
      "|    ep_rew_mean     | 0.987    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 1824     |\n",
      "|    total_timesteps | 239616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=0.71 +/- 0.75\n",
      "Episode length: 5.80 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 0.714        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 240000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017371353 |\n",
      "|    clip_fraction        | 0.000651     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.456       |\n",
      "|    explained_variance   | 0.199        |\n",
      "|    learning_rate        | 0.000156     |\n",
      "|    loss                 | 10.1         |\n",
      "|    n_updates            | 1248         |\n",
      "|    policy_gradient_loss | -0.00555     |\n",
      "|    value_loss           | 18.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=0.70 +/- 0.77\n",
      "Episode length: 5.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 0.7      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.69     |\n",
      "|    ep_rew_mean     | 0.932    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 1839     |\n",
      "|    total_timesteps | 241152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 242000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026110674 |\n",
      "|    clip_fraction        | 0.00179      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.469       |\n",
      "|    explained_variance   | 0.0838       |\n",
      "|    learning_rate        | 0.000155     |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 1256         |\n",
      "|    policy_gradient_loss | -0.0049      |\n",
      "|    value_loss           | 39.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.55     |\n",
      "|    ep_rew_mean     | 1.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 1847     |\n",
      "|    total_timesteps | 242688   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=243000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 243000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006443962 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.418       |\n",
      "|    explained_variance   | -1.79        |\n",
      "|    learning_rate        | 0.000154     |\n",
      "|    loss                 | 0.607        |\n",
      "|    n_updates            | 1264         |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    value_loss           | 1.16         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 244000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.67     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 1856     |\n",
      "|    total_timesteps | 244224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.40 +/- 3.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 245000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065483437 |\n",
      "|    clip_fraction        | 0.00635      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.447       |\n",
      "|    explained_variance   | -2.04        |\n",
      "|    learning_rate        | 0.000153     |\n",
      "|    loss                 | 0.275        |\n",
      "|    n_updates            | 1272         |\n",
      "|    policy_gradient_loss | -0.00497     |\n",
      "|    value_loss           | 0.717        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.28     |\n",
      "|    ep_rew_mean     | 0.886    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 1871     |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.40 +/- 3.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 246000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030832905 |\n",
      "|    clip_fraction        | 0.0022       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.51        |\n",
      "|    explained_variance   | -1.4         |\n",
      "|    learning_rate        | 0.000153     |\n",
      "|    loss                 | 0.458        |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.00463     |\n",
      "|    value_loss           | 1.16         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=247000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.03     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 161      |\n",
      "|    time_elapsed    | 1886     |\n",
      "|    total_timesteps | 247296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.32\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.8          |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 248000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060618445 |\n",
      "|    clip_fraction        | 0.00871      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | -1.66        |\n",
      "|    learning_rate        | 0.000152     |\n",
      "|    loss                 | 0.231        |\n",
      "|    n_updates            | 1288         |\n",
      "|    policy_gradient_loss | -0.00938     |\n",
      "|    value_loss           | 0.647        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.59     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 1895     |\n",
      "|    total_timesteps | 248832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 249000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096214935 |\n",
      "|    clip_fraction        | 0.00773      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.463       |\n",
      "|    explained_variance   | 0.0522       |\n",
      "|    learning_rate        | 0.000151     |\n",
      "|    loss                 | 6.03         |\n",
      "|    n_updates            | 1296         |\n",
      "|    policy_gradient_loss | -0.00787     |\n",
      "|    value_loss           | 20.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.40 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.47     |\n",
      "|    ep_rew_mean     | 0.986    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 1903     |\n",
      "|    total_timesteps | 250368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 251000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050978833 |\n",
      "|    clip_fraction        | 0.00293      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.436       |\n",
      "|    explained_variance   | -0.194       |\n",
      "|    learning_rate        | 0.00015      |\n",
      "|    loss                 | 0.123        |\n",
      "|    n_updates            | 1304         |\n",
      "|    policy_gradient_loss | -0.00445     |\n",
      "|    value_loss           | 0.235        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.64     |\n",
      "|    ep_rew_mean     | -0.0195  |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 1912     |\n",
      "|    total_timesteps | 251904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=0.71 +/- 0.78\n",
      "Episode length: 5.60 +/- 1.62\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.6           |\n",
      "|    mean_reward          | 0.71          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 252000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00057897496 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.441        |\n",
      "|    explained_variance   | 0.153         |\n",
      "|    learning_rate        | 0.000149      |\n",
      "|    loss                 | 3.65          |\n",
      "|    n_updates            | 1312          |\n",
      "|    policy_gradient_loss | -0.00438      |\n",
      "|    value_loss           | 19.1          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=253000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.82     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 1927     |\n",
      "|    total_timesteps | 253440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.40 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 254000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011442419 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.461       |\n",
      "|    explained_variance   | 0.13         |\n",
      "|    learning_rate        | 0.000148     |\n",
      "|    loss                 | 18.7         |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0027      |\n",
      "|    value_loss           | 19.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.91     |\n",
      "|    ep_rew_mean     | 0.00691  |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 1942     |\n",
      "|    total_timesteps | 254976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=0.73 +/- 0.75\n",
      "Episode length: 6.60 +/- 3.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.6           |\n",
      "|    mean_reward          | 0.733         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 255000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017423998 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.47         |\n",
      "|    explained_variance   | 0.166         |\n",
      "|    learning_rate        | 0.000147      |\n",
      "|    loss                 | 11.9          |\n",
      "|    n_updates            | 1328          |\n",
      "|    policy_gradient_loss | -0.000518     |\n",
      "|    value_loss           | 37.1          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.15     |\n",
      "|    ep_rew_mean     | 0.923    |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 1957     |\n",
      "|    total_timesteps | 256512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 257000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016511594 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.446       |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.000146     |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 1336         |\n",
      "|    policy_gradient_loss | -0.00395     |\n",
      "|    value_loss           | 19.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 258000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6        |\n",
      "|    ep_rew_mean     | 0.999    |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 1972     |\n",
      "|    total_timesteps | 258048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 259000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046699704 |\n",
      "|    clip_fraction        | 0.00171      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.449       |\n",
      "|    explained_variance   | -0.831       |\n",
      "|    learning_rate        | 0.000145     |\n",
      "|    loss                 | 0.585        |\n",
      "|    n_updates            | 1344         |\n",
      "|    policy_gradient_loss | -0.00513     |\n",
      "|    value_loss           | 0.875        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.11     |\n",
      "|    ep_rew_mean     | -0.958   |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 1987     |\n",
      "|    total_timesteps | 259584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 260000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031230513 |\n",
      "|    clip_fraction        | 0.000651     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.445       |\n",
      "|    explained_variance   | 0.115        |\n",
      "|    learning_rate        | 0.000144     |\n",
      "|    loss                 | 20.6         |\n",
      "|    n_updates            | 1352         |\n",
      "|    policy_gradient_loss | -0.00484     |\n",
      "|    value_loss           | 38.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 261000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.47     |\n",
      "|    ep_rew_mean     | -0.97    |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 2002     |\n",
      "|    total_timesteps | 261120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 262000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016814243 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | 0.26         |\n",
      "|    learning_rate        | 0.000143     |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.00434     |\n",
      "|    value_loss           | 34.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.91     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 2016     |\n",
      "|    total_timesteps | 262656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 263000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006732215 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | -0.986       |\n",
      "|    learning_rate        | 0.000142     |\n",
      "|    loss                 | 0.84         |\n",
      "|    n_updates            | 1368         |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 1.94         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.8      |\n",
      "|    ep_rew_mean     | -0.0157  |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 2031     |\n",
      "|    total_timesteps | 264192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=0.73 +/- 0.75\n",
      "Episode length: 6.60 +/- 3.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 0.733        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 265000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040461165 |\n",
      "|    clip_fraction        | 0.00285      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.443       |\n",
      "|    explained_variance   | 0.346        |\n",
      "|    learning_rate        | 0.000141     |\n",
      "|    loss                 | 12.5         |\n",
      "|    n_updates            | 1376         |\n",
      "|    policy_gradient_loss | -0.00874     |\n",
      "|    value_loss           | 16.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.66     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 173      |\n",
      "|    time_elapsed    | 2047     |\n",
      "|    total_timesteps | 265728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 266000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006702028 |\n",
      "|    clip_fraction        | 0.00374     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | -0.781      |\n",
      "|    learning_rate        | 0.000141    |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 1384        |\n",
      "|    policy_gradient_loss | -0.00661    |\n",
      "|    value_loss           | 0.495       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 267000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.01     |\n",
      "|    ep_rew_mean     | -0.981   |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 2056     |\n",
      "|    total_timesteps | 267264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 268000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001358694 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.406      |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 1392        |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    value_loss           | 37.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.75     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 175      |\n",
      "|    time_elapsed    | 2064     |\n",
      "|    total_timesteps | 268800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 269000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011533182 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.456       |\n",
      "|    explained_variance   | -1.73        |\n",
      "|    learning_rate        | 0.000139     |\n",
      "|    loss                 | 0.49         |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.00266     |\n",
      "|    value_loss           | 0.91         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=1.12 +/- 0.10\n",
      "Episode length: 6.00 +/- 4.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.6      |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 2079     |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 271000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075130756 |\n",
      "|    clip_fraction        | 0.00749      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.452       |\n",
      "|    explained_variance   | -0.0411      |\n",
      "|    learning_rate        | 0.000138     |\n",
      "|    loss                 | 12.7         |\n",
      "|    n_updates            | 1408         |\n",
      "|    policy_gradient_loss | -0.00783     |\n",
      "|    value_loss           | 21.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.83     |\n",
      "|    ep_rew_mean     | 0.005    |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 177      |\n",
      "|    time_elapsed    | 2094     |\n",
      "|    total_timesteps | 271872   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=272000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.40 +/- 3.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 272000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014463045 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.418       |\n",
      "|    explained_variance   | 0.233        |\n",
      "|    learning_rate        | 0.000137     |\n",
      "|    loss                 | 5.32         |\n",
      "|    n_updates            | 1416         |\n",
      "|    policy_gradient_loss | -0.00299     |\n",
      "|    value_loss           | 18.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.20 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 273000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.39     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 2109     |\n",
      "|    total_timesteps | 273408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.20 +/- 1.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 274000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002261878 |\n",
      "|    clip_fraction        | 0.000488    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.443      |\n",
      "|    explained_variance   | -1.55       |\n",
      "|    learning_rate        | 0.000136    |\n",
      "|    loss                 | 0.291       |\n",
      "|    n_updates            | 1424        |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 0.47        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6        |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 2118     |\n",
      "|    total_timesteps | 274944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 7.00 +/- 2.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | 1.14        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 275000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004770845 |\n",
      "|    clip_fraction        | 0.00415     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.447      |\n",
      "|    explained_variance   | -1.29       |\n",
      "|    learning_rate        | 0.000135    |\n",
      "|    loss                 | 0.382       |\n",
      "|    n_updates            | 1432        |\n",
      "|    policy_gradient_loss | -0.00953    |\n",
      "|    value_loss           | 0.556       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.92     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 2127     |\n",
      "|    total_timesteps | 276480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 277000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005856827 |\n",
      "|    clip_fraction        | 0.00488     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.402      |\n",
      "|    explained_variance   | -1.54       |\n",
      "|    learning_rate        | 0.000134    |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    value_loss           | 0.551       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=0.36 +/- 0.96\n",
      "Episode length: 7.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 0.362    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.82     |\n",
      "|    ep_rew_mean     | 0.955    |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 2136     |\n",
      "|    total_timesteps | 278016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 279000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013663345 |\n",
      "|    clip_fraction        | 0.016       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | -1.01       |\n",
      "|    learning_rate        | 0.000133    |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 1448        |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 0.418       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.07     |\n",
      "|    ep_rew_mean     | 0.921    |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 2151     |\n",
      "|    total_timesteps | 279552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014029858 |\n",
      "|    clip_fraction        | 0.0194      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | -0.7        |\n",
      "|    learning_rate        | 0.000132    |\n",
      "|    loss                 | 0.0692      |\n",
      "|    n_updates            | 1456        |\n",
      "|    policy_gradient_loss | -0.00993    |\n",
      "|    value_loss           | 0.276       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 281000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.5      |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 183      |\n",
      "|    time_elapsed    | 2160     |\n",
      "|    total_timesteps | 281088   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=282000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.40 +/- 2.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 282000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027492044 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.45        |\n",
      "|    explained_variance   | 0.0288       |\n",
      "|    learning_rate        | 0.000131     |\n",
      "|    loss                 | 6.48         |\n",
      "|    n_updates            | 1464         |\n",
      "|    policy_gradient_loss | -0.00349     |\n",
      "|    value_loss           | 20.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.93     |\n",
      "|    ep_rew_mean     | -0.0726  |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 2175     |\n",
      "|    total_timesteps | 282624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=283000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.20 +/- 1.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 283000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032968645 |\n",
      "|    clip_fraction        | 0.00212      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.446       |\n",
      "|    explained_variance   | 0.0138       |\n",
      "|    learning_rate        | 0.00013      |\n",
      "|    loss                 | 7.13         |\n",
      "|    n_updates            | 1472         |\n",
      "|    policy_gradient_loss | -0.00656     |\n",
      "|    value_loss           | 21           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.20 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.71     |\n",
      "|    ep_rew_mean     | 0.992    |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 185      |\n",
      "|    time_elapsed    | 2190     |\n",
      "|    total_timesteps | 284160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 285000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005388709 |\n",
      "|    clip_fraction        | 0.00163     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.401      |\n",
      "|    explained_variance   | -0.682      |\n",
      "|    learning_rate        | 0.00013     |\n",
      "|    loss                 | 0.0649      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.00413    |\n",
      "|    value_loss           | 0.294       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.55     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 2205     |\n",
      "|    total_timesteps | 285696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 286000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025364219 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.405       |\n",
      "|    explained_variance   | 0.213        |\n",
      "|    learning_rate        | 0.000129     |\n",
      "|    loss                 | 11           |\n",
      "|    n_updates            | 1488         |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    value_loss           | 18.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 287000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.27     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 187      |\n",
      "|    time_elapsed    | 2220     |\n",
      "|    total_timesteps | 287232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.80 +/- 1.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 288000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058375783 |\n",
      "|    clip_fraction        | 0.00293      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.384       |\n",
      "|    explained_variance   | -0.716       |\n",
      "|    learning_rate        | 0.000128     |\n",
      "|    loss                 | 0.253        |\n",
      "|    n_updates            | 1496         |\n",
      "|    policy_gradient_loss | -0.00629     |\n",
      "|    value_loss           | 0.45         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.68     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 2235     |\n",
      "|    total_timesteps | 288768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=1.11 +/- 0.09\n",
      "Episode length: 5.80 +/- 3.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 289000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007146602 |\n",
      "|    clip_fraction        | 0.00684     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.4        |\n",
      "|    explained_variance   | -0.892      |\n",
      "|    learning_rate        | 0.000127    |\n",
      "|    loss                 | 0.174       |\n",
      "|    n_updates            | 1504        |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 0.361       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=0.68 +/- 0.79\n",
      "Episode length: 4.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 0.676    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.25     |\n",
      "|    ep_rew_mean     | 1.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 189      |\n",
      "|    time_elapsed    | 2250     |\n",
      "|    total_timesteps | 290304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 291000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066423137 |\n",
      "|    clip_fraction        | 0.00415      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.463       |\n",
      "|    explained_variance   | 0.00371      |\n",
      "|    learning_rate        | 0.000126     |\n",
      "|    loss                 | 5.37         |\n",
      "|    n_updates            | 1512         |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    value_loss           | 20.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.46     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 2259     |\n",
      "|    total_timesteps | 291840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.00 +/- 2.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 292000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009420571 |\n",
      "|    clip_fraction        | 0.00838     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | -0.164      |\n",
      "|    learning_rate        | 0.000125    |\n",
      "|    loss                 | 0.0475      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0062     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 293000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.9      |\n",
      "|    ep_rew_mean     | 0.0667   |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 2268     |\n",
      "|    total_timesteps | 293376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 294000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019636392 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.458       |\n",
      "|    explained_variance   | 0.196        |\n",
      "|    learning_rate        | 0.000124     |\n",
      "|    loss                 | 7.51         |\n",
      "|    n_updates            | 1528         |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    value_loss           | 18.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.75     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 129      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 2283     |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 1.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 295000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006410592 |\n",
      "|    clip_fraction        | 0.00252     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.464      |\n",
      "|    explained_variance   | -0.0759     |\n",
      "|    learning_rate        | 0.000123    |\n",
      "|    loss                 | 0.132       |\n",
      "|    n_updates            | 1536        |\n",
      "|    policy_gradient_loss | -0.00686    |\n",
      "|    value_loss           | 0.282       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.20 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.67     |\n",
      "|    ep_rew_mean     | 0.951    |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 193      |\n",
      "|    time_elapsed    | 2298     |\n",
      "|    total_timesteps | 296448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 1.07        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 297000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005130676 |\n",
      "|    clip_fraction        | 0.00252     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | -0.687      |\n",
      "|    learning_rate        | 0.000122    |\n",
      "|    loss                 | 0.0693      |\n",
      "|    n_updates            | 1544        |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    value_loss           | 0.256       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.57     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 2313     |\n",
      "|    total_timesteps | 297984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 298000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009783936 |\n",
      "|    clip_fraction        | 0.0126      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | -0.296      |\n",
      "|    learning_rate        | 0.000121    |\n",
      "|    loss                 | 0.0415      |\n",
      "|    n_updates            | 1552        |\n",
      "|    policy_gradient_loss | -0.00815    |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=1.13 +/- 0.07\n",
      "Episode length: 6.40 +/- 3.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 299000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.62     |\n",
      "|    ep_rew_mean     | -0.02    |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 195      |\n",
      "|    time_elapsed    | 2328     |\n",
      "|    total_timesteps | 299520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 300000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010829557 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.444       |\n",
      "|    explained_variance   | 0.0931       |\n",
      "|    learning_rate        | 0.00012      |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    value_loss           | 19.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=301000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.60 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 301000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.82     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 2337     |\n",
      "|    total_timesteps | 301056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=0.69 +/- 0.77\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 302000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005344371 |\n",
      "|    clip_fraction        | 0.00228     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.412      |\n",
      "|    explained_variance   | -0.017      |\n",
      "|    learning_rate        | 0.000119    |\n",
      "|    loss                 | 0.0382      |\n",
      "|    n_updates            | 1568        |\n",
      "|    policy_gradient_loss | -0.00672    |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.61     |\n",
      "|    ep_rew_mean     | 0.97     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 197      |\n",
      "|    time_elapsed    | 2352     |\n",
      "|    total_timesteps | 302592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=303000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.6         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 303000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007883992 |\n",
      "|    clip_fraction        | 0.00627     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.424      |\n",
      "|    explained_variance   | 0.0673      |\n",
      "|    learning_rate        | 0.000118    |\n",
      "|    loss                 | 0.0667      |\n",
      "|    n_updates            | 1576        |\n",
      "|    policy_gradient_loss | -0.0092     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 304000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.02     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 2367     |\n",
      "|    total_timesteps | 304128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 305000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011797507 |\n",
      "|    clip_fraction        | 0.0142      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | -0.298      |\n",
      "|    learning_rate        | 0.000118    |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 1584        |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.19     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 199      |\n",
      "|    time_elapsed    | 2381     |\n",
      "|    total_timesteps | 305664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 2.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6             |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 306000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00086447474 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.478        |\n",
      "|    explained_variance   | 0.0634        |\n",
      "|    learning_rate        | 0.000117      |\n",
      "|    loss                 | 17.3          |\n",
      "|    n_updates            | 1592          |\n",
      "|    policy_gradient_loss | -0.00221      |\n",
      "|    value_loss           | 20.1          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=307000, episode_reward=1.13 +/- 0.04\n",
      "Episode length: 6.60 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 307000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.5      |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 2397     |\n",
      "|    total_timesteps | 307200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=0.74 +/- 0.78\n",
      "Episode length: 7.00 +/- 2.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7            |\n",
      "|    mean_reward          | 0.743        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 308000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049843914 |\n",
      "|    clip_fraction        | 0.00187      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | -0.256       |\n",
      "|    learning_rate        | 0.000116     |\n",
      "|    loss                 | 0.0429       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.00509     |\n",
      "|    value_loss           | 0.173        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.8      |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 2411     |\n",
      "|    total_timesteps | 308736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 309000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031149145 |\n",
      "|    clip_fraction        | 0.00138      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.429       |\n",
      "|    explained_variance   | -0.805       |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 0.152        |\n",
      "|    n_updates            | 1608         |\n",
      "|    policy_gradient_loss | -0.0035      |\n",
      "|    value_loss           | 0.227        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=0.71 +/- 0.81\n",
      "Episode length: 5.60 +/- 2.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 0.71     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.96     |\n",
      "|    ep_rew_mean     | 1.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 2421     |\n",
      "|    total_timesteps | 310272   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=311000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 2.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 311000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028637133 |\n",
      "|    clip_fraction        | 0.00114      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.465       |\n",
      "|    explained_variance   | -0.116       |\n",
      "|    learning_rate        | 0.000114     |\n",
      "|    loss                 | 0.0137       |\n",
      "|    n_updates            | 1616         |\n",
      "|    policy_gradient_loss | -0.00667     |\n",
      "|    value_loss           | 0.0863       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.89     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 203      |\n",
      "|    time_elapsed    | 2430     |\n",
      "|    total_timesteps | 311808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.40 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 312000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009533388 |\n",
      "|    clip_fraction        | 0.00423     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | -0.918      |\n",
      "|    learning_rate        | 0.000113    |\n",
      "|    loss                 | 0.0994      |\n",
      "|    n_updates            | 1624        |\n",
      "|    policy_gradient_loss | -0.00869    |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=313000, episode_reward=0.70 +/- 0.81\n",
      "Episode length: 5.40 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 0.705    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 313000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.03     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 2439     |\n",
      "|    total_timesteps | 313344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 314000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009412834 |\n",
      "|    clip_fraction        | 0.00863     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | -0.0389     |\n",
      "|    learning_rate        | 0.000112    |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 1632        |\n",
      "|    policy_gradient_loss | -0.00891    |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.25     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 205      |\n",
      "|    time_elapsed    | 2448     |\n",
      "|    total_timesteps | 314880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 315000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007429908 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.476       |\n",
      "|    explained_variance   | 0.147        |\n",
      "|    learning_rate        | 0.000111     |\n",
      "|    loss                 | 0.943        |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 19.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 316000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.04     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 2463     |\n",
      "|    total_timesteps | 316416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=317000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 317000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006555144 |\n",
      "|    clip_fraction        | 0.00334     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.0152      |\n",
      "|    learning_rate        | 0.00011     |\n",
      "|    loss                 | 0.036       |\n",
      "|    n_updates            | 1648        |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.91     |\n",
      "|    ep_rew_mean     | 0.997    |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 207      |\n",
      "|    time_elapsed    | 2478     |\n",
      "|    total_timesteps | 317952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | 1.14        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 318000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014021956 |\n",
      "|    clip_fraction        | 0.0151      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.44       |\n",
      "|    explained_variance   | 0.0302      |\n",
      "|    learning_rate        | 0.000109    |\n",
      "|    loss                 | 0.0348      |\n",
      "|    n_updates            | 1656        |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    value_loss           | 0.0779      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=319000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 2.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 319000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.42     |\n",
      "|    ep_rew_mean     | 0.989    |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 2492     |\n",
      "|    total_timesteps | 319488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 3.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016757404 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.479       |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.000108     |\n",
      "|    loss                 | 8.67         |\n",
      "|    n_updates            | 1664         |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    value_loss           | 19.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=321000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.62     |\n",
      "|    ep_rew_mean     | 0.89     |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 2508     |\n",
      "|    total_timesteps | 321024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 322000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033278381 |\n",
      "|    clip_fraction        | 0.00212      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.401       |\n",
      "|    explained_variance   | 0.16         |\n",
      "|    learning_rate        | 0.000107     |\n",
      "|    loss                 | 0.0839       |\n",
      "|    n_updates            | 1672         |\n",
      "|    policy_gradient_loss | -0.00472     |\n",
      "|    value_loss           | 0.172        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.81     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 2522     |\n",
      "|    total_timesteps | 322560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=323000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 323000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004190273 |\n",
      "|    clip_fraction        | 0.00187     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.429      |\n",
      "|    explained_variance   | -0.107      |\n",
      "|    learning_rate        | 0.000106    |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.00509    |\n",
      "|    value_loss           | 0.0899      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=1.16 +/- 0.08\n",
      "Episode length: 7.60 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 1.16     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 324000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.98     |\n",
      "|    ep_rew_mean     | 0.999    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 211      |\n",
      "|    time_elapsed    | 2538     |\n",
      "|    total_timesteps | 324096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 1.07        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 325000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001846143 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.013       |\n",
      "|    learning_rate        | 0.000106    |\n",
      "|    loss                 | 16.5        |\n",
      "|    n_updates            | 1688        |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    value_loss           | 20.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.97     |\n",
      "|    ep_rew_mean     | -0.0117  |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 2546     |\n",
      "|    total_timesteps | 325632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 326000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005688951 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.438       |\n",
      "|    explained_variance   | 0.0326       |\n",
      "|    learning_rate        | 0.000105     |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 1696         |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    value_loss           | 20.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=327000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.05     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 213      |\n",
      "|    time_elapsed    | 2561     |\n",
      "|    total_timesteps | 327168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 328000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010566252 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.438       |\n",
      "|    explained_variance   | 0.0346       |\n",
      "|    learning_rate        | 0.000104     |\n",
      "|    loss                 | 6.12         |\n",
      "|    n_updates            | 1704         |\n",
      "|    policy_gradient_loss | -0.00246     |\n",
      "|    value_loss           | 20.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.81     |\n",
      "|    ep_rew_mean     | 0.935    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 2576     |\n",
      "|    total_timesteps | 328704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=329000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 329000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004642204 |\n",
      "|    clip_fraction        | 0.00334     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.42       |\n",
      "|    explained_variance   | -0.466      |\n",
      "|    learning_rate        | 0.000103    |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 1712        |\n",
      "|    policy_gradient_loss | -0.00466    |\n",
      "|    value_loss           | 0.285       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.40 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.6      |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 2591     |\n",
      "|    total_timesteps | 330240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=331000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 331000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075029195 |\n",
      "|    clip_fraction        | 0.00472      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.401       |\n",
      "|    explained_variance   | -0.171       |\n",
      "|    learning_rate        | 0.000102     |\n",
      "|    loss                 | 0.0517       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.67     |\n",
      "|    ep_rew_mean     | 0.911    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 2606     |\n",
      "|    total_timesteps | 331776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 332000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012737386 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.126        |\n",
      "|    learning_rate        | 0.000101     |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 1728         |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 19.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=333000, episode_reward=0.72 +/- 0.80\n",
      "Episode length: 6.20 +/- 2.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 0.724    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.78     |\n",
      "|    ep_rew_mean     | 0.994    |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 2621     |\n",
      "|    total_timesteps | 333312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 334000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041931923 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.407       |\n",
      "|    explained_variance   | -0.176       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0847       |\n",
      "|    n_updates            | 1736         |\n",
      "|    policy_gradient_loss | -0.005       |\n",
      "|    value_loss           | 0.196        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.78     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 2636     |\n",
      "|    total_timesteps | 334848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=0.73 +/- 0.81\n",
      "Episode length: 6.40 +/- 2.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.4         |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 335000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011197255 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | -0.162      |\n",
      "|    learning_rate        | 9.91e-05    |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 1744        |\n",
      "|    policy_gradient_loss | -0.00997    |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.60 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 336000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.27     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 2651     |\n",
      "|    total_timesteps | 336384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=337000, episode_reward=1.14 +/- 0.05\n",
      "Episode length: 6.80 +/- 2.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.8          |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 337000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012930563 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | 0.024        |\n",
      "|    learning_rate        | 9.82e-05     |\n",
      "|    loss                 | 5.4          |\n",
      "|    n_updates            | 1752         |\n",
      "|    policy_gradient_loss | -0.0051      |\n",
      "|    value_loss           | 20.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.24     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 2666     |\n",
      "|    total_timesteps | 337920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.60 +/- 1.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 338000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015058244 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.355       |\n",
      "|    explained_variance   | -0.203       |\n",
      "|    learning_rate        | 9.72e-05     |\n",
      "|    loss                 | 0.0816       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    value_loss           | 0.207        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=339000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.58     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 221      |\n",
      "|    time_elapsed    | 2681     |\n",
      "|    total_timesteps | 339456   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=340000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 340000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021453085 |\n",
      "|    clip_fraction        | 0.000651     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.374       |\n",
      "|    explained_variance   | -0.58        |\n",
      "|    learning_rate        | 9.63e-05     |\n",
      "|    loss                 | 0.0996       |\n",
      "|    n_updates            | 1768         |\n",
      "|    policy_gradient_loss | -0.00434     |\n",
      "|    value_loss           | 0.26         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.86     |\n",
      "|    ep_rew_mean     | 1.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 2696     |\n",
      "|    total_timesteps | 340992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=341000, episode_reward=0.71 +/- 0.78\n",
      "Episode length: 5.60 +/- 2.06\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.6           |\n",
      "|    mean_reward          | 0.71          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 341000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00094964827 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.394        |\n",
      "|    explained_variance   | 0.12          |\n",
      "|    learning_rate        | 9.54e-05      |\n",
      "|    loss                 | 19            |\n",
      "|    n_updates            | 1776          |\n",
      "|    policy_gradient_loss | -0.00127      |\n",
      "|    value_loss           | 38.7          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=0.70 +/- 0.75\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.695    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 342000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.53     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 2712     |\n",
      "|    total_timesteps | 342528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=343000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 343000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.067811e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.365       |\n",
      "|    explained_variance   | 0.0665       |\n",
      "|    learning_rate        | 9.45e-05     |\n",
      "|    loss                 | 13           |\n",
      "|    n_updates            | 1784         |\n",
      "|    policy_gradient_loss | -0.000919    |\n",
      "|    value_loss           | 39.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=0.69 +/- 0.77\n",
      "Episode length: 4.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 0.69     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.84     |\n",
      "|    ep_rew_mean     | 0.995    |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 2720     |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 345000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014590938 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.386        |\n",
      "|    explained_variance   | 0.117         |\n",
      "|    learning_rate        | 9.36e-05      |\n",
      "|    loss                 | 16            |\n",
      "|    n_updates            | 1792          |\n",
      "|    policy_gradient_loss | -0.00087      |\n",
      "|    value_loss           | 19.5          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.88     |\n",
      "|    ep_rew_mean     | 1.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 2729     |\n",
      "|    total_timesteps | 345600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 346000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007458076 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.374       |\n",
      "|    explained_variance   | -0.323       |\n",
      "|    learning_rate        | 9.26e-05     |\n",
      "|    loss                 | 0.115        |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    value_loss           | 0.39         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=347000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 347000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.39     |\n",
      "|    ep_rew_mean     | 0.945    |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 2745     |\n",
      "|    total_timesteps | 347136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=0.70 +/- 0.78\n",
      "Episode length: 5.40 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 0.705        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 348000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009688269 |\n",
      "|    clip_fraction        | 0.000651     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.352       |\n",
      "|    explained_variance   | -0.375       |\n",
      "|    learning_rate        | 9.17e-05     |\n",
      "|    loss                 | 0.182        |\n",
      "|    n_updates            | 1808         |\n",
      "|    policy_gradient_loss | -0.00476     |\n",
      "|    value_loss           | 0.275        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.97     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 2760     |\n",
      "|    total_timesteps | 348672   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=349000, episode_reward=1.11 +/- 0.03\n",
      "Episode length: 5.80 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 349000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027929877 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.358       |\n",
      "|    explained_variance   | -0.351       |\n",
      "|    learning_rate        | 9.08e-05     |\n",
      "|    loss                 | 0.0472       |\n",
      "|    n_updates            | 1816         |\n",
      "|    policy_gradient_loss | -0.00608     |\n",
      "|    value_loss           | 0.175        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.54     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 2775     |\n",
      "|    total_timesteps | 350208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=351000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 351000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008685322 |\n",
      "|    clip_fraction        | 0.01        |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.352      |\n",
      "|    explained_variance   | -0.238      |\n",
      "|    learning_rate        | 8.99e-05    |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 1824        |\n",
      "|    policy_gradient_loss | -0.00583    |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.7      |\n",
      "|    ep_rew_mean     | 0.992    |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 229      |\n",
      "|    time_elapsed    | 2791     |\n",
      "|    total_timesteps | 351744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 352000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000168704 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.352      |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 8.9e-05     |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 1832        |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 19.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=353000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 353000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.34     |\n",
      "|    ep_rew_mean     | -0.00667 |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 2800     |\n",
      "|    total_timesteps | 353280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.00 +/- 1.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 354000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014376427 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.339       |\n",
      "|    explained_variance   | 0.00621      |\n",
      "|    learning_rate        | 8.8e-05      |\n",
      "|    loss                 | 15.6         |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.00435     |\n",
      "|    value_loss           | 20.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.66     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 231      |\n",
      "|    time_elapsed    | 2809     |\n",
      "|    total_timesteps | 354816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.60 +/- 2.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 355000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004582729 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.389       |\n",
      "|    explained_variance   | 0.16         |\n",
      "|    learning_rate        | 8.71e-05     |\n",
      "|    loss                 | 16.3         |\n",
      "|    n_updates            | 1848         |\n",
      "|    policy_gradient_loss | -0.000443    |\n",
      "|    value_loss           | 37.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 2.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.51     |\n",
      "|    ep_rew_mean     | 0.0174   |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 2825     |\n",
      "|    total_timesteps | 356352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=357000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.20 +/- 1.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 357000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.669449e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | 0.162        |\n",
      "|    learning_rate        | 8.62e-05     |\n",
      "|    loss                 | 8.72         |\n",
      "|    n_updates            | 1856         |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 37.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.94     |\n",
      "|    ep_rew_mean     | 0.0476   |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 2839     |\n",
      "|    total_timesteps | 357888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 358000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014234435 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.384        |\n",
      "|    explained_variance   | 0.195         |\n",
      "|    learning_rate        | 8.53e-05      |\n",
      "|    loss                 | 7.75          |\n",
      "|    n_updates            | 1864          |\n",
      "|    policy_gradient_loss | -0.00104      |\n",
      "|    value_loss           | 18.5          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=359000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 359000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.5      |\n",
      "|    ep_rew_mean     | 0.987    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 2854     |\n",
      "|    total_timesteps | 359424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 360000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014911413 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.373        |\n",
      "|    explained_variance   | -1.23         |\n",
      "|    learning_rate        | 8.43e-05      |\n",
      "|    loss                 | 0.323         |\n",
      "|    n_updates            | 1872          |\n",
      "|    policy_gradient_loss | -0.00103      |\n",
      "|    value_loss           | 0.446         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.98     |\n",
      "|    ep_rew_mean     | 0.0486   |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 2869     |\n",
      "|    total_timesteps | 360960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=361000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.17\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 361000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068554113 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.403        |\n",
      "|    explained_variance   | 0.225         |\n",
      "|    learning_rate        | 8.34e-05      |\n",
      "|    loss                 | 13.5          |\n",
      "|    n_updates            | 1880          |\n",
      "|    policy_gradient_loss | -0.000609     |\n",
      "|    value_loss           | 18.3          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=1.13 +/- 0.04\n",
      "Episode length: 6.60 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 362000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.87     |\n",
      "|    ep_rew_mean     | -0.014   |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 2884     |\n",
      "|    total_timesteps | 362496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=363000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 363000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2507774e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.377        |\n",
      "|    explained_variance   | 0.173         |\n",
      "|    learning_rate        | 8.25e-05      |\n",
      "|    loss                 | 4.74          |\n",
      "|    n_updates            | 1888          |\n",
      "|    policy_gradient_loss | -0.000109     |\n",
      "|    value_loss           | 18.7          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.02     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 237      |\n",
      "|    time_elapsed    | 2899     |\n",
      "|    total_timesteps | 364032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 365000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026035577 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.394        |\n",
      "|    explained_variance   | -0.683        |\n",
      "|    learning_rate        | 8.16e-05      |\n",
      "|    loss                 | 0.147         |\n",
      "|    n_updates            | 1896          |\n",
      "|    policy_gradient_loss | -0.00153      |\n",
      "|    value_loss           | 0.337         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.55     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 2915     |\n",
      "|    total_timesteps | 365568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 366000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042412165 |\n",
      "|    clip_fraction        | 0.00317      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | -1.18        |\n",
      "|    learning_rate        | 8.07e-05     |\n",
      "|    loss                 | 0.0602       |\n",
      "|    n_updates            | 1904         |\n",
      "|    policy_gradient_loss | -0.0054      |\n",
      "|    value_loss           | 0.369        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=367000, episode_reward=0.70 +/- 0.77\n",
      "Episode length: 5.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 0.7      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 367000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.69     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 239      |\n",
      "|    time_elapsed    | 2924     |\n",
      "|    total_timesteps | 367104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 368000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002489492 |\n",
      "|    clip_fraction        | 0.00057     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.384      |\n",
      "|    explained_variance   | -1.02       |\n",
      "|    learning_rate        | 7.97e-05    |\n",
      "|    loss                 | 0.0577      |\n",
      "|    n_updates            | 1912        |\n",
      "|    policy_gradient_loss | -0.00384    |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.12     |\n",
      "|    ep_rew_mean     | 0.962    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 2933     |\n",
      "|    total_timesteps | 368640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=369000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 369000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002012564 |\n",
      "|    clip_fraction        | 0.00122     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | -0.396      |\n",
      "|    learning_rate        | 7.88e-05    |\n",
      "|    loss                 | 0.0914      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0047     |\n",
      "|    value_loss           | 0.433       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.43     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 2946     |\n",
      "|    total_timesteps | 370176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=371000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 1.07        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 371000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001344998 |\n",
      "|    clip_fraction        | 0.000407    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.415      |\n",
      "|    explained_variance   | -0.738      |\n",
      "|    learning_rate        | 7.79e-05    |\n",
      "|    loss                 | 0.0864      |\n",
      "|    n_updates            | 1928        |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.65     |\n",
      "|    ep_rew_mean     | 0.971    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 2961     |\n",
      "|    total_timesteps | 371712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 372000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024678346 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.388        |\n",
      "|    explained_variance   | 0.0972        |\n",
      "|    learning_rate        | 7.7e-05       |\n",
      "|    loss                 | 9.15          |\n",
      "|    n_updates            | 1936          |\n",
      "|    policy_gradient_loss | 0.000533      |\n",
      "|    value_loss           | 19.8          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=373000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 373000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.84     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 243      |\n",
      "|    time_elapsed    | 2976     |\n",
      "|    total_timesteps | 373248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 374000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005768149 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.415       |\n",
      "|    explained_variance   | -0.377       |\n",
      "|    learning_rate        | 7.61e-05     |\n",
      "|    loss                 | 0.055        |\n",
      "|    n_updates            | 1944         |\n",
      "|    policy_gradient_loss | -0.00252     |\n",
      "|    value_loss           | 0.228        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.73     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 2991     |\n",
      "|    total_timesteps | 374784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=1.14 +/- 0.09\n",
      "Episode length: 7.00 +/- 3.79\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7             |\n",
      "|    mean_reward          | 1.14          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 375000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046258944 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.414        |\n",
      "|    explained_variance   | 0.189         |\n",
      "|    learning_rate        | 7.51e-05      |\n",
      "|    loss                 | 26            |\n",
      "|    n_updates            | 1952          |\n",
      "|    policy_gradient_loss | -0.000551     |\n",
      "|    value_loss           | 55.8          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=1.12 +/- 0.08\n",
      "Episode length: 6.20 +/- 3.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.26     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 3007     |\n",
      "|    total_timesteps | 376320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=377000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 2.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 377000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013227718 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | -0.791       |\n",
      "|    learning_rate        | 7.42e-05     |\n",
      "|    loss                 | 0.249        |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0039      |\n",
      "|    value_loss           | 0.35         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.99     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 3021     |\n",
      "|    total_timesteps | 377856   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=378000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.60 +/- 2.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.6         |\n",
      "|    mean_reward          | 1.13        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 378000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002863453 |\n",
      "|    clip_fraction        | 0.00179     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | -0.56       |\n",
      "|    learning_rate        | 7.33e-05    |\n",
      "|    loss                 | 0.0451      |\n",
      "|    n_updates            | 1968        |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=379000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 379000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.56     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 247      |\n",
      "|    time_elapsed    | 3037     |\n",
      "|    total_timesteps | 379392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 380000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072160047 |\n",
      "|    clip_fraction        | 0.00553      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.407       |\n",
      "|    explained_variance   | -0.238       |\n",
      "|    learning_rate        | 7.24e-05     |\n",
      "|    loss                 | 0.0611       |\n",
      "|    n_updates            | 1976         |\n",
      "|    policy_gradient_loss | -0.00553     |\n",
      "|    value_loss           | 0.177        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.87     |\n",
      "|    ep_rew_mean     | 0.996    |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 3052     |\n",
      "|    total_timesteps | 380928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=381000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 381000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00066216785 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.412        |\n",
      "|    explained_variance   | 0.194         |\n",
      "|    learning_rate        | 7.14e-05      |\n",
      "|    loss                 | 2.08          |\n",
      "|    n_updates            | 1984          |\n",
      "|    policy_gradient_loss | -0.00136      |\n",
      "|    value_loss           | 18.5          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=382000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.55     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 249      |\n",
      "|    time_elapsed    | 3060     |\n",
      "|    total_timesteps | 382464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383000, episode_reward=1.16 +/- 0.08\n",
      "Episode length: 7.60 +/- 3.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | 1.16        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 383000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010732971 |\n",
      "|    clip_fraction        | 0.00928     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | -0.219      |\n",
      "|    learning_rate        | 7.05e-05    |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 1992        |\n",
      "|    policy_gradient_loss | -0.00757    |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.87     |\n",
      "|    ep_rew_mean     | 0.976    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 3070     |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 385000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048960834 |\n",
      "|    clip_fraction        | 0.00269      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | -1.01        |\n",
      "|    learning_rate        | 6.96e-05     |\n",
      "|    loss                 | 0.122        |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.00435     |\n",
      "|    value_loss           | 0.295        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.8      |\n",
      "|    ep_rew_mean     | 0.894    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 251      |\n",
      "|    time_elapsed    | 3078     |\n",
      "|    total_timesteps | 385536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=1.18 +/- 0.06\n",
      "Episode length: 8.40 +/- 2.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.4         |\n",
      "|    mean_reward          | 1.18        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 386000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006605994 |\n",
      "|    clip_fraction        | 0.00285     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.372      |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 6.87e-05    |\n",
      "|    loss                 | 0.0391      |\n",
      "|    n_updates            | 2008        |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=387000, episode_reward=0.73 +/- 0.80\n",
      "Episode length: 6.60 +/- 2.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 0.733    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 387000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.57     |\n",
      "|    ep_rew_mean     | 0.989    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 3087     |\n",
      "|    total_timesteps | 387072   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=388000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 388000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068285135 |\n",
      "|    clip_fraction        | 0.00399      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.392       |\n",
      "|    explained_variance   | -0.183       |\n",
      "|    learning_rate        | 6.78e-05     |\n",
      "|    loss                 | 0.0654       |\n",
      "|    n_updates            | 2016         |\n",
      "|    policy_gradient_loss | -0.00847     |\n",
      "|    value_loss           | 0.155        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.82     |\n",
      "|    ep_rew_mean     | 0.0248   |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 253      |\n",
      "|    time_elapsed    | 3103     |\n",
      "|    total_timesteps | 388608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=389000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 389000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009218352 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.37        |\n",
      "|    explained_variance   | 0.0704       |\n",
      "|    learning_rate        | 6.68e-05     |\n",
      "|    loss                 | 3.2          |\n",
      "|    n_updates            | 2024         |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    value_loss           | 20.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=0.71 +/- 0.81\n",
      "Episode length: 5.60 +/- 2.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 0.71     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.86     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 3118     |\n",
      "|    total_timesteps | 390144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=391000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 391000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002643023 |\n",
      "|    clip_fraction        | 0.000895    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | -0.11       |\n",
      "|    learning_rate        | 6.59e-05    |\n",
      "|    loss                 | 0.066       |\n",
      "|    n_updates            | 2032        |\n",
      "|    policy_gradient_loss | -0.00253    |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.69     |\n",
      "|    ep_rew_mean     | 0.992    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 255      |\n",
      "|    time_elapsed    | 3132     |\n",
      "|    total_timesteps | 391680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=1.15 +/- 0.06\n",
      "Episode length: 7.20 +/- 2.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 392000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023457345 |\n",
      "|    clip_fraction        | 0.00106      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.369       |\n",
      "|    explained_variance   | 0.00882      |\n",
      "|    learning_rate        | 6.5e-05      |\n",
      "|    loss                 | 5.48         |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00373     |\n",
      "|    value_loss           | 20.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=393000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 393000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.92     |\n",
      "|    ep_rew_mean     | 0.997    |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 3148     |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 2.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 394000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017785458 |\n",
      "|    clip_fraction        | 0.00187      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | 0.0217       |\n",
      "|    learning_rate        | 6.41e-05     |\n",
      "|    loss                 | 0.0604       |\n",
      "|    n_updates            | 2048         |\n",
      "|    policy_gradient_loss | -0.00273     |\n",
      "|    value_loss           | 0.158        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.4      |\n",
      "|    ep_rew_mean     | 0.965    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 257      |\n",
      "|    time_elapsed    | 3157     |\n",
      "|    total_timesteps | 394752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=0.75 +/- 0.72\n",
      "Episode length: 7.40 +/- 3.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.4         |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 395000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003423689 |\n",
      "|    clip_fraction        | 0.000326    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | -0.356      |\n",
      "|    learning_rate        | 6.31e-05    |\n",
      "|    loss                 | 0.167       |\n",
      "|    n_updates            | 2056        |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=1.14 +/- 0.04\n",
      "Episode length: 6.80 +/- 1.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.54     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 3166     |\n",
      "|    total_timesteps | 396288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=397000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 397000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00066506927 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.31         |\n",
      "|    explained_variance   | 0.0467        |\n",
      "|    learning_rate        | 6.22e-05      |\n",
      "|    loss                 | 9.4           |\n",
      "|    n_updates            | 2064          |\n",
      "|    policy_gradient_loss | 2.58e-05      |\n",
      "|    value_loss           | 20.4          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.76     |\n",
      "|    ep_rew_mean     | 0.0233   |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 259      |\n",
      "|    time_elapsed    | 3176     |\n",
      "|    total_timesteps | 397824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=1.09 +/- 0.02\n",
      "Episode length: 4.60 +/- 0.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 398000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019127887 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.354        |\n",
      "|    explained_variance   | 0.134         |\n",
      "|    learning_rate        | 6.13e-05      |\n",
      "|    loss                 | 6.33          |\n",
      "|    n_updates            | 2072          |\n",
      "|    policy_gradient_loss | -0.00169      |\n",
      "|    value_loss           | 19.2          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=399000, episode_reward=0.73 +/- 0.82\n",
      "Episode length: 6.60 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 0.733    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 399000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.74     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 3185     |\n",
      "|    total_timesteps | 399360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.40 +/- 2.24\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.4           |\n",
      "|    mean_reward          | 1.13          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 400000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016642029 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.322        |\n",
      "|    explained_variance   | -1.62         |\n",
      "|    learning_rate        | 6.04e-05      |\n",
      "|    loss                 | 0.187         |\n",
      "|    n_updates            | 2080          |\n",
      "|    policy_gradient_loss | -0.000653     |\n",
      "|    value_loss           | 0.413         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.41     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 261      |\n",
      "|    time_elapsed    | 3200     |\n",
      "|    total_timesteps | 400896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=401000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 401000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074804476 |\n",
      "|    clip_fraction        | 0.00675      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.307       |\n",
      "|    explained_variance   | -0.0756      |\n",
      "|    learning_rate        | 5.95e-05     |\n",
      "|    loss                 | 0.0274       |\n",
      "|    n_updates            | 2088         |\n",
      "|    policy_gradient_loss | -0.00803     |\n",
      "|    value_loss           | 0.0816       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=0.72 +/- 0.80\n",
      "Episode length: 6.20 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 0.724    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 402000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.33     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 3216     |\n",
      "|    total_timesteps | 402432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=403000, episode_reward=1.20 +/- 0.04\n",
      "Episode length: 9.60 +/- 1.85\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 9.6           |\n",
      "|    mean_reward          | 1.2           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 403000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025903797 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.327        |\n",
      "|    explained_variance   | 0.0856        |\n",
      "|    learning_rate        | 5.85e-05      |\n",
      "|    loss                 | 20.7          |\n",
      "|    n_updates            | 2096          |\n",
      "|    policy_gradient_loss | 0.000638      |\n",
      "|    value_loss           | 39.6          |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.79     |\n",
      "|    ep_rew_mean     | -0.056   |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 263      |\n",
      "|    time_elapsed    | 3231     |\n",
      "|    total_timesteps | 403968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 404000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002689459 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.361       |\n",
      "|    explained_variance   | 0.0732       |\n",
      "|    learning_rate        | 5.76e-05     |\n",
      "|    loss                 | 6.18         |\n",
      "|    n_updates            | 2104         |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 20           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=0.75 +/- 0.80\n",
      "Episode length: 7.20 +/- 2.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 0.748    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.68     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 3246     |\n",
      "|    total_timesteps | 405504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.8           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 406000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032993662 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.36         |\n",
      "|    explained_variance   | -0.473        |\n",
      "|    learning_rate        | 5.67e-05      |\n",
      "|    loss                 | 0.173         |\n",
      "|    n_updates            | 2112          |\n",
      "|    policy_gradient_loss | -0.000964     |\n",
      "|    value_loss           | 0.384         |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=407000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.65     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 265      |\n",
      "|    time_elapsed    | 3261     |\n",
      "|    total_timesteps | 407040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.8           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 408000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010299357 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.387        |\n",
      "|    explained_variance   | 0.114         |\n",
      "|    learning_rate        | 5.58e-05      |\n",
      "|    loss                 | 15.3          |\n",
      "|    n_updates            | 2120          |\n",
      "|    policy_gradient_loss | -0.000602     |\n",
      "|    value_loss           | 19.6          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.77     |\n",
      "|    ep_rew_mean     | 0.00357  |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 3276     |\n",
      "|    total_timesteps | 408576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=409000, episode_reward=1.17 +/- 0.07\n",
      "Episode length: 8.20 +/- 2.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8.2          |\n",
      "|    mean_reward          | 1.17         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 409000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.592072e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.343       |\n",
      "|    explained_variance   | 0.0733       |\n",
      "|    learning_rate        | 5.49e-05     |\n",
      "|    loss                 | 38.9         |\n",
      "|    n_updates            | 2128         |\n",
      "|    policy_gradient_loss | -9.95e-05    |\n",
      "|    value_loss           | 39.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.72     |\n",
      "|    ep_rew_mean     | 0.972    |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 267      |\n",
      "|    time_elapsed    | 3291     |\n",
      "|    total_timesteps | 410112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.53\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6             |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 411000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035214445 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.334        |\n",
      "|    explained_variance   | 0.039         |\n",
      "|    learning_rate        | 5.39e-05      |\n",
      "|    loss                 | 0.087         |\n",
      "|    n_updates            | 2136          |\n",
      "|    policy_gradient_loss | -0.00126      |\n",
      "|    value_loss           | 0.213         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.42     |\n",
      "|    ep_rew_mean     | -0.0248  |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 3307     |\n",
      "|    total_timesteps | 411648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=0.72 +/- 0.76\n",
      "Episode length: 6.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.2           |\n",
      "|    mean_reward          | 0.724         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 412000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012262638 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.334        |\n",
      "|    explained_variance   | 0.176         |\n",
      "|    learning_rate        | 5.3e-05       |\n",
      "|    loss                 | 11.9          |\n",
      "|    n_updates            | 2144          |\n",
      "|    policy_gradient_loss | -0.000951     |\n",
      "|    value_loss           | 37.3          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=413000, episode_reward=1.11 +/- 0.09\n",
      "Episode length: 5.80 +/- 3.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.59     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 269      |\n",
      "|    time_elapsed    | 3316     |\n",
      "|    total_timesteps | 413184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.40 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 414000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.223978e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | 0.138        |\n",
      "|    learning_rate        | 5.21e-05     |\n",
      "|    loss                 | 9.94         |\n",
      "|    n_updates            | 2152         |\n",
      "|    policy_gradient_loss | -0.000417    |\n",
      "|    value_loss           | 19.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6        |\n",
      "|    ep_rew_mean     | 0.999    |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 3325     |\n",
      "|    total_timesteps | 414720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=1.18 +/- 0.06\n",
      "Episode length: 8.40 +/- 2.42\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 8.4           |\n",
      "|    mean_reward          | 1.18          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 415000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011632475 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.375        |\n",
      "|    explained_variance   | -0.979        |\n",
      "|    learning_rate        | 5.12e-05      |\n",
      "|    loss                 | 0.431         |\n",
      "|    n_updates            | 2160          |\n",
      "|    policy_gradient_loss | -0.000739     |\n",
      "|    value_loss           | 0.816         |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=416000, episode_reward=1.11 +/- 0.07\n",
      "Episode length: 5.80 +/- 3.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 416000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.29     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 271      |\n",
      "|    time_elapsed    | 3334     |\n",
      "|    total_timesteps | 416256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 417000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001224199 |\n",
      "|    clip_fraction        | 0.000244    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.36       |\n",
      "|    explained_variance   | -0.865      |\n",
      "|    learning_rate        | 5.02e-05    |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 2168        |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.74     |\n",
      "|    ep_rew_mean     | 0.893    |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 3348     |\n",
      "|    total_timesteps | 417792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=418000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 418000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031660385 |\n",
      "|    clip_fraction        | 0.00155      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.375       |\n",
      "|    explained_variance   | -0.362       |\n",
      "|    learning_rate        | 4.93e-05     |\n",
      "|    loss                 | 0.115        |\n",
      "|    n_updates            | 2176         |\n",
      "|    policy_gradient_loss | -0.00499     |\n",
      "|    value_loss           | 0.293        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=419000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.84     |\n",
      "|    ep_rew_mean     | 0.00524  |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 273      |\n",
      "|    time_elapsed    | 3364     |\n",
      "|    total_timesteps | 419328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 420000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053699064 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.362        |\n",
      "|    explained_variance   | 0.0681        |\n",
      "|    learning_rate        | 4.84e-05      |\n",
      "|    loss                 | 8.37          |\n",
      "|    n_updates            | 2184          |\n",
      "|    policy_gradient_loss | -0.000485     |\n",
      "|    value_loss           | 20.6          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.6      |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 3379     |\n",
      "|    total_timesteps | 420864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=421000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 421000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004243739 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | -0.478       |\n",
      "|    learning_rate        | 4.75e-05     |\n",
      "|    loss                 | 0.148        |\n",
      "|    n_updates            | 2192         |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 0.211        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.00 +/- 1.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 422000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.63     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 275      |\n",
      "|    time_elapsed    | 3388     |\n",
      "|    total_timesteps | 422400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423000, episode_reward=0.33 +/- 0.95\n",
      "Episode length: 6.60 +/- 2.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 0.333        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 423000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020123613 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.368       |\n",
      "|    explained_variance   | -0.779       |\n",
      "|    learning_rate        | 4.66e-05     |\n",
      "|    loss                 | 0.15         |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.00438     |\n",
      "|    value_loss           | 0.601        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.98     |\n",
      "|    ep_rew_mean     | -0.941   |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 3397     |\n",
      "|    total_timesteps | 423936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=0.30 +/- 0.95\n",
      "Episode length: 5.40 +/- 1.85\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 0.305         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 424000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045111382 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.392        |\n",
      "|    explained_variance   | 0.0774        |\n",
      "|    learning_rate        | 4.56e-05      |\n",
      "|    loss                 | 11.4          |\n",
      "|    n_updates            | 2208          |\n",
      "|    policy_gradient_loss | -0.000466     |\n",
      "|    value_loss           | 40            |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.47     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 277      |\n",
      "|    time_elapsed    | 3405     |\n",
      "|    total_timesteps | 425472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 426000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037078487 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.351        |\n",
      "|    explained_variance   | -0.529        |\n",
      "|    learning_rate        | 4.47e-05      |\n",
      "|    loss                 | 0.0871        |\n",
      "|    n_updates            | 2216          |\n",
      "|    policy_gradient_loss | -0.00127      |\n",
      "|    value_loss           | 0.206         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=427000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.65     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 3414     |\n",
      "|    total_timesteps | 427008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 428000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020703119 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.364        |\n",
      "|    explained_variance   | -1.15         |\n",
      "|    learning_rate        | 4.38e-05      |\n",
      "|    loss                 | 0.0463        |\n",
      "|    n_updates            | 2224          |\n",
      "|    policy_gradient_loss | -0.00122      |\n",
      "|    value_loss           | 0.176         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.72     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 279      |\n",
      "|    time_elapsed    | 3423     |\n",
      "|    total_timesteps | 428544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=429000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 429000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031248343 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.361        |\n",
      "|    explained_variance   | -0.889        |\n",
      "|    learning_rate        | 4.29e-05      |\n",
      "|    loss                 | 0.0674        |\n",
      "|    n_updates            | 2232          |\n",
      "|    policy_gradient_loss | -0.00196      |\n",
      "|    value_loss           | 0.181         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.40 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.66     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 3433     |\n",
      "|    total_timesteps | 430080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=431000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 431000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024331503 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | 0.042        |\n",
      "|    learning_rate        | 4.2e-05      |\n",
      "|    loss                 | 0.0323       |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    value_loss           | 0.0777       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.95     |\n",
      "|    ep_rew_mean     | 0.978    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 281      |\n",
      "|    time_elapsed    | 3441     |\n",
      "|    total_timesteps | 431616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=0.75 +/- 0.78\n",
      "Episode length: 7.40 +/- 1.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.4          |\n",
      "|    mean_reward          | 0.752        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 432000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006400202 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.364       |\n",
      "|    explained_variance   | -0.502       |\n",
      "|    learning_rate        | 4.1e-05      |\n",
      "|    loss                 | 0.213        |\n",
      "|    n_updates            | 2248         |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    value_loss           | 0.237        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=433000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 433000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.64     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 3451     |\n",
      "|    total_timesteps | 433152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 434000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044908377 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.345        |\n",
      "|    explained_variance   | 0.293         |\n",
      "|    learning_rate        | 4.01e-05      |\n",
      "|    loss                 | 6.57          |\n",
      "|    n_updates            | 2256          |\n",
      "|    policy_gradient_loss | -0.000962     |\n",
      "|    value_loss           | 17.3          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.8      |\n",
      "|    ep_rew_mean     | 0.994    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 283      |\n",
      "|    time_elapsed    | 3460     |\n",
      "|    total_timesteps | 434688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.60 +/- 2.33\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.6           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 435000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026381898 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.382        |\n",
      "|    explained_variance   | -0.149        |\n",
      "|    learning_rate        | 3.92e-05      |\n",
      "|    loss                 | 0.0745        |\n",
      "|    n_updates            | 2264          |\n",
      "|    policy_gradient_loss | -0.00161      |\n",
      "|    value_loss           | 0.149         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=0.70 +/- 0.74\n",
      "Episode length: 5.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 0.7      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.75     |\n",
      "|    ep_rew_mean     | 0.0031   |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 3469     |\n",
      "|    total_timesteps | 436224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=437000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 437000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019190302 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.385        |\n",
      "|    explained_variance   | 0.12          |\n",
      "|    learning_rate        | 3.83e-05      |\n",
      "|    loss                 | 5.48          |\n",
      "|    n_updates            | 2272          |\n",
      "|    policy_gradient_loss | -0.000558     |\n",
      "|    value_loss           | 19.6          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.59     |\n",
      "|    ep_rew_mean     | 0.989    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 285      |\n",
      "|    time_elapsed    | 3484     |\n",
      "|    total_timesteps | 437760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 438000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004168211 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.398       |\n",
      "|    explained_variance   | -1.31        |\n",
      "|    learning_rate        | 3.73e-05     |\n",
      "|    loss                 | 0.143        |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    value_loss           | 0.307        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=439000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 439000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.29     |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 3498     |\n",
      "|    total_timesteps | 439296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.00 +/- 3.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6             |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 440000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053906866 |\n",
      "|    clip_fraction        | 0.000326      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.367        |\n",
      "|    explained_variance   | -0.451        |\n",
      "|    learning_rate        | 3.64e-05      |\n",
      "|    loss                 | 0.0582        |\n",
      "|    n_updates            | 2288          |\n",
      "|    policy_gradient_loss | -0.00359      |\n",
      "|    value_loss           | 0.131         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.13     |\n",
      "|    ep_rew_mean     | 0.0121   |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 287      |\n",
      "|    time_elapsed    | 3513     |\n",
      "|    total_timesteps | 440832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=441000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.60 +/- 2.33\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.6           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 441000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010461271 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.368        |\n",
      "|    explained_variance   | 0.163         |\n",
      "|    learning_rate        | 3.55e-05      |\n",
      "|    loss                 | 8.08          |\n",
      "|    n_updates            | 2296          |\n",
      "|    policy_gradient_loss | -0.000392     |\n",
      "|    value_loss           | 19.1          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 442000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.96     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 3529     |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443000, episode_reward=0.31 +/- 0.97\n",
      "Episode length: 5.60 +/- 1.50\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 5.6            |\n",
      "|    mean_reward          | 0.31           |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 443000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000106582804 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.4            |\n",
      "|    entropy_loss         | -0.368         |\n",
      "|    explained_variance   | 0.113          |\n",
      "|    learning_rate        | 3.46e-05       |\n",
      "|    loss                 | 13.3           |\n",
      "|    n_updates            | 2304           |\n",
      "|    policy_gradient_loss | -0.00233       |\n",
      "|    value_loss           | 19.6           |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.48     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 289      |\n",
      "|    time_elapsed    | 3544     |\n",
      "|    total_timesteps | 443904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=1.14 +/- 0.05\n",
      "Episode length: 6.80 +/- 2.14\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.8           |\n",
      "|    mean_reward          | 1.14          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 444000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025917075 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.355        |\n",
      "|    explained_variance   | -0.184        |\n",
      "|    learning_rate        | 3.37e-05      |\n",
      "|    loss                 | 0.0814        |\n",
      "|    n_updates            | 2312          |\n",
      "|    policy_gradient_loss | -0.00146      |\n",
      "|    value_loss           | 0.212         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 445000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.71     |\n",
      "|    ep_rew_mean     | 0.0621   |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 3553     |\n",
      "|    total_timesteps | 445440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=0.70 +/- 0.79\n",
      "Episode length: 5.20 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 0.7           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 446000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023137075 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.366        |\n",
      "|    explained_variance   | 0.0992        |\n",
      "|    learning_rate        | 3.27e-05      |\n",
      "|    loss                 | 12.1          |\n",
      "|    n_updates            | 2320          |\n",
      "|    policy_gradient_loss | -0.00139      |\n",
      "|    value_loss           | 19.7          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.5      |\n",
      "|    ep_rew_mean     | 0.987    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 291      |\n",
      "|    time_elapsed    | 3562     |\n",
      "|    total_timesteps | 446976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=447000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 447000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.625657e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | -0.388       |\n",
      "|    learning_rate        | 3.18e-05     |\n",
      "|    loss                 | 0.0956       |\n",
      "|    n_updates            | 2328         |\n",
      "|    policy_gradient_loss | -0.00091     |\n",
      "|    value_loss           | 0.152        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.89     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 3571     |\n",
      "|    total_timesteps | 448512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=449000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 449000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018214976 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.362       |\n",
      "|    explained_variance   | -0.382       |\n",
      "|    learning_rate        | 3.09e-05     |\n",
      "|    loss                 | 0.0933       |\n",
      "|    n_updates            | 2336         |\n",
      "|    policy_gradient_loss | -0.00338     |\n",
      "|    value_loss           | 0.166        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.40 +/- 2.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.95     |\n",
      "|    ep_rew_mean     | -0.0921  |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 3587     |\n",
      "|    total_timesteps | 450048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=451000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 2.28\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6             |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 451000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044588465 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.364        |\n",
      "|    explained_variance   | 0.108         |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 2.55          |\n",
      "|    n_updates            | 2344          |\n",
      "|    policy_gradient_loss | -0.00151      |\n",
      "|    value_loss           | 19.7          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.97     |\n",
      "|    ep_rew_mean     | 0.978    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 3601     |\n",
      "|    total_timesteps | 451584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 452000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047173744 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.385        |\n",
      "|    explained_variance   | -0.136        |\n",
      "|    learning_rate        | 2.9e-05       |\n",
      "|    loss                 | 0.0616        |\n",
      "|    n_updates            | 2352          |\n",
      "|    policy_gradient_loss | -0.0017       |\n",
      "|    value_loss           | 0.147         |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=453000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 453000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.71     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 295      |\n",
      "|    time_elapsed    | 3616     |\n",
      "|    total_timesteps | 453120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 454000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053610257 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.375        |\n",
      "|    explained_variance   | -0.0119       |\n",
      "|    learning_rate        | 2.81e-05      |\n",
      "|    loss                 | 0.0474        |\n",
      "|    n_updates            | 2360          |\n",
      "|    policy_gradient_loss | -0.00144      |\n",
      "|    value_loss           | 0.0958        |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.31     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 3632     |\n",
      "|    total_timesteps | 454656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 7.00 +/- 2.53\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7             |\n",
      "|    mean_reward          | 1.14          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 455000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022834381 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.351        |\n",
      "|    explained_variance   | -1.53         |\n",
      "|    learning_rate        | 2.72e-05      |\n",
      "|    loss                 | 0.171         |\n",
      "|    n_updates            | 2368          |\n",
      "|    policy_gradient_loss | -0.00119      |\n",
      "|    value_loss           | 0.315         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.20 +/- 1.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.66     |\n",
      "|    ep_rew_mean     | 0.041    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 297      |\n",
      "|    time_elapsed    | 3647     |\n",
      "|    total_timesteps | 456192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=457000, episode_reward=0.75 +/- 0.78\n",
      "Episode length: 7.40 +/- 2.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.4          |\n",
      "|    mean_reward          | 0.752        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 457000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.954888e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.365       |\n",
      "|    explained_variance   | 0.159        |\n",
      "|    learning_rate        | 2.63e-05     |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 2376         |\n",
      "|    policy_gradient_loss | -0.00045     |\n",
      "|    value_loss           | 19.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.93     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 3662     |\n",
      "|    total_timesteps | 457728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=1.11 +/- 0.03\n",
      "Episode length: 5.60 +/- 1.36\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.6           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 458000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046859073 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.381        |\n",
      "|    explained_variance   | 0.0581        |\n",
      "|    learning_rate        | 2.54e-05      |\n",
      "|    loss                 | 0.0605        |\n",
      "|    n_updates            | 2384          |\n",
      "|    policy_gradient_loss | -0.00175      |\n",
      "|    value_loss           | 0.167         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=459000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 459000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.09     |\n",
      "|    ep_rew_mean     | 0.0112   |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 299      |\n",
      "|    time_elapsed    | 3670     |\n",
      "|    total_timesteps | 459264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=0.71 +/- 0.78\n",
      "Episode length: 5.60 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 0.71         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 460000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.374639e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | 0.0718       |\n",
      "|    learning_rate        | 2.44e-05     |\n",
      "|    loss                 | 20.4         |\n",
      "|    n_updates            | 2392         |\n",
      "|    policy_gradient_loss | -2.12e-05    |\n",
      "|    value_loss           | 20.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.98     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 3679     |\n",
      "|    total_timesteps | 460800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=461000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.4           |\n",
      "|    mean_reward          | 1.08          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 461000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016790547 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.354        |\n",
      "|    explained_variance   | -0.691        |\n",
      "|    learning_rate        | 2.35e-05      |\n",
      "|    loss                 | 0.0591        |\n",
      "|    n_updates            | 2400          |\n",
      "|    policy_gradient_loss | -0.000922     |\n",
      "|    value_loss           | 0.186         |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=462000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.85     |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 301      |\n",
      "|    time_elapsed    | 3695     |\n",
      "|    total_timesteps | 462336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=463000, episode_reward=0.73 +/- 0.76\n",
      "Episode length: 6.40 +/- 2.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 0.729        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 463000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005314915 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | -0.973       |\n",
      "|    learning_rate        | 2.26e-05     |\n",
      "|    loss                 | 0.162        |\n",
      "|    n_updates            | 2408         |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    value_loss           | 0.331        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.8      |\n",
      "|    ep_rew_mean     | 0.954    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 3709     |\n",
      "|    total_timesteps | 463872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.40 +/- 1.36\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 464000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043882025 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.375        |\n",
      "|    explained_variance   | -1.2          |\n",
      "|    learning_rate        | 2.17e-05      |\n",
      "|    loss                 | 0.18          |\n",
      "|    n_updates            | 2416          |\n",
      "|    policy_gradient_loss | -0.00231      |\n",
      "|    value_loss           | 0.318         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=1.13 +/- 0.07\n",
      "Episode length: 6.40 +/- 2.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 465000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.1      |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 303      |\n",
      "|    time_elapsed    | 3725     |\n",
      "|    total_timesteps | 465408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=466000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 466000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003250487 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.365       |\n",
      "|    explained_variance   | -0.443       |\n",
      "|    learning_rate        | 2.08e-05     |\n",
      "|    loss                 | 0.0571       |\n",
      "|    n_updates            | 2424         |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    value_loss           | 0.115        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.61     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 304      |\n",
      "|    time_elapsed    | 3740     |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=467000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 467000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003255234 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | 0.00774      |\n",
      "|    learning_rate        | 1.98e-05     |\n",
      "|    loss                 | 0.0613       |\n",
      "|    n_updates            | 2432         |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    value_loss           | 0.117        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=0.69 +/- 0.76\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 0.69     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 468000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.86     |\n",
      "|    ep_rew_mean     | 0.976    |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 305      |\n",
      "|    time_elapsed    | 3754     |\n",
      "|    total_timesteps | 468480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=469000, episode_reward=0.75 +/- 0.74\n",
      "Episode length: 7.20 +/- 2.99\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.2           |\n",
      "|    mean_reward          | 0.748         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 469000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015200679 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.405        |\n",
      "|    explained_variance   | 0.0241        |\n",
      "|    learning_rate        | 1.89e-05      |\n",
      "|    loss                 | 0.0508        |\n",
      "|    n_updates            | 2440          |\n",
      "|    policy_gradient_loss | -0.00131      |\n",
      "|    value_loss           | 0.122         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.7      |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 3763     |\n",
      "|    total_timesteps | 470016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 471000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019600813 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.375        |\n",
      "|    explained_variance   | -1.61         |\n",
      "|    learning_rate        | 1.8e-05       |\n",
      "|    loss                 | 0.161         |\n",
      "|    n_updates            | 2448          |\n",
      "|    policy_gradient_loss | -0.000715     |\n",
      "|    value_loss           | 0.213         |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.52     |\n",
      "|    ep_rew_mean     | -0.0224  |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 307      |\n",
      "|    time_elapsed    | 3772     |\n",
      "|    total_timesteps | 471552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=1.11 +/- 0.02\n",
      "Episode length: 5.60 +/- 1.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 472000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.111855e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.352       |\n",
      "|    explained_variance   | 0.186        |\n",
      "|    learning_rate        | 1.71e-05     |\n",
      "|    loss                 | 1.54         |\n",
      "|    n_updates            | 2456         |\n",
      "|    policy_gradient_loss | -0.000174    |\n",
      "|    value_loss           | 18.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=473000, episode_reward=0.75 +/- 0.77\n",
      "Episode length: 7.40 +/- 2.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 0.752    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 473000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.85     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 3782     |\n",
      "|    total_timesteps | 473088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=0.75 +/- 0.76\n",
      "Episode length: 7.20 +/- 2.32\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 0.748        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 474000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001215135 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.38        |\n",
      "|    explained_variance   | -0.364       |\n",
      "|    learning_rate        | 1.61e-05     |\n",
      "|    loss                 | 0.1          |\n",
      "|    n_updates            | 2464         |\n",
      "|    policy_gradient_loss | -0.000978    |\n",
      "|    value_loss           | 0.146        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.93     |\n",
      "|    ep_rew_mean     | -0.963   |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 309      |\n",
      "|    time_elapsed    | 3790     |\n",
      "|    total_timesteps | 474624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=0.73 +/- 0.79\n",
      "Episode length: 6.60 +/- 3.56\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.6           |\n",
      "|    mean_reward          | 0.733         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 475000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8662383e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.372        |\n",
      "|    explained_variance   | 0.039         |\n",
      "|    learning_rate        | 1.52e-05      |\n",
      "|    loss                 | 6.12          |\n",
      "|    n_updates            | 2472          |\n",
      "|    policy_gradient_loss | -0.000268     |\n",
      "|    value_loss           | 41            |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 476000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.01     |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 3799     |\n",
      "|    total_timesteps | 476160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=477000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 477000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6851116e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.38         |\n",
      "|    explained_variance   | -0.127        |\n",
      "|    learning_rate        | 1.43e-05      |\n",
      "|    loss                 | 0.0418        |\n",
      "|    n_updates            | 2480          |\n",
      "|    policy_gradient_loss | -0.00043      |\n",
      "|    value_loss           | 0.0948        |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.54     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 311      |\n",
      "|    time_elapsed    | 3808     |\n",
      "|    total_timesteps | 477696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=478000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 478000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000125332 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | -0.0654     |\n",
      "|    learning_rate        | 1.34e-05    |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 2488        |\n",
      "|    policy_gradient_loss | -0.000947   |\n",
      "|    value_loss           | 0.0895      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=479000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 479000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.83     |\n",
      "|    ep_rew_mean     | 0.955    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 3816     |\n",
      "|    total_timesteps | 479232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=1.13 +/- 0.07\n",
      "Episode length: 6.40 +/- 3.01\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.4           |\n",
      "|    mean_reward          | 1.13          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 480000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5601088e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.379        |\n",
      "|    explained_variance   | 0.291         |\n",
      "|    learning_rate        | 1.25e-05      |\n",
      "|    loss                 | 3.68          |\n",
      "|    n_updates            | 2496          |\n",
      "|    policy_gradient_loss | -0.00022      |\n",
      "|    value_loss           | 17.5          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.68     |\n",
      "|    ep_rew_mean     | 0.991    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 3825     |\n",
      "|    total_timesteps | 480768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=481000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.00 +/- 1.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 481000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.434403e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.363       |\n",
      "|    explained_variance   | -0.133       |\n",
      "|    learning_rate        | 1.15e-05     |\n",
      "|    loss                 | 0.0605       |\n",
      "|    n_updates            | 2504         |\n",
      "|    policy_gradient_loss | -0.000413    |\n",
      "|    value_loss           | 0.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=482000, episode_reward=0.70 +/- 0.78\n",
      "Episode length: 5.40 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 0.705    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 482000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.49     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 3834     |\n",
      "|    total_timesteps | 482304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.2           |\n",
      "|    mean_reward          | 1.08          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 483000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014349462 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.341        |\n",
      "|    explained_variance   | -0.279        |\n",
      "|    learning_rate        | 1.06e-05      |\n",
      "|    loss                 | 0.0435        |\n",
      "|    n_updates            | 2512          |\n",
      "|    policy_gradient_loss | -0.000837     |\n",
      "|    value_loss           | 0.132         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.78     |\n",
      "|    ep_rew_mean     | 0.994    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 315      |\n",
      "|    time_elapsed    | 3850     |\n",
      "|    total_timesteps | 483840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.20 +/- 1.94\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 484000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014338545 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.386        |\n",
      "|    explained_variance   | -0.0104       |\n",
      "|    learning_rate        | 9.7e-06       |\n",
      "|    loss                 | 0.0418        |\n",
      "|    n_updates            | 2520          |\n",
      "|    policy_gradient_loss | -0.000985     |\n",
      "|    value_loss           | 0.155         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 485000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.89     |\n",
      "|    ep_rew_mean     | 0.976    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 316      |\n",
      "|    time_elapsed    | 3864     |\n",
      "|    total_timesteps | 485376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=486000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 486000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 2.16049e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.349      |\n",
      "|    explained_variance   | -0.683      |\n",
      "|    learning_rate        | 8.77e-06    |\n",
      "|    loss                 | 0.0805      |\n",
      "|    n_updates            | 2528        |\n",
      "|    policy_gradient_loss | -0.000481   |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.6      |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 317      |\n",
      "|    time_elapsed    | 3879     |\n",
      "|    total_timesteps | 486912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=487000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 487000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.349579e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.368       |\n",
      "|    explained_variance   | 0.00295      |\n",
      "|    learning_rate        | 7.85e-06     |\n",
      "|    loss                 | 13           |\n",
      "|    n_updates            | 2536         |\n",
      "|    policy_gradient_loss | -0.000255    |\n",
      "|    value_loss           | 20.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 488000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.53     |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 3889     |\n",
      "|    total_timesteps | 488448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=489000, episode_reward=1.17 +/- 0.06\n",
      "Episode length: 8.20 +/- 2.32\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 8.2           |\n",
      "|    mean_reward          | 1.17          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 489000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4929954e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.362        |\n",
      "|    explained_variance   | 0.0296        |\n",
      "|    learning_rate        | 6.93e-06      |\n",
      "|    loss                 | 0.0397        |\n",
      "|    n_updates            | 2544          |\n",
      "|    policy_gradient_loss | -0.000497     |\n",
      "|    value_loss           | 0.103         |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.97     |\n",
      "|    ep_rew_mean     | 0.998    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 319      |\n",
      "|    time_elapsed    | 3901     |\n",
      "|    total_timesteps | 489984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 490000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.914077e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | 0.171        |\n",
      "|    learning_rate        | 6.01e-06     |\n",
      "|    loss                 | 9.59         |\n",
      "|    n_updates            | 2552         |\n",
      "|    policy_gradient_loss | -0.000376    |\n",
      "|    value_loss           | 19           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=491000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 491000   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 5.6       |\n",
      "|    ep_rew_mean     | -0.000476 |\n",
      "| time/              |           |\n",
      "|    fps             | 125       |\n",
      "|    iterations      | 320       |\n",
      "|    time_elapsed    | 3916      |\n",
      "|    total_timesteps | 491520    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 492000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6394382e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.377        |\n",
      "|    explained_variance   | 0.044         |\n",
      "|    learning_rate        | 5.09e-06      |\n",
      "|    loss                 | 32            |\n",
      "|    n_updates            | 2560          |\n",
      "|    policy_gradient_loss | -0.000154     |\n",
      "|    value_loss           | 40.7          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=493000, episode_reward=0.69 +/- 0.80\n",
      "Episode length: 4.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 0.69     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 493000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.69     |\n",
      "|    ep_rew_mean     | 0.992    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 321      |\n",
      "|    time_elapsed    | 3931     |\n",
      "|    total_timesteps | 493056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=494000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 494000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 5.99504e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | -0.258      |\n",
      "|    learning_rate        | 4.17e-06    |\n",
      "|    loss                 | 0.208       |\n",
      "|    n_updates            | 2568        |\n",
      "|    policy_gradient_loss | -0.000193   |\n",
      "|    value_loss           | 0.268       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.65     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 322      |\n",
      "|    time_elapsed    | 3946     |\n",
      "|    total_timesteps | 494592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 495000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2986281e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.34         |\n",
      "|    explained_variance   | -0.136        |\n",
      "|    learning_rate        | 3.24e-06      |\n",
      "|    loss                 | 0.0558        |\n",
      "|    n_updates            | 2576          |\n",
      "|    policy_gradient_loss | -0.000104     |\n",
      "|    value_loss           | 0.0937        |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.40 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 496000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.55     |\n",
      "|    ep_rew_mean     | 1.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 3962     |\n",
      "|    total_timesteps | 496128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=497000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.50\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 497000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0689255e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.349        |\n",
      "|    explained_variance   | 0.0693        |\n",
      "|    learning_rate        | 2.32e-06      |\n",
      "|    loss                 | 7.36          |\n",
      "|    n_updates            | 2584          |\n",
      "|    policy_gradient_loss | -0.000137     |\n",
      "|    value_loss           | 20.2          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 6.01     |\n",
      "|    ep_rew_mean     | 0.899    |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 3977     |\n",
      "|    total_timesteps | 497664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=1.11 +/- 0.08\n",
      "Episode length: 5.60 +/- 3.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.6           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 498000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1269155e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.382        |\n",
      "|    explained_variance   | 0.012         |\n",
      "|    learning_rate        | 1.4e-06       |\n",
      "|    loss                 | 11.4          |\n",
      "|    n_updates            | 2592          |\n",
      "|    policy_gradient_loss | -6.2e-05      |\n",
      "|    value_loss           | 20.8          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=499000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 499000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.8      |\n",
      "|    ep_rew_mean     | 1.03     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 325      |\n",
      "|    time_elapsed    | 3992     |\n",
      "|    total_timesteps | 499200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 2.73576e-08 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.335      |\n",
      "|    explained_variance   | -0.112      |\n",
      "|    learning_rate        | 4.8e-07     |\n",
      "|    loss                 | 0.0458      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -1.82e-05   |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5.34     |\n",
      "|    ep_rew_mean     | 1.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 4007     |\n",
      "|    total_timesteps | 500736   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fa898cc1520>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for a large number of timesteps\n",
    "agent.learn(total_timesteps=500000,\n",
    "            reset_num_timesteps = True,\n",
    "           callback = eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67d54869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7549/3286586875.py:5: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(evaluation_log_df['timesteps'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_log = np.load(eval_log_path + '/evaluations.npz')\n",
    "evaluation_log_df = pd.DataFrame({item: [np.mean(ep) for ep in evaluation_log[item]] for item in evaluation_log.files})\n",
    "ax = evaluation_log_df.loc[0:len(evaluation_log_df), 'results'].plot(color = 'lightgray', xlim = [-5, len(evaluation_log_df)], figsize = (10,5))\n",
    "evaluation_log_df['results'].rolling(5).mean().plot(color = 'black', xlim = [-5, len(evaluation_log_df)])\n",
    "ax.set_xticklabels(evaluation_log_df['timesteps'])\n",
    "ax.set_xlabel(\"Eval Episode\")\n",
    "plt.ylabel(\"Rolling Mean Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a7dd93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 1.1031748333333333 +/- 0.048535941233333026\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the agent in the environment for 30 episodes\n",
    "agent.set_env(eval_env)\n",
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent, \n",
    "                                                                agent.get_env(), \n",
    "                                                                n_eval_episodes=30)\n",
    "\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce20db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = ConnectFourGym(agent2=\"negamax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37caff02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pelin/Desktop/advancedml/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:30: UserWarning: It seems that your observation  is an image but its `dtype` is (int64) whereas it has to be `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\n",
      "  warnings.warn(\n",
      "/home/pelin/Desktop/advancedml/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:38: UserWarning: It seems that your observation space  is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
      "  warnings.warn(\n",
      "/home/pelin/Desktop/advancedml/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:51: UserWarning: The minimal resolution for an image is 36x36 for the default `CnnPolicy`. You might need to use a custom features extractor cf. https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "check_env(env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b6766a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = Monitor(env2)\n",
    "env2 = DummyVecEnv([lambda: env2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24631689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an evaluation callback that is called every at regular intervals and renders the episode\n",
    "eval_log_path2 = './log_connectx2'\n",
    "eval_env2= Monitor(Monitor(ConnectFourGym()))\n",
    "\n",
    "eval_env2=DummyVecEnv([lambda:eval_env2])\n",
    "\n",
    "eval_callback2 = EvalCallback(eval_env2 , \n",
    "                              best_model_save_path=eval_log_path2 ,\n",
    "                              log_path=eval_log_path2 , \n",
    "                              eval_freq=1000,\n",
    "                              render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0807c46d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=501000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 501000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=502000, episode_reward=0.75 +/- 0.79\n",
      "Episode length: 7.20 +/- 1.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 0.748    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 502000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.1      |\n",
      "|    ep_rew_mean     | -3.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 502272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=503000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 503000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001269187 |\n",
      "|    clip_fraction        | 0.000488    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.326      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.000149    |\n",
      "|    loss                 | 31.7        |\n",
      "|    n_updates            | 2616        |\n",
      "|    policy_gradient_loss | -0.00461    |\n",
      "|    value_loss           | 77.7        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.14     |\n",
      "|    ep_rew_mean     | -1.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 503808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 504000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.637687e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.343       |\n",
      "|    explained_variance   | 0.088        |\n",
      "|    learning_rate        | 0.000149     |\n",
      "|    loss                 | 14.5         |\n",
      "|    n_updates            | 2624         |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    value_loss           | 58.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 505000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.83     |\n",
      "|    ep_rew_mean     | -0.817   |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 505344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=506000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 506000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00071431487 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.341        |\n",
      "|    explained_variance   | 0.227         |\n",
      "|    learning_rate        | 0.000149      |\n",
      "|    loss                 | 13.2          |\n",
      "|    n_updates            | 2632          |\n",
      "|    policy_gradient_loss | -0.00405      |\n",
      "|    value_loss           | 35.5          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.96     |\n",
      "|    ep_rew_mean     | -0.754   |\n",
      "| time/              |          |\n",
      "|    fps             | 23       |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 506880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=507000, episode_reward=0.72 +/- 0.77\n",
      "Episode length: 6.20 +/- 2.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 0.724        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 507000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009807573 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.319       |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.000148     |\n",
      "|    loss                 | 3.46         |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    value_loss           | 18.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 508000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.26     |\n",
      "|    ep_rew_mean     | -2.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 312      |\n",
      "|    total_timesteps | 508416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=509000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 509000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006890146 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 0.168        |\n",
      "|    learning_rate        | 0.000148     |\n",
      "|    loss                 | 36           |\n",
      "|    n_updates            | 2648         |\n",
      "|    policy_gradient_loss | -0.00409     |\n",
      "|    value_loss           | 72.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.66     |\n",
      "|    ep_rew_mean     | -1.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 406      |\n",
      "|    total_timesteps | 509952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.8           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 510000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027774545 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.341        |\n",
      "|    explained_variance   | 0.241         |\n",
      "|    learning_rate        | 0.000147      |\n",
      "|    loss                 | 14.8          |\n",
      "|    n_updates            | 2656          |\n",
      "|    policy_gradient_loss | -0.00321      |\n",
      "|    value_loss           | 67.3          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=511000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 511000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.22     |\n",
      "|    ep_rew_mean     | -2.75    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 500      |\n",
      "|    total_timesteps | 511488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 1.08        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 512000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.93176e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.335      |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.000147    |\n",
      "|    loss                 | 39.9        |\n",
      "|    n_updates            | 2664        |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    value_loss           | 84.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=513000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 2.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 513000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.67     |\n",
      "|    ep_rew_mean     | -0.801   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 595      |\n",
      "|    total_timesteps | 513024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514000, episode_reward=1.13 +/- 0.03\n",
      "Episode length: 6.60 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.6        |\n",
      "|    mean_reward          | 1.13       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 514000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00068498 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.323     |\n",
      "|    explained_variance   | 0.204      |\n",
      "|    learning_rate        | 0.000146   |\n",
      "|    loss                 | 10.9       |\n",
      "|    n_updates            | 2672       |\n",
      "|    policy_gradient_loss | -0.00269   |\n",
      "|    value_loss           | 19.1       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.32     |\n",
      "|    ep_rew_mean     | -0.726   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 688      |\n",
      "|    total_timesteps | 514560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=1.15 +/- 0.05\n",
      "Episode length: 7.20 +/- 1.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 515000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015901251 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.326       |\n",
      "|    explained_variance   | 0.168        |\n",
      "|    learning_rate        | 0.000146     |\n",
      "|    loss                 | 9.6          |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    value_loss           | 19.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.00 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 516000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.68     |\n",
      "|    ep_rew_mean     | -1.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 746      |\n",
      "|    total_timesteps | 516096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=517000, episode_reward=0.70 +/- 0.76\n",
      "Episode length: 5.00 +/- 1.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 517000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001971885 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.342       |\n",
      "|    explained_variance   | 0.272        |\n",
      "|    learning_rate        | 0.000145     |\n",
      "|    loss                 | 6.43         |\n",
      "|    n_updates            | 2688         |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 34.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.17     |\n",
      "|    ep_rew_mean     | -0.749   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 800      |\n",
      "|    total_timesteps | 517632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=518000, episode_reward=0.71 +/- 0.80\n",
      "Episode length: 5.60 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 0.71         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 518000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030542982 |\n",
      "|    clip_fraction        | 0.00155      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.317       |\n",
      "|    explained_variance   | 0.22         |\n",
      "|    learning_rate        | 0.000145     |\n",
      "|    loss                 | 18.9         |\n",
      "|    n_updates            | 2696         |\n",
      "|    policy_gradient_loss | -0.00741     |\n",
      "|    value_loss           | 35.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=519000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 519000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.83     |\n",
      "|    ep_rew_mean     | -0.757   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 854      |\n",
      "|    total_timesteps | 519168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 520000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027043873 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.327       |\n",
      "|    explained_variance   | 0.219        |\n",
      "|    learning_rate        | 0.000144     |\n",
      "|    loss                 | 5.01         |\n",
      "|    n_updates            | 2704         |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    value_loss           | 19.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.78     |\n",
      "|    ep_rew_mean     | -0.759   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 909      |\n",
      "|    total_timesteps | 520704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=521000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 521000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016259606 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.334       |\n",
      "|    explained_variance   | 0.176        |\n",
      "|    learning_rate        | 0.000144     |\n",
      "|    loss                 | 17.4         |\n",
      "|    n_updates            | 2712         |\n",
      "|    policy_gradient_loss | -0.00488     |\n",
      "|    value_loss           | 19.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=522000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 522000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.77     |\n",
      "|    ep_rew_mean     | -1.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 964      |\n",
      "|    total_timesteps | 522240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=523000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.20 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 523000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000805825 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.339      |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.000143    |\n",
      "|    loss                 | 17          |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.00138    |\n",
      "|    value_loss           | 34.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 1017     |\n",
      "|    total_timesteps | 523776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 524000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021703558 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.344       |\n",
      "|    explained_variance   | 0.327        |\n",
      "|    learning_rate        | 0.000143     |\n",
      "|    loss                 | 21.3         |\n",
      "|    n_updates            | 2728         |\n",
      "|    policy_gradient_loss | -0.00669     |\n",
      "|    value_loss           | 33.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.20 +/- 2.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 525000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.97     |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 1071     |\n",
      "|    total_timesteps | 525312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=526000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 526000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011525052 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 0.247        |\n",
      "|    learning_rate        | 0.000143     |\n",
      "|    loss                 | 31           |\n",
      "|    n_updates            | 2736         |\n",
      "|    policy_gradient_loss | -0.00329     |\n",
      "|    value_loss           | 35.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.1      |\n",
      "|    ep_rew_mean     | -0.771   |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 1164     |\n",
      "|    total_timesteps | 526848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=527000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 527000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006181015 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.349       |\n",
      "|    explained_variance   | 0.284        |\n",
      "|    learning_rate        | 0.000142     |\n",
      "|    loss                 | 11.8         |\n",
      "|    n_updates            | 2744         |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    value_loss           | 34.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 528000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.92     |\n",
      "|    ep_rew_mean     | -1.81    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 1258     |\n",
      "|    total_timesteps | 528384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=529000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.00 +/- 1.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 529000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006988612 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.348       |\n",
      "|    explained_variance   | 0.298        |\n",
      "|    learning_rate        | 0.000142     |\n",
      "|    loss                 | 34.3         |\n",
      "|    n_updates            | 2752         |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 49.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.1      |\n",
      "|    ep_rew_mean     | -2.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 1351     |\n",
      "|    total_timesteps | 529920   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=530000, episode_reward=1.09 +/- 0.02\n",
      "Episode length: 4.80 +/- 0.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 530000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011574447 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.338       |\n",
      "|    explained_variance   | 0.283        |\n",
      "|    learning_rate        | 0.000141     |\n",
      "|    loss                 | 14.2         |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.00414     |\n",
      "|    value_loss           | 35.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=531000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 531000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.05     |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 1443     |\n",
      "|    total_timesteps | 531456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 1.90\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6             |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 532000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00041442513 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.357        |\n",
      "|    explained_variance   | 0.307         |\n",
      "|    learning_rate        | 0.000141      |\n",
      "|    loss                 | 32.4          |\n",
      "|    n_updates            | 2768          |\n",
      "|    policy_gradient_loss | -0.00177      |\n",
      "|    value_loss           | 82            |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.19     |\n",
      "|    ep_rew_mean     | -1.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 1497     |\n",
      "|    total_timesteps | 532992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=533000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 533000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016091615 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.332       |\n",
      "|    explained_variance   | 0.213        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | 12.2         |\n",
      "|    n_updates            | 2776         |\n",
      "|    policy_gradient_loss | -0.0055      |\n",
      "|    value_loss           | 37.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=534000, episode_reward=0.70 +/- 0.80\n",
      "Episode length: 5.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.695    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 534000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.15     |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 1551     |\n",
      "|    total_timesteps | 534528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=0.71 +/- 0.78\n",
      "Episode length: 5.60 +/- 1.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 0.71         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 535000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028620826 |\n",
      "|    clip_fraction        | 0.00138      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.339       |\n",
      "|    explained_variance   | 0.23         |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | 4.55         |\n",
      "|    n_updates            | 2784         |\n",
      "|    policy_gradient_loss | -0.00927     |\n",
      "|    value_loss           | 36.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 2.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 536000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.11     |\n",
      "|    ep_rew_mean     | -0.711   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 1644     |\n",
      "|    total_timesteps | 536064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=537000, episode_reward=1.13 +/- 0.07\n",
      "Episode length: 6.60 +/- 2.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 537000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024619491 |\n",
      "|    clip_fraction        | 0.000814     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.345       |\n",
      "|    explained_variance   | 0.211        |\n",
      "|    learning_rate        | 0.000139     |\n",
      "|    loss                 | 10.7         |\n",
      "|    n_updates            | 2792         |\n",
      "|    policy_gradient_loss | -0.00649     |\n",
      "|    value_loss           | 19.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -0.695   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 1738     |\n",
      "|    total_timesteps | 537600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=538000, episode_reward=0.72 +/- 0.77\n",
      "Episode length: 6.00 +/- 2.45\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6             |\n",
      "|    mean_reward          | 0.719         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 538000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00055776426 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.347        |\n",
      "|    explained_variance   | 0.164         |\n",
      "|    learning_rate        | 0.000139      |\n",
      "|    loss                 | 13.7          |\n",
      "|    n_updates            | 2800          |\n",
      "|    policy_gradient_loss | -0.00193      |\n",
      "|    value_loss           | 21.2          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=539000, episode_reward=1.09 +/- 0.02\n",
      "Episode length: 4.60 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 539000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.82     |\n",
      "|    ep_rew_mean     | -0.778   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 1832     |\n",
      "|    total_timesteps | 539136   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=540000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 540000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053017883 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.386        |\n",
      "|    explained_variance   | 0.0775        |\n",
      "|    learning_rate        | 0.000138      |\n",
      "|    loss                 | 4.95          |\n",
      "|    n_updates            | 2808          |\n",
      "|    policy_gradient_loss | -0.00228      |\n",
      "|    value_loss           | 22.2          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.62     |\n",
      "|    ep_rew_mean     | -0.642   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 1926     |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=541000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 541000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010070271 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | -1.44        |\n",
      "|    learning_rate        | 0.000138     |\n",
      "|    loss                 | 1.65         |\n",
      "|    n_updates            | 2816         |\n",
      "|    policy_gradient_loss | -0.00268     |\n",
      "|    value_loss           | 2.85         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=542000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.40 +/- 2.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 542000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.29     |\n",
      "|    ep_rew_mean     | -5.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 2018     |\n",
      "|    total_timesteps | 542208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=543000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 543000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003630039 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.394       |\n",
      "|    explained_variance   | 0.281        |\n",
      "|    learning_rate        | 0.000137     |\n",
      "|    loss                 | 61.4         |\n",
      "|    n_updates            | 2824         |\n",
      "|    policy_gradient_loss | -0.000227    |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.16     |\n",
      "|    ep_rew_mean     | -1.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 2072     |\n",
      "|    total_timesteps | 543744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=0.70 +/- 0.79\n",
      "Episode length: 5.40 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 0.705         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 544000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00078186457 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.38         |\n",
      "|    explained_variance   | 0.224         |\n",
      "|    learning_rate        | 0.000137      |\n",
      "|    loss                 | 14.2          |\n",
      "|    n_updates            | 2832          |\n",
      "|    policy_gradient_loss | -0.00286      |\n",
      "|    value_loss           | 36.3          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 545000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.95     |\n",
      "|    ep_rew_mean     | -2.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 2159     |\n",
      "|    total_timesteps | 545280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=546000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 546000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008905428 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.365       |\n",
      "|    explained_variance   | 0.341        |\n",
      "|    learning_rate        | 0.000137     |\n",
      "|    loss                 | 20.6         |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    value_loss           | 48.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.14     |\n",
      "|    ep_rew_mean     | -1.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 2252     |\n",
      "|    total_timesteps | 546816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=547000, episode_reward=1.20 +/- 0.04\n",
      "Episode length: 9.20 +/- 1.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.2         |\n",
      "|    mean_reward          | 1.2         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 547000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001485003 |\n",
      "|    clip_fraction        | 0.000488    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.369      |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.000136    |\n",
      "|    loss                 | 25.3        |\n",
      "|    n_updates            | 2848        |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    value_loss           | 36.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 548000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.41     |\n",
      "|    ep_rew_mean     | -1.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 2346     |\n",
      "|    total_timesteps | 548352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=549000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 1.07        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 549000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002108898 |\n",
      "|    clip_fraction        | 0.000163    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.000136    |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 2856        |\n",
      "|    policy_gradient_loss | -0.0036     |\n",
      "|    value_loss           | 37.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | -1.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 2400     |\n",
      "|    total_timesteps | 549888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=1.18 +/- 0.12\n",
      "Episode length: 8.40 +/- 4.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.4         |\n",
      "|    mean_reward          | 1.18        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 550000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001428072 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.000135    |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 2864        |\n",
      "|    policy_gradient_loss | -0.0032     |\n",
      "|    value_loss           | 47.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=551000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 551000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.92     |\n",
      "|    ep_rew_mean     | -1.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 2455     |\n",
      "|    total_timesteps | 551424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.40 +/- 0.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 552000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014431499 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.382       |\n",
      "|    explained_variance   | 0.285        |\n",
      "|    learning_rate        | 0.000135     |\n",
      "|    loss                 | 18.6         |\n",
      "|    n_updates            | 2872         |\n",
      "|    policy_gradient_loss | -0.00411     |\n",
      "|    value_loss           | 36.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | -1.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 2509     |\n",
      "|    total_timesteps | 552960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=553000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 2.28\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6             |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 553000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00069805747 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.339        |\n",
      "|    explained_variance   | 0.342         |\n",
      "|    learning_rate        | 0.000134      |\n",
      "|    loss                 | 22.8          |\n",
      "|    n_updates            | 2880          |\n",
      "|    policy_gradient_loss | -0.00264      |\n",
      "|    value_loss           | 33.1          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=554000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 554000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.78     |\n",
      "|    ep_rew_mean     | -0.719   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 2565     |\n",
      "|    total_timesteps | 554496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 555000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007512788 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | -1.34        |\n",
      "|    learning_rate        | 0.000134     |\n",
      "|    loss                 | 1.5          |\n",
      "|    n_updates            | 2888         |\n",
      "|    policy_gradient_loss | -0.00317     |\n",
      "|    value_loss           | 5.42         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 556000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 2619     |\n",
      "|    total_timesteps | 556032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=557000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.00 +/- 1.55\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 557000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00092399615 |\n",
      "|    clip_fraction        | 0.000407      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.362        |\n",
      "|    explained_variance   | 0.345         |\n",
      "|    learning_rate        | 0.000133      |\n",
      "|    loss                 | 5.14          |\n",
      "|    n_updates            | 2896          |\n",
      "|    policy_gradient_loss | -0.00279      |\n",
      "|    value_loss           | 17            |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | -1.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 2714     |\n",
      "|    total_timesteps | 557568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=558000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 558000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048790072 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.363        |\n",
      "|    explained_variance   | 0.363         |\n",
      "|    learning_rate        | 0.000133      |\n",
      "|    loss                 | 21.1          |\n",
      "|    n_updates            | 2904          |\n",
      "|    policy_gradient_loss | -0.0023       |\n",
      "|    value_loss           | 31.8          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=559000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.40 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 559000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.06     |\n",
      "|    ep_rew_mean     | -0.792   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 2808     |\n",
      "|    total_timesteps | 559104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=0.70 +/- 0.80\n",
      "Episode length: 5.00 +/- 1.55\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.695         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 560000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036185968 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.357        |\n",
      "|    explained_variance   | 0.262         |\n",
      "|    learning_rate        | 0.000132      |\n",
      "|    loss                 | 28.2          |\n",
      "|    n_updates            | 2912          |\n",
      "|    policy_gradient_loss | -0.00212      |\n",
      "|    value_loss           | 35.8          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -1.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 2903     |\n",
      "|    total_timesteps | 560640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=561000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 561000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001764169 |\n",
      "|    clip_fraction        | 0.000977    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.332      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.000132    |\n",
      "|    loss                 | 4.03        |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.00694    |\n",
      "|    value_loss           | 31.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=562000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 562000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.51     |\n",
      "|    ep_rew_mean     | -1.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 2957     |\n",
      "|    total_timesteps | 562176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=563000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 563000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010180457 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.392        |\n",
      "|    learning_rate        | 0.000131     |\n",
      "|    loss                 | 12.1         |\n",
      "|    n_updates            | 2928         |\n",
      "|    policy_gradient_loss | -0.00475     |\n",
      "|    value_loss           | 31.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.38     |\n",
      "|    ep_rew_mean     | -1.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 3011     |\n",
      "|    total_timesteps | 563712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 564000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023850913 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.352       |\n",
      "|    explained_variance   | 0.313        |\n",
      "|    learning_rate        | 0.000131     |\n",
      "|    loss                 | 8.92         |\n",
      "|    n_updates            | 2936         |\n",
      "|    policy_gradient_loss | -0.00436     |\n",
      "|    value_loss           | 17.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 565000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.85     |\n",
      "|    ep_rew_mean     | -1.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 3066     |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=566000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 566000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009576262 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | 0.328        |\n",
      "|    learning_rate        | 0.000131     |\n",
      "|    loss                 | 20.6         |\n",
      "|    n_updates            | 2944         |\n",
      "|    policy_gradient_loss | -0.00362     |\n",
      "|    value_loss           | 34           |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.58     |\n",
      "|    ep_rew_mean     | -1.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 3159     |\n",
      "|    total_timesteps | 566784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=567000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 567000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047621015 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.352        |\n",
      "|    explained_variance   | 0.336         |\n",
      "|    learning_rate        | 0.00013       |\n",
      "|    loss                 | 30.2          |\n",
      "|    n_updates            | 2952          |\n",
      "|    policy_gradient_loss | -0.00173      |\n",
      "|    value_loss           | 49.8          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=0.73 +/- 0.78\n",
      "Episode length: 6.40 +/- 2.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 0.729    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 568000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.96     |\n",
      "|    ep_rew_mean     | -0.674   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 3254     |\n",
      "|    total_timesteps | 568320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=569000, episode_reward=1.14 +/- 0.07\n",
      "Episode length: 7.00 +/- 2.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7            |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 569000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006846444 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.342       |\n",
      "|    explained_variance   | 0.382        |\n",
      "|    learning_rate        | 0.00013      |\n",
      "|    loss                 | 7.17         |\n",
      "|    n_updates            | 2960         |\n",
      "|    policy_gradient_loss | -0.00436     |\n",
      "|    value_loss           | 16.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.09     |\n",
      "|    ep_rew_mean     | -1.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 3346     |\n",
      "|    total_timesteps | 569856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.20 +/- 1.83\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.2           |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 570000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036986868 |\n",
      "|    clip_fraction        | 0.000163      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.322        |\n",
      "|    explained_variance   | 0.429         |\n",
      "|    learning_rate        | 0.000129      |\n",
      "|    loss                 | 8.17          |\n",
      "|    n_updates            | 2968          |\n",
      "|    policy_gradient_loss | -0.00247      |\n",
      "|    value_loss           | 30.2          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=571000, episode_reward=1.14 +/- 0.09\n",
      "Episode length: 6.80 +/- 3.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 571000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.82     |\n",
      "|    ep_rew_mean     | -0.758   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 3441     |\n",
      "|    total_timesteps | 571392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=0.71 +/- 0.80\n",
      "Episode length: 5.80 +/- 2.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 0.714        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 572000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018775653 |\n",
      "|    clip_fraction        | 0.00057      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.326       |\n",
      "|    explained_variance   | -2.84        |\n",
      "|    learning_rate        | 0.000129     |\n",
      "|    loss                 | 0.556        |\n",
      "|    n_updates            | 2976         |\n",
      "|    policy_gradient_loss | -0.00415     |\n",
      "|    value_loss           | 1.62         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.42     |\n",
      "|    ep_rew_mean     | -1.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 3534     |\n",
      "|    total_timesteps | 572928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=573000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 573000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002229459 |\n",
      "|    clip_fraction        | 0.000651    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.35       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.000128    |\n",
      "|    loss                 | 7.7         |\n",
      "|    n_updates            | 2984        |\n",
      "|    policy_gradient_loss | -0.00457    |\n",
      "|    value_loss           | 19.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=574000, episode_reward=0.76 +/- 0.74\n",
      "Episode length: 7.60 +/- 3.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 0.757    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 574000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.44     |\n",
      "|    ep_rew_mean     | -1.84    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 3589     |\n",
      "|    total_timesteps | 574464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 575000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011726691 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.34        |\n",
      "|    explained_variance   | 0.423        |\n",
      "|    learning_rate        | 0.000128     |\n",
      "|    loss                 | 9.55         |\n",
      "|    n_updates            | 2992         |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    value_loss           | 29.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.40 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 576000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -0.815   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 3644     |\n",
      "|    total_timesteps | 576000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=577000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 577000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047909883 |\n",
      "|    clip_fraction        | 0.00431      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.316       |\n",
      "|    explained_variance   | -2.32        |\n",
      "|    learning_rate        | 0.000127     |\n",
      "|    loss                 | 0.682        |\n",
      "|    n_updates            | 3000         |\n",
      "|    policy_gradient_loss | -0.00347     |\n",
      "|    value_loss           | 2.08         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.69     |\n",
      "|    ep_rew_mean     | -1.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 3697     |\n",
      "|    total_timesteps | 577536   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=578000, episode_reward=0.71 +/- 0.76\n",
      "Episode length: 5.80 +/- 1.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 0.714        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 578000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005131804 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.345       |\n",
      "|    explained_variance   | 0.392        |\n",
      "|    learning_rate        | 0.000127     |\n",
      "|    loss                 | 16.7         |\n",
      "|    n_updates            | 3008         |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    value_loss           | 31           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=579000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 579000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | -0.798   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 3751     |\n",
      "|    total_timesteps | 579072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 580000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009301289 |\n",
      "|    clip_fraction        | 0.000814     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.318       |\n",
      "|    explained_variance   | 0.377        |\n",
      "|    learning_rate        | 0.000126     |\n",
      "|    loss                 | 17.3         |\n",
      "|    n_updates            | 3016         |\n",
      "|    policy_gradient_loss | -0.00561     |\n",
      "|    value_loss           | 30           |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.9      |\n",
      "|    ep_rew_mean     | -0.776   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 3845     |\n",
      "|    total_timesteps | 580608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=581000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.00 +/- 1.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 581000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015367275 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.329       |\n",
      "|    explained_variance   | 0.314        |\n",
      "|    learning_rate        | 0.000126     |\n",
      "|    loss                 | 4            |\n",
      "|    n_updates            | 3024         |\n",
      "|    policy_gradient_loss | -0.00709     |\n",
      "|    value_loss           | 18           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=582000, episode_reward=0.74 +/- 0.75\n",
      "Episode length: 6.80 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 0.738    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 582000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.63     |\n",
      "|    ep_rew_mean     | -0.762   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 3940     |\n",
      "|    total_timesteps | 582144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=583000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 583000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003838133 |\n",
      "|    clip_fraction        | 0.00155     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | -2.22       |\n",
      "|    learning_rate        | 0.000125    |\n",
      "|    loss                 | 0.357       |\n",
      "|    n_updates            | 3032        |\n",
      "|    policy_gradient_loss | -0.00381    |\n",
      "|    value_loss           | 0.891       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.59     |\n",
      "|    ep_rew_mean     | -1.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 4034     |\n",
      "|    total_timesteps | 583680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.80 +/- 1.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 584000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007289024 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | 0.372        |\n",
      "|    learning_rate        | 0.000125     |\n",
      "|    loss                 | 8.71         |\n",
      "|    n_updates            | 3040         |\n",
      "|    policy_gradient_loss | -0.00427     |\n",
      "|    value_loss           | 16.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.60 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 585000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.7      |\n",
      "|    ep_rew_mean     | -1.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 4128     |\n",
      "|    total_timesteps | 585216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=586000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 586000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00095547223 |\n",
      "|    clip_fraction        | 0.000163      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.317        |\n",
      "|    explained_variance   | 0.323         |\n",
      "|    learning_rate        | 0.000125      |\n",
      "|    loss                 | 2.86          |\n",
      "|    n_updates            | 3048          |\n",
      "|    policy_gradient_loss | -0.00407      |\n",
      "|    value_loss           | 17.3          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.57     |\n",
      "|    ep_rew_mean     | -1.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 4184     |\n",
      "|    total_timesteps | 586752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=587000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.60 +/- 3.32\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.6        |\n",
      "|    mean_reward          | 1.13       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 587000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00202976 |\n",
      "|    clip_fraction        | 0.000895   |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.334     |\n",
      "|    explained_variance   | 0.201      |\n",
      "|    learning_rate        | 0.000124   |\n",
      "|    loss                 | 19.4       |\n",
      "|    n_updates            | 3056       |\n",
      "|    policy_gradient_loss | -0.00894   |\n",
      "|    value_loss           | 35.4       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=588000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 588000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.94     |\n",
      "|    ep_rew_mean     | -0.775   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 4278     |\n",
      "|    total_timesteps | 588288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=589000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 589000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025988617 |\n",
      "|    clip_fraction        | 0.00163      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.333       |\n",
      "|    explained_variance   | -1.97        |\n",
      "|    learning_rate        | 0.000124     |\n",
      "|    loss                 | 0.778        |\n",
      "|    n_updates            | 3064         |\n",
      "|    policy_gradient_loss | -0.00566     |\n",
      "|    value_loss           | 2.6          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.85     |\n",
      "|    ep_rew_mean     | -0.737   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 4333     |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=0.73 +/- 0.75\n",
      "Episode length: 6.60 +/- 3.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 0.733        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 590000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009344234 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.345       |\n",
      "|    explained_variance   | 0.324        |\n",
      "|    learning_rate        | 0.000123     |\n",
      "|    loss                 | 5.13         |\n",
      "|    n_updates            | 3072         |\n",
      "|    policy_gradient_loss | -0.00264     |\n",
      "|    value_loss           | 17.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=591000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 591000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.13     |\n",
      "|    ep_rew_mean     | -1.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 4428     |\n",
      "|    total_timesteps | 591360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=0.75 +/- 0.81\n",
      "Episode length: 7.20 +/- 4.07\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.2           |\n",
      "|    mean_reward          | 0.748         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 592000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048083995 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.36         |\n",
      "|    explained_variance   | 0.278         |\n",
      "|    learning_rate        | 0.000123      |\n",
      "|    loss                 | 15.2          |\n",
      "|    n_updates            | 3080          |\n",
      "|    policy_gradient_loss | -0.000826     |\n",
      "|    value_loss           | 17.1          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.09     |\n",
      "|    ep_rew_mean     | -1.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 4482     |\n",
      "|    total_timesteps | 592896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=593000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 593000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025630316 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 0.359        |\n",
      "|    learning_rate        | 0.000122     |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 3088         |\n",
      "|    policy_gradient_loss | -0.00804     |\n",
      "|    value_loss           | 31.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=594000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 594000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.42     |\n",
      "|    ep_rew_mean     | -1.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 4575     |\n",
      "|    total_timesteps | 594432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 595000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042265565 |\n",
      "|    clip_fraction        | 0.00252      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.338        |\n",
      "|    learning_rate        | 0.000122     |\n",
      "|    loss                 | 20           |\n",
      "|    n_updates            | 3096         |\n",
      "|    policy_gradient_loss | -0.00802     |\n",
      "|    value_loss           | 32.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.62     |\n",
      "|    ep_rew_mean     | -0.802   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 4669     |\n",
      "|    total_timesteps | 595968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=0.68 +/- 0.78\n",
      "Episode length: 4.40 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 596000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.945679e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.341       |\n",
      "|    explained_variance   | -2.69        |\n",
      "|    learning_rate        | 0.000121     |\n",
      "|    loss                 | 0.768        |\n",
      "|    n_updates            | 3104         |\n",
      "|    policy_gradient_loss | -0.000449    |\n",
      "|    value_loss           | 1.59         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=597000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 597000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.53     |\n",
      "|    ep_rew_mean     | -0.785   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 4763     |\n",
      "|    total_timesteps | 597504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=598000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.00 +/- 1.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 598000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015839854 |\n",
      "|    clip_fraction        | 0.00057      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.33        |\n",
      "|    explained_variance   | 0.435        |\n",
      "|    learning_rate        | 0.000121     |\n",
      "|    loss                 | 8.85         |\n",
      "|    n_updates            | 3112         |\n",
      "|    policy_gradient_loss | -0.00435     |\n",
      "|    value_loss           | 15           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=599000, episode_reward=0.72 +/- 0.77\n",
      "Episode length: 6.20 +/- 2.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 0.724    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 599000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.06     |\n",
      "|    ep_rew_mean     | -2.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 4857     |\n",
      "|    total_timesteps | 599040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 600000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040901045 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.323        |\n",
      "|    explained_variance   | 0.308         |\n",
      "|    learning_rate        | 0.00012       |\n",
      "|    loss                 | 17            |\n",
      "|    n_updates            | 3120          |\n",
      "|    policy_gradient_loss | -0.00317      |\n",
      "|    value_loss           | 32.5          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -0.755   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 4910     |\n",
      "|    total_timesteps | 600576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=601000, episode_reward=0.75 +/- 0.71\n",
      "Episode length: 7.20 +/- 4.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 0.748        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 601000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014838091 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.336       |\n",
      "|    explained_variance   | 0.302        |\n",
      "|    learning_rate        | 0.00012      |\n",
      "|    loss                 | 18.4         |\n",
      "|    n_updates            | 3128         |\n",
      "|    policy_gradient_loss | -0.00284     |\n",
      "|    value_loss           | 34.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=602000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 3.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 602000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | -2.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 4964     |\n",
      "|    total_timesteps | 602112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=603000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 603000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00067685853 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.345        |\n",
      "|    explained_variance   | 0.349         |\n",
      "|    learning_rate        | 0.000119      |\n",
      "|    loss                 | 16.6          |\n",
      "|    n_updates            | 3136          |\n",
      "|    policy_gradient_loss | -0.00322      |\n",
      "|    value_loss           | 47.1          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | -0.795   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 5017     |\n",
      "|    total_timesteps | 603648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=0.70 +/- 0.78\n",
      "Episode length: 5.20 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 0.7          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 604000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011317284 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.364       |\n",
      "|    explained_variance   | 0.279        |\n",
      "|    learning_rate        | 0.000119     |\n",
      "|    loss                 | 17.3         |\n",
      "|    n_updates            | 3144         |\n",
      "|    policy_gradient_loss | -0.00294     |\n",
      "|    value_loss           | 35.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=1.15 +/- 0.05\n",
      "Episode length: 7.20 +/- 2.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 605000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.6      |\n",
      "|    ep_rew_mean     | -1.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 5073     |\n",
      "|    total_timesteps | 605184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=606000, episode_reward=0.67 +/- 0.80\n",
      "Episode length: 4.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 4        |\n",
      "|    mean_reward          | 0.671    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 606000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.000745 |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.349   |\n",
      "|    explained_variance   | 0.389    |\n",
      "|    learning_rate        | 0.000119 |\n",
      "|    loss                 | 13.1     |\n",
      "|    n_updates            | 3152     |\n",
      "|    policy_gradient_loss | -0.00337 |\n",
      "|    value_loss           | 15.7     |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | -0.775   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 5126     |\n",
      "|    total_timesteps | 606720   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=607000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 607000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020511406 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.329        |\n",
      "|    explained_variance   | -1.78         |\n",
      "|    learning_rate        | 0.000118      |\n",
      "|    loss                 | 1.24          |\n",
      "|    n_updates            | 3160          |\n",
      "|    policy_gradient_loss | -0.000498     |\n",
      "|    value_loss           | 3.03          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 608000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.87     |\n",
      "|    ep_rew_mean     | -0.736   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 5181     |\n",
      "|    total_timesteps | 608256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=609000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.8           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 609000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030572538 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.332        |\n",
      "|    explained_variance   | -1.87         |\n",
      "|    learning_rate        | 0.000118      |\n",
      "|    loss                 | 2.03          |\n",
      "|    n_updates            | 3168          |\n",
      "|    policy_gradient_loss | -0.00224      |\n",
      "|    value_loss           | 2.73          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02     |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 5235     |\n",
      "|    total_timesteps | 609792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 610000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016702036 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | 0.285        |\n",
      "|    learning_rate        | 0.000117     |\n",
      "|    loss                 | 10.1         |\n",
      "|    n_updates            | 3176         |\n",
      "|    policy_gradient_loss | -0.0066      |\n",
      "|    value_loss           | 34.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=611000, episode_reward=0.69 +/- 0.80\n",
      "Episode length: 4.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 0.69     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 611000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.49     |\n",
      "|    ep_rew_mean     | -0.725   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 5288     |\n",
      "|    total_timesteps | 611328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.40 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 612000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023907216 |\n",
      "|    clip_fraction        | 0.00106      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.338       |\n",
      "|    explained_variance   | -1.67        |\n",
      "|    learning_rate        | 0.000117     |\n",
      "|    loss                 | 0.77         |\n",
      "|    n_updates            | 3184         |\n",
      "|    policy_gradient_loss | -0.00337     |\n",
      "|    value_loss           | 0.965        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.95     |\n",
      "|    ep_rew_mean     | -1.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 5342     |\n",
      "|    total_timesteps | 612864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=613000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 613000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015542271 |\n",
      "|    clip_fraction        | 0.00057      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.355       |\n",
      "|    explained_variance   | 0.32         |\n",
      "|    learning_rate        | 0.000116     |\n",
      "|    loss                 | 16           |\n",
      "|    n_updates            | 3192         |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    value_loss           | 33.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=614000, episode_reward=0.74 +/- 0.73\n",
      "Episode length: 7.00 +/- 3.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 0.743    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 614000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.69     |\n",
      "|    ep_rew_mean     | -1.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 5396     |\n",
      "|    total_timesteps | 614400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.60 +/- 2.33\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 6.6       |\n",
      "|    mean_reward          | 1.13      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 615000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0054037 |\n",
      "|    clip_fraction        | 0.0035    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.325    |\n",
      "|    explained_variance   | 0.201     |\n",
      "|    learning_rate        | 0.000116  |\n",
      "|    loss                 | 14.1      |\n",
      "|    n_updates            | 3200      |\n",
      "|    policy_gradient_loss | -0.00957  |\n",
      "|    value_loss           | 18.3      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.38     |\n",
      "|    ep_rew_mean     | -1.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 5450     |\n",
      "|    total_timesteps | 615936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 616000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039015298 |\n",
      "|    clip_fraction        | 0.00244      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.338       |\n",
      "|    explained_variance   | 0.396        |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 6.18         |\n",
      "|    n_updates            | 3208         |\n",
      "|    policy_gradient_loss | -0.00433     |\n",
      "|    value_loss           | 16.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=617000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 617000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -1.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 5503     |\n",
      "|    total_timesteps | 617472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.00 +/- 1.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 618000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008093926 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | 0.438        |\n",
      "|    learning_rate        | 0.000115     |\n",
      "|    loss                 | 17.4         |\n",
      "|    n_updates            | 3216         |\n",
      "|    policy_gradient_loss | -0.0033      |\n",
      "|    value_loss           | 29.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=619000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 619000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.16     |\n",
      "|    ep_rew_mean     | -1.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 5558     |\n",
      "|    total_timesteps | 619008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 620000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038164917 |\n",
      "|    clip_fraction        | 0.00212      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.346       |\n",
      "|    explained_variance   | 0.334        |\n",
      "|    learning_rate        | 0.000114     |\n",
      "|    loss                 | 10.9         |\n",
      "|    n_updates            | 3224         |\n",
      "|    policy_gradient_loss | -0.00738     |\n",
      "|    value_loss           | 33.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.82     |\n",
      "|    ep_rew_mean     | -0.798   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 5613     |\n",
      "|    total_timesteps | 620544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=621000, episode_reward=1.14 +/- 0.04\n",
      "Episode length: 6.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.8          |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 621000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018167879 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.326       |\n",
      "|    explained_variance   | -1.74        |\n",
      "|    learning_rate        | 0.000114     |\n",
      "|    loss                 | 1.81         |\n",
      "|    n_updates            | 3232         |\n",
      "|    policy_gradient_loss | -0.00431     |\n",
      "|    value_loss           | 3.01         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=622000, episode_reward=0.70 +/- 0.81\n",
      "Episode length: 5.40 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 0.705    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 622000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.09     |\n",
      "|    ep_rew_mean     | -0.731   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 5666     |\n",
      "|    total_timesteps | 622080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=623000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 1.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 623000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002100188 |\n",
      "|    clip_fraction        | 0.000326    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.000114    |\n",
      "|    loss                 | 8.75        |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.0032     |\n",
      "|    value_loss           | 16.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -1.78    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 5721     |\n",
      "|    total_timesteps | 623616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.60 +/- 1.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 624000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022826048 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.337       |\n",
      "|    explained_variance   | 0.449        |\n",
      "|    learning_rate        | 0.000113     |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 3248         |\n",
      "|    policy_gradient_loss | -0.00324     |\n",
      "|    value_loss           | 14.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 625000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.77     |\n",
      "|    ep_rew_mean     | -0.759   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 5815     |\n",
      "|    total_timesteps | 625152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=626000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 3.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 626000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012321618 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.341       |\n",
      "|    explained_variance   | -2.68        |\n",
      "|    learning_rate        | 0.000113     |\n",
      "|    loss                 | 0.935        |\n",
      "|    n_updates            | 3256         |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 1.45         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.86     |\n",
      "|    ep_rew_mean     | -1.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 5910     |\n",
      "|    total_timesteps | 626688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=627000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 627000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051488314 |\n",
      "|    clip_fraction        | 0.000407      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.364        |\n",
      "|    explained_variance   | 0.455         |\n",
      "|    learning_rate        | 0.000112      |\n",
      "|    loss                 | 7.45          |\n",
      "|    n_updates            | 3264          |\n",
      "|    policy_gradient_loss | -0.00164      |\n",
      "|    value_loss           | 14.6          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=0.71 +/- 0.75\n",
      "Episode length: 5.60 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 0.71     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 628000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.14     |\n",
      "|    ep_rew_mean     | -1.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 6002     |\n",
      "|    total_timesteps | 628224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=629000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 629000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025416361 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | 0.399        |\n",
      "|    learning_rate        | 0.000112     |\n",
      "|    loss                 | 3.14         |\n",
      "|    n_updates            | 3272         |\n",
      "|    policy_gradient_loss | -0.00504     |\n",
      "|    value_loss           | 15.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.73     |\n",
      "|    ep_rew_mean     | -0.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 6097     |\n",
      "|    total_timesteps | 629760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=1.15 +/- 0.06\n",
      "Episode length: 7.20 +/- 2.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 630000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025504339 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | 0.504        |\n",
      "|    learning_rate        | 0.000111     |\n",
      "|    loss                 | 6.67         |\n",
      "|    n_updates            | 3280         |\n",
      "|    policy_gradient_loss | -0.0071      |\n",
      "|    value_loss           | 13.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=631000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 631000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.62     |\n",
      "|    ep_rew_mean     | -0.742   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 6192     |\n",
      "|    total_timesteps | 631296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=0.72 +/- 0.81\n",
      "Episode length: 6.00 +/- 2.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 0.719        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 632000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020914697 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | 0.374        |\n",
      "|    learning_rate        | 0.000111     |\n",
      "|    loss                 | 4.52         |\n",
      "|    n_updates            | 3288         |\n",
      "|    policy_gradient_loss | -0.00563     |\n",
      "|    value_loss           | 17.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.63     |\n",
      "|    ep_rew_mean     | -0.702   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 6286     |\n",
      "|    total_timesteps | 632832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=633000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 633000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008341889 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.37        |\n",
      "|    explained_variance   | -2.48        |\n",
      "|    learning_rate        | 0.00011      |\n",
      "|    loss                 | 1.08         |\n",
      "|    n_updates            | 3296         |\n",
      "|    policy_gradient_loss | -0.00363     |\n",
      "|    value_loss           | 3.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=634000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 634000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.49     |\n",
      "|    ep_rew_mean     | -0.805   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 6381     |\n",
      "|    total_timesteps | 634368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 635000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008544363 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.379        |\n",
      "|    learning_rate        | 0.00011      |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 3304         |\n",
      "|    policy_gradient_loss | -0.00363     |\n",
      "|    value_loss           | 15.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.8      |\n",
      "|    ep_rew_mean     | -1.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 6473     |\n",
      "|    total_timesteps | 635904   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=636000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.00 +/- 1.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 636000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005729461 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.361       |\n",
      "|    explained_variance   | 0.409        |\n",
      "|    learning_rate        | 0.000109     |\n",
      "|    loss                 | 16.5         |\n",
      "|    n_updates            | 3312         |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    value_loss           | 30.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=637000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 2.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 637000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.88     |\n",
      "|    ep_rew_mean     | -1.77    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 89       |\n",
      "|    time_elapsed    | 6568     |\n",
      "|    total_timesteps | 637440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=638000, episode_reward=0.71 +/- 0.77\n",
      "Episode length: 5.60 +/- 1.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.6        |\n",
      "|    mean_reward          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 638000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00350974 |\n",
      "|    clip_fraction        | 0.00163    |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.381     |\n",
      "|    explained_variance   | 0.287      |\n",
      "|    learning_rate        | 0.000109   |\n",
      "|    loss                 | 11.2       |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.00578   |\n",
      "|    value_loss           | 20.1       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.1      |\n",
      "|    ep_rew_mean     | -1.68    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 6662     |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=639000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.20 +/- 2.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 639000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005465314 |\n",
      "|    clip_fraction        | 0.00529     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.386      |\n",
      "|    explained_variance   | 0.289       |\n",
      "|    learning_rate        | 0.000108    |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 3328        |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=0.69 +/- 0.77\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 0.686    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | -0.718   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 6757     |\n",
      "|    total_timesteps | 640512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641000, episode_reward=0.69 +/- 0.80\n",
      "Episode length: 4.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 641000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009790403 |\n",
      "|    clip_fraction        | 0.00789     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.378      |\n",
      "|    explained_variance   | -1.87       |\n",
      "|    learning_rate        | 0.000108    |\n",
      "|    loss                 | 0.352       |\n",
      "|    n_updates            | 3336        |\n",
      "|    policy_gradient_loss | -0.00541    |\n",
      "|    value_loss           | 0.519       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=642000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 642000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.23     |\n",
      "|    ep_rew_mean     | -1.74    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 6852     |\n",
      "|    total_timesteps | 642048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=643000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 643000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031949335 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.36        |\n",
      "|    explained_variance   | 0.412        |\n",
      "|    learning_rate        | 0.000108     |\n",
      "|    loss                 | 15           |\n",
      "|    n_updates            | 3344         |\n",
      "|    policy_gradient_loss | -0.00677     |\n",
      "|    value_loss           | 15.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.47     |\n",
      "|    ep_rew_mean     | -0.722   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 6905     |\n",
      "|    total_timesteps | 643584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=1.11 +/- 0.08\n",
      "Episode length: 5.60 +/- 3.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.6           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 644000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00094606244 |\n",
      "|    clip_fraction        | 0.000326      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.381        |\n",
      "|    explained_variance   | -2.03         |\n",
      "|    learning_rate        | 0.000107      |\n",
      "|    loss                 | 1.91          |\n",
      "|    n_updates            | 3352          |\n",
      "|    policy_gradient_loss | -0.00283      |\n",
      "|    value_loss           | 4.39          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=0.74 +/- 0.76\n",
      "Episode length: 6.80 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 0.738    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 645000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.88     |\n",
      "|    ep_rew_mean     | -0.756   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 6959     |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=646000, episode_reward=1.16 +/- 0.10\n",
      "Episode length: 7.60 +/- 4.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.6          |\n",
      "|    mean_reward          | 1.16         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 646000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070183813 |\n",
      "|    clip_fraction        | 0.00578      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.368       |\n",
      "|    explained_variance   | -1.77        |\n",
      "|    learning_rate        | 0.000107     |\n",
      "|    loss                 | 0.342        |\n",
      "|    n_updates            | 3360         |\n",
      "|    policy_gradient_loss | -0.00744     |\n",
      "|    value_loss           | 1.74         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.66     |\n",
      "|    ep_rew_mean     | -0.781   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 95       |\n",
      "|    time_elapsed    | 7054     |\n",
      "|    total_timesteps | 646656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=647000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.20 +/- 1.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 647000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056119063 |\n",
      "|    clip_fraction        | 0.00627      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.328       |\n",
      "|    explained_variance   | -0.83        |\n",
      "|    learning_rate        | 0.000106     |\n",
      "|    loss                 | 0.0492       |\n",
      "|    n_updates            | 3368         |\n",
      "|    policy_gradient_loss | -0.00733     |\n",
      "|    value_loss           | 0.137        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 648000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.89     |\n",
      "|    ep_rew_mean     | -1.81    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 7150     |\n",
      "|    total_timesteps | 648192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649000, episode_reward=1.14 +/- 0.07\n",
      "Episode length: 6.80 +/- 2.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.8          |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 649000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016507794 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.34        |\n",
      "|    explained_variance   | 0.305        |\n",
      "|    learning_rate        | 0.000106     |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 3376         |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    value_loss           | 16.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.24     |\n",
      "|    ep_rew_mean     | -0.648   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 7242     |\n",
      "|    total_timesteps | 649728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 650000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012945856 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | -2.13        |\n",
      "|    learning_rate        | 0.000105     |\n",
      "|    loss                 | 0.222        |\n",
      "|    n_updates            | 3384         |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    value_loss           | 1.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=651000, episode_reward=1.15 +/- 0.07\n",
      "Episode length: 7.40 +/- 2.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 651000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.65     |\n",
      "|    ep_rew_mean     | -0.722   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 7333     |\n",
      "|    total_timesteps | 651264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=0.74 +/- 0.75\n",
      "Episode length: 7.00 +/- 2.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | 0.743       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 652000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019961426 |\n",
      "|    clip_fraction        | 0.0392      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | -0.714      |\n",
      "|    learning_rate        | 0.000105    |\n",
      "|    loss                 | 0.138       |\n",
      "|    n_updates            | 3392        |\n",
      "|    policy_gradient_loss | -0.00984    |\n",
      "|    value_loss           | 0.217       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.87     |\n",
      "|    ep_rew_mean     | -0.636   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 99       |\n",
      "|    time_elapsed    | 7424     |\n",
      "|    total_timesteps | 652800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=653000, episode_reward=0.70 +/- 0.76\n",
      "Episode length: 5.40 +/- 1.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 0.705        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 653000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030588272 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | -2.68        |\n",
      "|    learning_rate        | 0.000104     |\n",
      "|    loss                 | 2.22         |\n",
      "|    n_updates            | 3400         |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 2.54         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=654000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 654000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.71     |\n",
      "|    ep_rew_mean     | -1.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 7519     |\n",
      "|    total_timesteps | 654336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 655000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013659771 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.39         |\n",
      "|    explained_variance   | 0.338         |\n",
      "|    learning_rate        | 0.000104      |\n",
      "|    loss                 | 42.5          |\n",
      "|    n_updates            | 3408          |\n",
      "|    policy_gradient_loss | -0.00086      |\n",
      "|    value_loss           | 64.3          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.85     |\n",
      "|    ep_rew_mean     | -1.71    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 7610     |\n",
      "|    total_timesteps | 655872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 656000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007543425 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.397       |\n",
      "|    explained_variance   | 0.257        |\n",
      "|    learning_rate        | 0.000103     |\n",
      "|    loss                 | 29.1         |\n",
      "|    n_updates            | 3416         |\n",
      "|    policy_gradient_loss | -0.00355     |\n",
      "|    value_loss           | 33.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=657000, episode_reward=1.14 +/- 0.09\n",
      "Episode length: 7.00 +/- 3.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 657000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.86     |\n",
      "|    ep_rew_mean     | -0.677   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 7704     |\n",
      "|    total_timesteps | 657408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=658000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 658000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005652054 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | 0.376        |\n",
      "|    learning_rate        | 0.000103     |\n",
      "|    loss                 | 11.8         |\n",
      "|    n_updates            | 3424         |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 30.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.9      |\n",
      "|    ep_rew_mean     | -1.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 7757     |\n",
      "|    total_timesteps | 658944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=659000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 659000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032976948 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.368        |\n",
      "|    explained_variance   | 0.448         |\n",
      "|    learning_rate        | 0.000102      |\n",
      "|    loss                 | 2.55          |\n",
      "|    n_updates            | 3432          |\n",
      "|    policy_gradient_loss | -0.00344      |\n",
      "|    value_loss           | 14.9          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=1.13 +/- 0.09\n",
      "Episode length: 6.60 +/- 3.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.08     |\n",
      "|    ep_rew_mean     | -0.651   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 7810     |\n",
      "|    total_timesteps | 660480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=661000, episode_reward=0.72 +/- 0.74\n",
      "Episode length: 6.20 +/- 2.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 0.724        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 661000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021683953 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.381       |\n",
      "|    explained_variance   | -1.78        |\n",
      "|    learning_rate        | 0.000102     |\n",
      "|    loss                 | 1.8          |\n",
      "|    n_updates            | 3440         |\n",
      "|    policy_gradient_loss | -0.00628     |\n",
      "|    value_loss           | 4.24         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=662000, episode_reward=0.70 +/- 0.73\n",
      "Episode length: 5.40 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 0.705    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 662000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.08     |\n",
      "|    ep_rew_mean     | -1.72    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 7863     |\n",
      "|    total_timesteps | 662016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=663000, episode_reward=0.72 +/- 0.79\n",
      "Episode length: 6.20 +/- 1.94\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.2           |\n",
      "|    mean_reward          | 0.724         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 663000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054894615 |\n",
      "|    clip_fraction        | 0.000407      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.38         |\n",
      "|    explained_variance   | 0.342         |\n",
      "|    learning_rate        | 0.000102      |\n",
      "|    loss                 | 14.8          |\n",
      "|    n_updates            | 3448          |\n",
      "|    policy_gradient_loss | -0.000751     |\n",
      "|    value_loss           | 33.1          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.74     |\n",
      "|    ep_rew_mean     | -0.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 7918     |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=0.76 +/- 0.74\n",
      "Episode length: 7.80 +/- 3.43\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.8           |\n",
      "|    mean_reward          | 0.762         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 664000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012416951 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.36         |\n",
      "|    explained_variance   | 0.383         |\n",
      "|    learning_rate        | 0.000101      |\n",
      "|    loss                 | 17.8          |\n",
      "|    n_updates            | 3456          |\n",
      "|    policy_gradient_loss | -0.00087      |\n",
      "|    value_loss           | 30.8          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=665000, episode_reward=0.72 +/- 0.74\n",
      "Episode length: 6.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 0.719    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 665000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.14     |\n",
      "|    ep_rew_mean     | -1.64    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 107      |\n",
      "|    time_elapsed    | 7971     |\n",
      "|    total_timesteps | 665088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=666000, episode_reward=0.72 +/- 0.78\n",
      "Episode length: 6.00 +/- 2.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 0.719        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 666000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003650247 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | 0.299        |\n",
      "|    learning_rate        | 0.000101     |\n",
      "|    loss                 | 14.6         |\n",
      "|    n_updates            | 3464         |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    value_loss           | 33.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.57     |\n",
      "|    ep_rew_mean     | -0.704   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 8024     |\n",
      "|    total_timesteps | 666624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=667000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.40 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 667000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068178214 |\n",
      "|    clip_fraction        | 0.00326      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | -1.77        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.748        |\n",
      "|    n_updates            | 3472         |\n",
      "|    policy_gradient_loss | -0.00936     |\n",
      "|    value_loss           | 1.43         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=-0.06 +/- 0.92\n",
      "Episode length: 7.00 +/- 2.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -0.0571  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 668000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.94     |\n",
      "|    ep_rew_mean     | -1.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 109      |\n",
      "|    time_elapsed    | 8078     |\n",
      "|    total_timesteps | 668160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=669000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 669000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038639586 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.35        |\n",
      "|    explained_variance   | 0.352        |\n",
      "|    learning_rate        | 9.97e-05     |\n",
      "|    loss                 | 9.12         |\n",
      "|    n_updates            | 3480         |\n",
      "|    policy_gradient_loss | -0.00352     |\n",
      "|    value_loss           | 16.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.19     |\n",
      "|    ep_rew_mean     | -0.649   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 8131     |\n",
      "|    total_timesteps | 669696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 670000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002521753 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.323      |\n",
      "|    explained_variance   | -1.6        |\n",
      "|    learning_rate        | 9.92e-05    |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 3488        |\n",
      "|    policy_gradient_loss | -0.00375    |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=671000, episode_reward=0.68 +/- 0.79\n",
      "Episode length: 4.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 0.676    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 671000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.02     |\n",
      "|    ep_rew_mean     | -0.733   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 111      |\n",
      "|    time_elapsed    | 8184     |\n",
      "|    total_timesteps | 671232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 672000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014431601 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.348       |\n",
      "|    explained_variance   | 0.32         |\n",
      "|    learning_rate        | 9.88e-05     |\n",
      "|    loss                 | 4.14         |\n",
      "|    n_updates            | 3496         |\n",
      "|    policy_gradient_loss | -0.00298     |\n",
      "|    value_loss           | 17.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.75     |\n",
      "|    ep_rew_mean     | -0.579   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 8236     |\n",
      "|    total_timesteps | 672768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=673000, episode_reward=1.11 +/- 0.08\n",
      "Episode length: 5.60 +/- 3.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 673000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018509364 |\n",
      "|    clip_fraction        | 0.0013       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.329       |\n",
      "|    explained_variance   | 0.435        |\n",
      "|    learning_rate        | 9.83e-05     |\n",
      "|    loss                 | 8.18         |\n",
      "|    n_updates            | 3504         |\n",
      "|    policy_gradient_loss | -0.00454     |\n",
      "|    value_loss           | 14.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=674000, episode_reward=1.13 +/- 0.07\n",
      "Episode length: 6.40 +/- 3.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 674000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.07     |\n",
      "|    ep_rew_mean     | -0.612   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 8289     |\n",
      "|    total_timesteps | 674304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 675000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006326623 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.347       |\n",
      "|    explained_variance   | 0.42         |\n",
      "|    learning_rate        | 9.79e-05     |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 3512         |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 30.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.82     |\n",
      "|    ep_rew_mean     | -1.69    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 8345     |\n",
      "|    total_timesteps | 675840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 676000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035105192 |\n",
      "|    clip_fraction        | 0.00155      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 0.0303       |\n",
      "|    learning_rate        | 9.74e-05     |\n",
      "|    loss                 | 9.43         |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    value_loss           | 20.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=677000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 677000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.8      |\n",
      "|    ep_rew_mean     | -0.658   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 8401     |\n",
      "|    total_timesteps | 677376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=678000, episode_reward=0.73 +/- 0.77\n",
      "Episode length: 6.40 +/- 2.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 0.729        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 678000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046195346 |\n",
      "|    clip_fraction        | 0.00342      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.33        |\n",
      "|    explained_variance   | -1.09        |\n",
      "|    learning_rate        | 9.69e-05     |\n",
      "|    loss                 | 0.264        |\n",
      "|    n_updates            | 3528         |\n",
      "|    policy_gradient_loss | -0.00506     |\n",
      "|    value_loss           | 2.1          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.66     |\n",
      "|    ep_rew_mean     | -0.621   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 8454     |\n",
      "|    total_timesteps | 678912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=679000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 3.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 679000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005509757 |\n",
      "|    clip_fraction        | 0.00155     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.335      |\n",
      "|    explained_variance   | -1.51       |\n",
      "|    learning_rate        | 9.65e-05    |\n",
      "|    loss                 | 0.366       |\n",
      "|    n_updates            | 3536        |\n",
      "|    policy_gradient_loss | -0.00492    |\n",
      "|    value_loss           | 0.977       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.04     |\n",
      "|    ep_rew_mean     | -2.59    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 117      |\n",
      "|    time_elapsed    | 8544     |\n",
      "|    total_timesteps | 680448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=681000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 681000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00069229497 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.313        |\n",
      "|    explained_variance   | 0.147         |\n",
      "|    learning_rate        | 9.6e-05       |\n",
      "|    loss                 | 34.5          |\n",
      "|    n_updates            | 3544          |\n",
      "|    policy_gradient_loss | -0.000593     |\n",
      "|    value_loss           | 37.2          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -0.493   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 8633     |\n",
      "|    total_timesteps | 681984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=682000, episode_reward=1.11 +/- 0.03\n",
      "Episode length: 5.80 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 682000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029995162 |\n",
      "|    clip_fraction        | 0.00236      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.333       |\n",
      "|    explained_variance   | 0.342        |\n",
      "|    learning_rate        | 9.56e-05     |\n",
      "|    loss                 | 14.3         |\n",
      "|    n_updates            | 3552         |\n",
      "|    policy_gradient_loss | -0.0077      |\n",
      "|    value_loss           | 16.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=683000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 683000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | -0.578   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 119      |\n",
      "|    time_elapsed    | 8725     |\n",
      "|    total_timesteps | 683520   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=684000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 684000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00087156956 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.327        |\n",
      "|    explained_variance   | 0.429         |\n",
      "|    learning_rate        | 9.51e-05      |\n",
      "|    loss                 | 5.91          |\n",
      "|    n_updates            | 3560          |\n",
      "|    policy_gradient_loss | -0.00157      |\n",
      "|    value_loss           | 15            |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 685000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.88     |\n",
      "|    ep_rew_mean     | -0.576   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 8816     |\n",
      "|    total_timesteps | 685056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=686000, episode_reward=0.80 +/- 0.77\n",
      "Episode length: 9.20 +/- 3.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.2          |\n",
      "|    mean_reward          | 0.795        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 686000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041227383 |\n",
      "|    clip_fraction        | 0.00496      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.338       |\n",
      "|    explained_variance   | -1.03        |\n",
      "|    learning_rate        | 9.46e-05     |\n",
      "|    loss                 | 0.734        |\n",
      "|    n_updates            | 3568         |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    value_loss           | 1.97         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.86     |\n",
      "|    ep_rew_mean     | -0.677   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 121      |\n",
      "|    time_elapsed    | 8906     |\n",
      "|    total_timesteps | 686592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=687000, episode_reward=1.16 +/- 0.05\n",
      "Episode length: 7.80 +/- 1.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.8          |\n",
      "|    mean_reward          | 1.16         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 687000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024839898 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.334       |\n",
      "|    explained_variance   | 0.315        |\n",
      "|    learning_rate        | 9.42e-05     |\n",
      "|    loss                 | 3.96         |\n",
      "|    n_updates            | 3576         |\n",
      "|    policy_gradient_loss | -0.00353     |\n",
      "|    value_loss           | 17.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.67     |\n",
      "|    ep_rew_mean     | -0.521   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 8997     |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=689000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.20 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 689000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016033262 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.305       |\n",
      "|    explained_variance   | 0.272        |\n",
      "|    learning_rate        | 9.37e-05     |\n",
      "|    loss                 | 6.03         |\n",
      "|    n_updates            | 3584         |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 17.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -0.513   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 9087     |\n",
      "|    total_timesteps | 689664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.60 +/- 3.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.6         |\n",
      "|    mean_reward          | 1.13        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 690000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003507847 |\n",
      "|    clip_fraction        | 0.00285     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.312      |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 9.33e-05    |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 3592        |\n",
      "|    policy_gradient_loss | -0.00758    |\n",
      "|    value_loss           | 35          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=691000, episode_reward=1.14 +/- 0.04\n",
      "Episode length: 6.80 +/- 1.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 691000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.78     |\n",
      "|    ep_rew_mean     | -1.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 9140     |\n",
      "|    total_timesteps | 691200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 692000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011172937 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.297       |\n",
      "|    explained_variance   | 0.149        |\n",
      "|    learning_rate        | 9.28e-05     |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 3600         |\n",
      "|    policy_gradient_loss | -0.00575     |\n",
      "|    value_loss           | 19.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.51     |\n",
      "|    ep_rew_mean     | -0.545   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 9232     |\n",
      "|    total_timesteps | 692736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=693000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 693000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012406801 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.301       |\n",
      "|    explained_variance   | -1.11        |\n",
      "|    learning_rate        | 9.23e-05     |\n",
      "|    loss                 | 0.746        |\n",
      "|    n_updates            | 3608         |\n",
      "|    policy_gradient_loss | -0.00551     |\n",
      "|    value_loss           | 1.91         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=694000, episode_reward=0.75 +/- 0.83\n",
      "Episode length: 7.40 +/- 2.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 0.752    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 694000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.06     |\n",
      "|    ep_rew_mean     | -1.52    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 9322     |\n",
      "|    total_timesteps | 694272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=0.69 +/- 0.77\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 695000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003229433 |\n",
      "|    clip_fraction        | 0.00212     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.0445      |\n",
      "|    learning_rate        | 9.19e-05    |\n",
      "|    loss                 | 13.5        |\n",
      "|    n_updates            | 3616        |\n",
      "|    policy_gradient_loss | -0.00548    |\n",
      "|    value_loss           | 19.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | -0.575   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 9413     |\n",
      "|    total_timesteps | 695808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=0.72 +/- 0.74\n",
      "Episode length: 6.20 +/- 2.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 0.724        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 696000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012411681 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.314       |\n",
      "|    explained_variance   | -1.24        |\n",
      "|    learning_rate        | 9.14e-05     |\n",
      "|    loss                 | 2.04         |\n",
      "|    n_updates            | 3624         |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 3.46         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=697000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 697000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | -0.515   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 9502     |\n",
      "|    total_timesteps | 697344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=698000, episode_reward=0.77 +/- 0.80\n",
      "Episode length: 8.00 +/- 2.90\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 8             |\n",
      "|    mean_reward          | 0.767         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 698000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018490078 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.314        |\n",
      "|    explained_variance   | 0.341         |\n",
      "|    learning_rate        | 9.1e-05       |\n",
      "|    loss                 | 13.6          |\n",
      "|    n_updates            | 3632          |\n",
      "|    policy_gradient_loss | -0.000895     |\n",
      "|    value_loss           | 33.4          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.92     |\n",
      "|    ep_rew_mean     | -1.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 9555     |\n",
      "|    total_timesteps | 698880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=699000, episode_reward=1.11 +/- 0.09\n",
      "Episode length: 5.80 +/- 3.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.8           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 699000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015774951 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.314        |\n",
      "|    explained_variance   | 0.242         |\n",
      "|    learning_rate        | 9.05e-05      |\n",
      "|    loss                 | 7.41          |\n",
      "|    n_updates            | 3640          |\n",
      "|    policy_gradient_loss | -0.00191      |\n",
      "|    value_loss           | 17.3          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-19.07 +/- 40.33\n",
      "Episode length: 6.40 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -19.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.77     |\n",
      "|    ep_rew_mean     | -0.539   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 9607     |\n",
      "|    total_timesteps | 700416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=701000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.20 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 701000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00096409797 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.31         |\n",
      "|    explained_variance   | -1.29         |\n",
      "|    learning_rate        | 9e-05         |\n",
      "|    loss                 | 0.656         |\n",
      "|    n_updates            | 3648          |\n",
      "|    policy_gradient_loss | -0.00287      |\n",
      "|    value_loss           | 1.33          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | -1.65    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 9660     |\n",
      "|    total_timesteps | 701952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=702000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 702000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00088824093 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.331        |\n",
      "|    explained_variance   | 0.121         |\n",
      "|    learning_rate        | 8.96e-05      |\n",
      "|    loss                 | 15.9          |\n",
      "|    n_updates            | 3656          |\n",
      "|    policy_gradient_loss | -0.0027       |\n",
      "|    value_loss           | 21.2          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=703000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 703000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.69     |\n",
      "|    ep_rew_mean     | -1.67    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 9751     |\n",
      "|    total_timesteps | 703488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=1.17 +/- 0.10\n",
      "Episode length: 8.20 +/- 4.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8.2          |\n",
      "|    mean_reward          | 1.17         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 704000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003681708 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.304       |\n",
      "|    explained_variance   | 0.388        |\n",
      "|    learning_rate        | 8.91e-05     |\n",
      "|    loss                 | 8.27         |\n",
      "|    n_updates            | 3664         |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    value_loss           | 16.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=0.72 +/- 0.74\n",
      "Episode length: 6.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 0.719    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 705000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | -0.715   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 133      |\n",
      "|    time_elapsed    | 9844     |\n",
      "|    total_timesteps | 705024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=706000, episode_reward=1.15 +/- 0.06\n",
      "Episode length: 7.20 +/- 2.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 706000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026743785 |\n",
      "|    clip_fraction        | 0.00155      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.295       |\n",
      "|    explained_variance   | -0.726       |\n",
      "|    learning_rate        | 8.86e-05     |\n",
      "|    loss                 | 0.474        |\n",
      "|    n_updates            | 3672         |\n",
      "|    policy_gradient_loss | -0.00901     |\n",
      "|    value_loss           | 1.11         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.98     |\n",
      "|    ep_rew_mean     | -0.534   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 9934     |\n",
      "|    total_timesteps | 706560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=707000, episode_reward=1.11 +/- 0.08\n",
      "Episode length: 5.60 +/- 3.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.6         |\n",
      "|    mean_reward          | 1.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 707000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004107818 |\n",
      "|    clip_fraction        | 0.00155     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.303      |\n",
      "|    explained_variance   | -0.848      |\n",
      "|    learning_rate        | 8.82e-05    |\n",
      "|    loss                 | 0.643       |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | -0.00627    |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=1.14 +/- 0.04\n",
      "Episode length: 7.00 +/- 1.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 708000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.52     |\n",
      "|    ep_rew_mean     | -0.685   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 9990     |\n",
      "|    total_timesteps | 708096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=709000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 709000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043247775 |\n",
      "|    clip_fraction        | 0.0022       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.307       |\n",
      "|    explained_variance   | -0.539       |\n",
      "|    learning_rate        | 8.77e-05     |\n",
      "|    loss                 | 0.353        |\n",
      "|    n_updates            | 3688         |\n",
      "|    policy_gradient_loss | -0.0054      |\n",
      "|    value_loss           | 0.833        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.95     |\n",
      "|    ep_rew_mean     | -0.555   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 10082    |\n",
      "|    total_timesteps | 709632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 710000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050204042 |\n",
      "|    clip_fraction        | 0.00545      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.3         |\n",
      "|    explained_variance   | -0.709       |\n",
      "|    learning_rate        | 8.73e-05     |\n",
      "|    loss                 | 1.21         |\n",
      "|    n_updates            | 3696         |\n",
      "|    policy_gradient_loss | -0.0072      |\n",
      "|    value_loss           | 1.42         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=711000, episode_reward=1.13 +/- 0.07\n",
      "Episode length: 6.40 +/- 2.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 711000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.2      |\n",
      "|    ep_rew_mean     | -0.509   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 10171    |\n",
      "|    total_timesteps | 711168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=0.79 +/- 0.83\n",
      "Episode length: 8.80 +/- 1.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.8         |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 712000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006813971 |\n",
      "|    clip_fraction        | 0.00635     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | -0.0236     |\n",
      "|    learning_rate        | 8.68e-05    |\n",
      "|    loss                 | 0.243       |\n",
      "|    n_updates            | 3704        |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    value_loss           | 0.353       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.87     |\n",
      "|    ep_rew_mean     | -0.576   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 10223    |\n",
      "|    total_timesteps | 712704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=713000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 713000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020606422 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.257       |\n",
      "|    explained_variance   | -0.645       |\n",
      "|    learning_rate        | 8.63e-05     |\n",
      "|    loss                 | 0.288        |\n",
      "|    n_updates            | 3712         |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    value_loss           | 1.2          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=714000, episode_reward=0.71 +/- 0.77\n",
      "Episode length: 5.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 0.71     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 714000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | -0.398   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 139      |\n",
      "|    time_elapsed    | 10275    |\n",
      "|    total_timesteps | 714240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.8          |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 715000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070042238 |\n",
      "|    clip_fraction        | 0.00643      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.3          |\n",
      "|    learning_rate        | 8.59e-05     |\n",
      "|    loss                 | 0.128        |\n",
      "|    n_updates            | 3720         |\n",
      "|    policy_gradient_loss | -0.00754     |\n",
      "|    value_loss           | 0.3          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.68     |\n",
      "|    ep_rew_mean     | -1.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 10367    |\n",
      "|    total_timesteps | 715776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 716000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003028413 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 8.54e-05     |\n",
      "|    loss                 | 27.8         |\n",
      "|    n_updates            | 3728         |\n",
      "|    policy_gradient_loss | 0.00104      |\n",
      "|    value_loss           | 38.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=717000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.40 +/- 3.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 717000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.29     |\n",
      "|    ep_rew_mean     | -1.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 141      |\n",
      "|    time_elapsed    | 10455    |\n",
      "|    total_timesteps | 717312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=718000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 718000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047569242 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.241        |\n",
      "|    explained_variance   | 0.241         |\n",
      "|    learning_rate        | 8.5e-05       |\n",
      "|    loss                 | 31.3          |\n",
      "|    n_updates            | 3736          |\n",
      "|    policy_gradient_loss | -0.00237      |\n",
      "|    value_loss           | 34.5          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.34     |\n",
      "|    ep_rew_mean     | -2.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 10544    |\n",
      "|    total_timesteps | 718848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=719000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.2           |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 719000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.1927785e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.25         |\n",
      "|    explained_variance   | 0.104         |\n",
      "|    learning_rate        | 8.45e-05      |\n",
      "|    loss                 | 37.9          |\n",
      "|    n_updates            | 3744          |\n",
      "|    policy_gradient_loss | -0.00111      |\n",
      "|    value_loss           | 74.6          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=0.73 +/- 0.76\n",
      "Episode length: 6.40 +/- 2.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 0.729    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.76     |\n",
      "|    ep_rew_mean     | -0.519   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 143      |\n",
      "|    time_elapsed    | 10596    |\n",
      "|    total_timesteps | 720384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=721000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 721000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001753554 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.252       |\n",
      "|    explained_variance   | 0.275        |\n",
      "|    learning_rate        | 8.4e-05      |\n",
      "|    loss                 | 28.3         |\n",
      "|    n_updates            | 3752         |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    value_loss           | 33.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.85     |\n",
      "|    ep_rew_mean     | -0.417   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 10648    |\n",
      "|    total_timesteps | 721920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=722000, episode_reward=1.11 +/- 0.03\n",
      "Episode length: 5.60 +/- 1.36\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.6           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 722000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028739424 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.244        |\n",
      "|    explained_variance   | 0.19          |\n",
      "|    learning_rate        | 8.36e-05      |\n",
      "|    loss                 | 6.51          |\n",
      "|    n_updates            | 3760          |\n",
      "|    policy_gradient_loss | -0.00168      |\n",
      "|    value_loss           | 19.2          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=723000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 723000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.13     |\n",
      "|    ep_rew_mean     | -1.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 10699    |\n",
      "|    total_timesteps | 723456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 724000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021420051 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.243        |\n",
      "|    explained_variance   | 0.319         |\n",
      "|    learning_rate        | 8.31e-05      |\n",
      "|    loss                 | 21.3          |\n",
      "|    n_updates            | 3768          |\n",
      "|    policy_gradient_loss | -0.00124      |\n",
      "|    value_loss           | 32.1          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.76     |\n",
      "|    ep_rew_mean     | -0.419   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 10750    |\n",
      "|    total_timesteps | 724992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=0.68 +/- 0.80\n",
      "Episode length: 4.40 +/- 0.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.4           |\n",
      "|    mean_reward          | 0.681         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 725000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00067360624 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.239        |\n",
      "|    explained_variance   | 0.117         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 17.7          |\n",
      "|    n_updates            | 3776          |\n",
      "|    policy_gradient_loss | -0.00227      |\n",
      "|    value_loss           | 19.1          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=726000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 726000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.32     |\n",
      "|    ep_rew_mean     | -2.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 10838    |\n",
      "|    total_timesteps | 726528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=727000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 727000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042482655 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.282        |\n",
      "|    explained_variance   | 0.223         |\n",
      "|    learning_rate        | 8.22e-05      |\n",
      "|    loss                 | 30.3          |\n",
      "|    n_updates            | 3784          |\n",
      "|    policy_gradient_loss | -0.00358      |\n",
      "|    value_loss           | 53            |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=0.71 +/- 0.77\n",
      "Episode length: 5.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 0.71     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 728000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -0.575   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 10926    |\n",
      "|    total_timesteps | 728064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=729000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 729000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003132264 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.262       |\n",
      "|    explained_variance   | -0.808       |\n",
      "|    learning_rate        | 8.17e-05     |\n",
      "|    loss                 | 0.399        |\n",
      "|    n_updates            | 3792         |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 1.38         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.63     |\n",
      "|    ep_rew_mean     | -1.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 11017    |\n",
      "|    total_timesteps | 729600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 730000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009864961 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.245       |\n",
      "|    explained_variance   | 0.277        |\n",
      "|    learning_rate        | 8.13e-05     |\n",
      "|    loss                 | 4.59         |\n",
      "|    n_updates            | 3800         |\n",
      "|    policy_gradient_loss | -0.00571     |\n",
      "|    value_loss           | 18.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=731000, episode_reward=0.73 +/- 0.79\n",
      "Episode length: 6.60 +/- 2.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 0.733    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 731000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.18     |\n",
      "|    ep_rew_mean     | -1.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 11067    |\n",
      "|    total_timesteps | 731136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 732000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043647116 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.259        |\n",
      "|    explained_variance   | 0.303         |\n",
      "|    learning_rate        | 8.08e-05      |\n",
      "|    loss                 | 6.66          |\n",
      "|    n_updates            | 3808          |\n",
      "|    policy_gradient_loss | -0.00178      |\n",
      "|    value_loss           | 33.9          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.87     |\n",
      "|    ep_rew_mean     | -0.556   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 151      |\n",
      "|    time_elapsed    | 11119    |\n",
      "|    total_timesteps | 732672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=733000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 733000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010486878 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.276       |\n",
      "|    explained_variance   | 0.00421      |\n",
      "|    learning_rate        | 8.04e-05     |\n",
      "|    loss                 | 18.5         |\n",
      "|    n_updates            | 3816         |\n",
      "|    policy_gradient_loss | -0.00445     |\n",
      "|    value_loss           | 22.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=734000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 734000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.96     |\n",
      "|    ep_rew_mean     | -0.494   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 11173    |\n",
      "|    total_timesteps | 734208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=1.11 +/- 0.09\n",
      "Episode length: 5.80 +/- 3.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 735000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004912606 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | -0.931       |\n",
      "|    learning_rate        | 7.99e-05     |\n",
      "|    loss                 | 0.747        |\n",
      "|    n_updates            | 3824         |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    value_loss           | 1.15         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.22     |\n",
      "|    ep_rew_mean     | -0.512   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 11228    |\n",
      "|    total_timesteps | 735744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 736000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015439397 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | -0.705       |\n",
      "|    learning_rate        | 7.94e-05     |\n",
      "|    loss                 | 0.455        |\n",
      "|    n_updates            | 3832         |\n",
      "|    policy_gradient_loss | -0.00487     |\n",
      "|    value_loss           | 0.816        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=737000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 737000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.89     |\n",
      "|    ep_rew_mean     | -0.536   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 11316    |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=738000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 738000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046106847 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.255        |\n",
      "|    explained_variance   | 0.255         |\n",
      "|    learning_rate        | 7.9e-05       |\n",
      "|    loss                 | 3.83          |\n",
      "|    n_updates            | 3840          |\n",
      "|    policy_gradient_loss | -0.00138      |\n",
      "|    value_loss           | 17.6          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.17     |\n",
      "|    ep_rew_mean     | -1.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 155      |\n",
      "|    time_elapsed    | 11404    |\n",
      "|    total_timesteps | 738816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=739000, episode_reward=0.76 +/- 0.74\n",
      "Episode length: 7.60 +/- 3.50\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.6           |\n",
      "|    mean_reward          | 0.757         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 739000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014827366 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.261        |\n",
      "|    explained_variance   | 0.262         |\n",
      "|    learning_rate        | 7.85e-05      |\n",
      "|    loss                 | 16.1          |\n",
      "|    n_updates            | 3848          |\n",
      "|    policy_gradient_loss | -0.00185      |\n",
      "|    value_loss           | 18.6          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=740000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.8      |\n",
      "|    ep_rew_mean     | -1.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 11492    |\n",
      "|    total_timesteps | 740352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741000, episode_reward=-19.06 +/- 40.35\n",
      "Episode length: 6.80 +/- 2.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.8          |\n",
      "|    mean_reward          | -19.1        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 741000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019068398 |\n",
      "|    clip_fraction        | 0.0022       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.412        |\n",
      "|    learning_rate        | 7.81e-05     |\n",
      "|    loss                 | 18.5         |\n",
      "|    n_updates            | 3856         |\n",
      "|    policy_gradient_loss | -0.00566     |\n",
      "|    value_loss           | 15.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -1.56    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 11545    |\n",
      "|    total_timesteps | 741888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=742000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 742000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011146956 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.271       |\n",
      "|    explained_variance   | 0.342        |\n",
      "|    learning_rate        | 7.76e-05     |\n",
      "|    loss                 | 12.7         |\n",
      "|    n_updates            | 3864         |\n",
      "|    policy_gradient_loss | -0.00439     |\n",
      "|    value_loss           | 16.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=743000, episode_reward=0.68 +/- 0.79\n",
      "Episode length: 4.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 0.676    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 743000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.08     |\n",
      "|    ep_rew_mean     | -1.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 11634    |\n",
      "|    total_timesteps | 743424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 2.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 744000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012326803 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.265       |\n",
      "|    explained_variance   | 0.296        |\n",
      "|    learning_rate        | 7.71e-05     |\n",
      "|    loss                 | 31.5         |\n",
      "|    n_updates            | 3872         |\n",
      "|    policy_gradient_loss | -0.00509     |\n",
      "|    value_loss           | 34.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.68     |\n",
      "|    ep_rew_mean     | -1.47    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 11724    |\n",
      "|    total_timesteps | 744960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=-19.46 +/- 40.16\n",
      "Episode length: 7.00 +/- 3.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7            |\n",
      "|    mean_reward          | -19.5        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 745000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014866238 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.281       |\n",
      "|    explained_variance   | 0.213        |\n",
      "|    learning_rate        | 7.67e-05     |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 3880         |\n",
      "|    policy_gradient_loss | -0.00443     |\n",
      "|    value_loss           | 52.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=746000, episode_reward=-19.07 +/- 40.36\n",
      "Episode length: 6.40 +/- 2.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -19.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 746000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.7      |\n",
      "|    ep_rew_mean     | -0.58    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 11813    |\n",
      "|    total_timesteps | 746496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=747000, episode_reward=1.16 +/- 0.07\n",
      "Episode length: 7.60 +/- 3.01\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.6           |\n",
      "|    mean_reward          | 1.16          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 747000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010340443 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.257        |\n",
      "|    explained_variance   | -1.36         |\n",
      "|    learning_rate        | 7.62e-05      |\n",
      "|    loss                 | 1.18          |\n",
      "|    n_updates            | 3888          |\n",
      "|    policy_gradient_loss | -0.000179     |\n",
      "|    value_loss           | 2.19          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 748000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.99     |\n",
      "|    ep_rew_mean     | -0.574   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 161      |\n",
      "|    time_elapsed    | 11906    |\n",
      "|    total_timesteps | 748032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 749000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026392867 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.285        |\n",
      "|    explained_variance   | -1.84         |\n",
      "|    learning_rate        | 7.58e-05      |\n",
      "|    loss                 | 0.887         |\n",
      "|    n_updates            | 3896          |\n",
      "|    policy_gradient_loss | -0.00202      |\n",
      "|    value_loss           | 2.61          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -0.373   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 11995    |\n",
      "|    total_timesteps | 749568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=0.70 +/- 0.79\n",
      "Episode length: 5.00 +/- 1.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 750000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000554108 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 7.53e-05    |\n",
      "|    loss                 | 3.95        |\n",
      "|    n_updates            | 3904        |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    value_loss           | 15.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=751000, episode_reward=0.73 +/- 0.75\n",
      "Episode length: 6.40 +/- 3.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 0.729    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 751000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.14     |\n",
      "|    ep_rew_mean     | -0.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 12086    |\n",
      "|    total_timesteps | 751104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.40 +/- 1.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 752000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014111154 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.257       |\n",
      "|    explained_variance   | -0.00399     |\n",
      "|    learning_rate        | 7.48e-05     |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 3912         |\n",
      "|    policy_gradient_loss | -0.00343     |\n",
      "|    value_loss           | 21.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.85     |\n",
      "|    ep_rew_mean     | -0.397   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 12173    |\n",
      "|    total_timesteps | 752640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753000, episode_reward=0.73 +/- 0.79\n",
      "Episode length: 6.60 +/- 3.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 0.733        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 753000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019733103 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.227       |\n",
      "|    explained_variance   | -0.198       |\n",
      "|    learning_rate        | 7.44e-05     |\n",
      "|    loss                 | 0.276        |\n",
      "|    n_updates            | 3920         |\n",
      "|    policy_gradient_loss | -0.00284     |\n",
      "|    value_loss           | 0.748        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=754000, episode_reward=1.15 +/- 0.09\n",
      "Episode length: 7.20 +/- 3.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 1.15     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 754000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.7      |\n",
      "|    ep_rew_mean     | -1.63    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 12265    |\n",
      "|    total_timesteps | 754176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=0.70 +/- 0.77\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 0.705         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 755000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00088295474 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.237        |\n",
      "|    explained_variance   | 0.345         |\n",
      "|    learning_rate        | 7.39e-05      |\n",
      "|    loss                 | 10.7          |\n",
      "|    n_updates            | 3928          |\n",
      "|    policy_gradient_loss | -0.00228      |\n",
      "|    value_loss           | 16.4          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.09     |\n",
      "|    ep_rew_mean     | -1.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 12353    |\n",
      "|    total_timesteps | 755712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=0.73 +/- 0.72\n",
      "Episode length: 6.60 +/- 3.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 0.733        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 756000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015251053 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.249       |\n",
      "|    explained_variance   | 0.429        |\n",
      "|    learning_rate        | 7.35e-05     |\n",
      "|    loss                 | 5.25         |\n",
      "|    n_updates            | 3936         |\n",
      "|    policy_gradient_loss | -0.00434     |\n",
      "|    value_loss           | 15.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=757000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 757000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.75     |\n",
      "|    ep_rew_mean     | -1.49    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 12408    |\n",
      "|    total_timesteps | 757248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=758000, episode_reward=0.72 +/- 0.77\n",
      "Episode length: 6.00 +/- 2.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 0.719        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 758000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013766405 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | 0.491        |\n",
      "|    learning_rate        | 7.3e-05      |\n",
      "|    loss                 | 6.7          |\n",
      "|    n_updates            | 3944         |\n",
      "|    policy_gradient_loss | -0.00398     |\n",
      "|    value_loss           | 27.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.66     |\n",
      "|    ep_rew_mean     | -0.521   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 12460    |\n",
      "|    total_timesteps | 758784   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=759000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.60 +/- 2.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 759000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025069623 |\n",
      "|    clip_fraction        | 0.00179      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.237       |\n",
      "|    explained_variance   | -0.594       |\n",
      "|    learning_rate        | 7.25e-05     |\n",
      "|    loss                 | 0.29         |\n",
      "|    n_updates            | 3952         |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 0.93         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.36     |\n",
      "|    ep_rew_mean     | -2.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 12512    |\n",
      "|    total_timesteps | 760320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=761000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 761000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049816264 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.259        |\n",
      "|    explained_variance   | 0.252         |\n",
      "|    learning_rate        | 7.21e-05      |\n",
      "|    loss                 | 33.3          |\n",
      "|    n_updates            | 3960          |\n",
      "|    policy_gradient_loss | -0.00106      |\n",
      "|    value_loss           | 52.9          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.88     |\n",
      "|    ep_rew_mean     | -0.476   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 12564    |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=762000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.64\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.2           |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 762000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030101606 |\n",
      "|    clip_fraction        | 8.14e-05      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.273        |\n",
      "|    explained_variance   | -0.761        |\n",
      "|    learning_rate        | 7.16e-05      |\n",
      "|    loss                 | 0.371         |\n",
      "|    n_updates            | 3968          |\n",
      "|    policy_gradient_loss | -0.00122      |\n",
      "|    value_loss           | 1.3           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=763000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 763000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.97     |\n",
      "|    ep_rew_mean     | -1.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 12619    |\n",
      "|    total_timesteps | 763392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 764000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005329777 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.263       |\n",
      "|    explained_variance   | 0.33         |\n",
      "|    learning_rate        | 7.12e-05     |\n",
      "|    loss                 | 10.2         |\n",
      "|    n_updates            | 3976         |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    value_loss           | 16.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.18     |\n",
      "|    ep_rew_mean     | -1.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 12672    |\n",
      "|    total_timesteps | 764928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 765000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005351135 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.256       |\n",
      "|    explained_variance   | 0.423        |\n",
      "|    learning_rate        | 7.07e-05     |\n",
      "|    loss                 | 2.65         |\n",
      "|    n_updates            | 3984         |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 14.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=766000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.60 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 766000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.96     |\n",
      "|    ep_rew_mean     | -1.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 173      |\n",
      "|    time_elapsed    | 12725    |\n",
      "|    total_timesteps | 766464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=767000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 767000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002884407 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.278       |\n",
      "|    explained_variance   | 0.067        |\n",
      "|    learning_rate        | 7.02e-05     |\n",
      "|    loss                 | 34.1         |\n",
      "|    n_updates            | 3992         |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    value_loss           | 39.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.95     |\n",
      "|    ep_rew_mean     | -2.55    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 12816    |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=769000, episode_reward=0.33 +/- 0.97\n",
      "Episode length: 6.40 +/- 2.24\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.4           |\n",
      "|    mean_reward          | 0.329         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 769000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015429244 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.293        |\n",
      "|    explained_variance   | 0.394         |\n",
      "|    learning_rate        | 6.98e-05      |\n",
      "|    loss                 | 24.1          |\n",
      "|    n_updates            | 4000          |\n",
      "|    policy_gradient_loss | -0.00225      |\n",
      "|    value_loss           | 45.7          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.71     |\n",
      "|    ep_rew_mean     | -0.56    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 175      |\n",
      "|    time_elapsed    | 12907    |\n",
      "|    total_timesteps | 769536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=0.69 +/- 0.77\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 770000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011278962 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.278       |\n",
      "|    explained_variance   | 0.471        |\n",
      "|    learning_rate        | 6.93e-05     |\n",
      "|    loss                 | 11.4         |\n",
      "|    n_updates            | 4008         |\n",
      "|    policy_gradient_loss | -0.00335     |\n",
      "|    value_loss           | 14.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=771000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 771000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.08     |\n",
      "|    ep_rew_mean     | -0.411   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 12978    |\n",
      "|    total_timesteps | 771072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=0.70 +/- 0.78\n",
      "Episode length: 5.20 +/- 1.47\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 0.7           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 772000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00084101193 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.291        |\n",
      "|    explained_variance   | -1.03         |\n",
      "|    learning_rate        | 6.88e-05      |\n",
      "|    loss                 | 0.669         |\n",
      "|    n_updates            | 4016          |\n",
      "|    policy_gradient_loss | -0.000877     |\n",
      "|    value_loss           | 1.69          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.85     |\n",
      "|    ep_rew_mean     | -0.637   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 177      |\n",
      "|    time_elapsed    | 13069    |\n",
      "|    total_timesteps | 772608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=773000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 773000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018277433 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.284       |\n",
      "|    explained_variance   | 0.471        |\n",
      "|    learning_rate        | 6.84e-05     |\n",
      "|    loss                 | 7.66         |\n",
      "|    n_updates            | 4024         |\n",
      "|    policy_gradient_loss | -0.00677     |\n",
      "|    value_loss           | 14.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=774000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 774000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.67     |\n",
      "|    ep_rew_mean     | -0.581   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 13161    |\n",
      "|    total_timesteps | 774144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 2.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 775000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026542626 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.299       |\n",
      "|    explained_variance   | -0.426       |\n",
      "|    learning_rate        | 6.79e-05     |\n",
      "|    loss                 | 0.2          |\n",
      "|    n_updates            | 4032         |\n",
      "|    policy_gradient_loss | -0.00291     |\n",
      "|    value_loss           | 0.625        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.63     |\n",
      "|    ep_rew_mean     | -0.602   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 13252    |\n",
      "|    total_timesteps | 775680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=1.14 +/- 0.08\n",
      "Episode length: 6.80 +/- 3.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.8          |\n",
      "|    mean_reward          | 1.14         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 776000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037146453 |\n",
      "|    clip_fraction        | 0.00155      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.324       |\n",
      "|    explained_variance   | -0.846       |\n",
      "|    learning_rate        | 6.75e-05     |\n",
      "|    loss                 | 0.261        |\n",
      "|    n_updates            | 4040         |\n",
      "|    policy_gradient_loss | -0.00613     |\n",
      "|    value_loss           | 0.719        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=777000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 777000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.34     |\n",
      "|    ep_rew_mean     | -0.609   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 13343    |\n",
      "|    total_timesteps | 777216   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=778000, episode_reward=0.70 +/- 0.75\n",
      "Episode length: 5.00 +/- 2.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 778000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017598044 |\n",
      "|    clip_fraction        | 0.0245      |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 6.7e-05     |\n",
      "|    loss                 | 0.142       |\n",
      "|    n_updates            | 4048        |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.342       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.84     |\n",
      "|    ep_rew_mean     | -1.43    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 13432    |\n",
      "|    total_timesteps | 778752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=779000, episode_reward=0.71 +/- 0.76\n",
      "Episode length: 5.80 +/- 2.23\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.8           |\n",
      "|    mean_reward          | 0.714         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 779000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054021826 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.276        |\n",
      "|    explained_variance   | 0.172         |\n",
      "|    learning_rate        | 6.65e-05      |\n",
      "|    loss                 | 27.3          |\n",
      "|    n_updates            | 4056          |\n",
      "|    policy_gradient_loss | -0.000926     |\n",
      "|    value_loss           | 54.6          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=0.74 +/- 0.76\n",
      "Episode length: 6.80 +/- 2.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 0.738    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.54     |\n",
      "|    ep_rew_mean     | -0.484   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 13522    |\n",
      "|    total_timesteps | 780288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=781000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 781000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019525867 |\n",
      "|    clip_fraction        | 0.000651     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.272       |\n",
      "|    explained_variance   | 0.0129       |\n",
      "|    learning_rate        | 6.61e-05     |\n",
      "|    loss                 | 0.182        |\n",
      "|    n_updates            | 4064         |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    value_loss           | 0.418        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.82     |\n",
      "|    ep_rew_mean     | -0.338   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 183      |\n",
      "|    time_elapsed    | 13611    |\n",
      "|    total_timesteps | 781824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782000, episode_reward=0.72 +/- 0.80\n",
      "Episode length: 6.20 +/- 2.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 782000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005008125 |\n",
      "|    clip_fraction        | 0.00415     |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | -0.142      |\n",
      "|    learning_rate        | 6.56e-05    |\n",
      "|    loss                 | 0.159       |\n",
      "|    n_updates            | 4072        |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    value_loss           | 0.625       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=783000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 783000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.89     |\n",
      "|    ep_rew_mean     | -1.51    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 13700    |\n",
      "|    total_timesteps | 783360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=0.69 +/- 0.77\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 0.686         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 784000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039099428 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.285        |\n",
      "|    explained_variance   | 0.0119        |\n",
      "|    learning_rate        | 6.52e-05      |\n",
      "|    loss                 | 24.5          |\n",
      "|    n_updates            | 4080          |\n",
      "|    policy_gradient_loss | -0.00127      |\n",
      "|    value_loss           | 20.1          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -0.293   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 185      |\n",
      "|    time_elapsed    | 13787    |\n",
      "|    total_timesteps | 784896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 785000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002111845 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.303       |\n",
      "|    explained_variance   | 0.425        |\n",
      "|    learning_rate        | 6.47e-05     |\n",
      "|    loss                 | 8.77         |\n",
      "|    n_updates            | 4088         |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 15.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=786000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 2.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 786000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.78     |\n",
      "|    ep_rew_mean     | -2.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 13837    |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=787000, episode_reward=1.11 +/- 0.09\n",
      "Episode length: 5.80 +/- 3.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 787000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037203513 |\n",
      "|    clip_fraction        | 0.0035       |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.281       |\n",
      "|    explained_variance   | 0.0101       |\n",
      "|    learning_rate        | 6.42e-05     |\n",
      "|    loss                 | 30.2         |\n",
      "|    n_updates            | 4096         |\n",
      "|    policy_gradient_loss | -0.00932     |\n",
      "|    value_loss           | 40           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.36     |\n",
      "|    ep_rew_mean     | -1.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 187      |\n",
      "|    time_elapsed    | 13889    |\n",
      "|    total_timesteps | 787968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.20 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 788000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019791655 |\n",
      "|    clip_fraction        | 0.00212      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.359        |\n",
      "|    learning_rate        | 6.38e-05     |\n",
      "|    loss                 | 2.54         |\n",
      "|    n_updates            | 4104         |\n",
      "|    policy_gradient_loss | -0.00533     |\n",
      "|    value_loss           | 16.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=789000, episode_reward=0.70 +/- 0.75\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.695    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 789000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.49     |\n",
      "|    ep_rew_mean     | -2.14    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 13938    |\n",
      "|    total_timesteps | 789504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.60 +/- 3.32\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.6           |\n",
      "|    mean_reward          | 1.13          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 790000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044291103 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.298        |\n",
      "|    explained_variance   | 0.234         |\n",
      "|    learning_rate        | 6.33e-05      |\n",
      "|    loss                 | 29.8          |\n",
      "|    n_updates            | 4112          |\n",
      "|    policy_gradient_loss | -0.000865     |\n",
      "|    value_loss           | 35.8          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=791000, episode_reward=0.73 +/- 0.77\n",
      "Episode length: 6.40 +/- 3.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 0.729    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 791000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.24     |\n",
      "|    ep_rew_mean     | -0.268   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 189      |\n",
      "|    time_elapsed    | 13988    |\n",
      "|    total_timesteps | 791040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 792000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036450333 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.294        |\n",
      "|    explained_variance   | 0.447         |\n",
      "|    learning_rate        | 6.29e-05      |\n",
      "|    loss                 | 13.6          |\n",
      "|    n_updates            | 4120          |\n",
      "|    policy_gradient_loss | -0.00195      |\n",
      "|    value_loss           | 29.4          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.13     |\n",
      "|    ep_rew_mean     | -1.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 14037    |\n",
      "|    total_timesteps | 792576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=793000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 793000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006768278 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.276       |\n",
      "|    explained_variance   | 0.419        |\n",
      "|    learning_rate        | 6.24e-05     |\n",
      "|    loss                 | 18.8         |\n",
      "|    n_updates            | 4128         |\n",
      "|    policy_gradient_loss | -0.00302     |\n",
      "|    value_loss           | 30.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=794000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 794000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.96     |\n",
      "|    ep_rew_mean     | -0.314   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 14091    |\n",
      "|    total_timesteps | 794112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 795000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027794947 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.279        |\n",
      "|    explained_variance   | -1.03         |\n",
      "|    learning_rate        | 6.19e-05      |\n",
      "|    loss                 | 1.05          |\n",
      "|    n_updates            | 4136          |\n",
      "|    policy_gradient_loss | -0.00211      |\n",
      "|    value_loss           | 2.16          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.72     |\n",
      "|    ep_rew_mean     | -0.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 14144    |\n",
      "|    total_timesteps | 795648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 796000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046407585 |\n",
      "|    clip_fraction        | 0.00382      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.284       |\n",
      "|    explained_variance   | -0.103       |\n",
      "|    learning_rate        | 6.15e-05     |\n",
      "|    loss                 | 0.256        |\n",
      "|    n_updates            | 4144         |\n",
      "|    policy_gradient_loss | -0.00447     |\n",
      "|    value_loss           | 0.6          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=797000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 797000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.06     |\n",
      "|    ep_rew_mean     | -0.372   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 193      |\n",
      "|    time_elapsed    | 14233    |\n",
      "|    total_timesteps | 797184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798000, episode_reward=0.70 +/- 0.74\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 0.7           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 798000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035014015 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.264        |\n",
      "|    explained_variance   | 0.288         |\n",
      "|    learning_rate        | 6.1e-05       |\n",
      "|    loss                 | 11.2          |\n",
      "|    n_updates            | 4152          |\n",
      "|    policy_gradient_loss | -0.000782     |\n",
      "|    value_loss           | 17.6          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.98     |\n",
      "|    ep_rew_mean     | -0.394   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 14320    |\n",
      "|    total_timesteps | 798720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=799000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 799000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046361878 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.285        |\n",
      "|    explained_variance   | 0.0504        |\n",
      "|    learning_rate        | 6.06e-05      |\n",
      "|    loss                 | 10.5          |\n",
      "|    n_updates            | 4160          |\n",
      "|    policy_gradient_loss | -0.0029       |\n",
      "|    value_loss           | 20.7          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=1.14 +/- 0.08\n",
      "Episode length: 6.80 +/- 3.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.14     |\n",
      "|    ep_rew_mean     | -0.33    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 195      |\n",
      "|    time_elapsed    | 14409    |\n",
      "|    total_timesteps | 800256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=801000, episode_reward=0.70 +/- 0.77\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 0.705        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 801000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021042938 |\n",
      "|    clip_fraction        | 0.00163      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.284       |\n",
      "|    explained_variance   | -0.615       |\n",
      "|    learning_rate        | 6.01e-05     |\n",
      "|    loss                 | 0.291        |\n",
      "|    n_updates            | 4168         |\n",
      "|    policy_gradient_loss | -0.0036      |\n",
      "|    value_loss           | 1.06         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.04     |\n",
      "|    ep_rew_mean     | -0.372   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 14458    |\n",
      "|    total_timesteps | 801792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802000, episode_reward=1.09 +/- 0.02\n",
      "Episode length: 4.80 +/- 0.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 802000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016971943 |\n",
      "|    clip_fraction        | 0.00187      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.267       |\n",
      "|    explained_variance   | 0.225        |\n",
      "|    learning_rate        | 5.96e-05     |\n",
      "|    loss                 | 0.152        |\n",
      "|    n_updates            | 4176         |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    value_loss           | 0.35         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=803000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 803000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.86     |\n",
      "|    ep_rew_mean     | -0.357   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 197      |\n",
      "|    time_elapsed    | 14508    |\n",
      "|    total_timesteps | 803328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 804000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027774958 |\n",
      "|    clip_fraction        | 8.14e-05      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.257        |\n",
      "|    explained_variance   | -0.28         |\n",
      "|    learning_rate        | 5.92e-05      |\n",
      "|    loss                 | 0.515         |\n",
      "|    n_updates            | 4184          |\n",
      "|    policy_gradient_loss | -0.00199      |\n",
      "|    value_loss           | 0.75          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.9      |\n",
      "|    ep_rew_mean     | -1.41    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 14561    |\n",
      "|    total_timesteps | 804864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=1.14 +/- 0.05\n",
      "Episode length: 7.00 +/- 1.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | 1.14        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 805000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001055684 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.265      |\n",
      "|    explained_variance   | 0.0087      |\n",
      "|    learning_rate        | 5.87e-05    |\n",
      "|    loss                 | 4.35        |\n",
      "|    n_updates            | 4192        |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    value_loss           | 21          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=806000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 806000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.72     |\n",
      "|    ep_rew_mean     | -0.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 199      |\n",
      "|    time_elapsed    | 14612    |\n",
      "|    total_timesteps | 806400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=807000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.26\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 807000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059518666 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.233        |\n",
      "|    explained_variance   | -0.661        |\n",
      "|    learning_rate        | 5.83e-05      |\n",
      "|    loss                 | 0.213         |\n",
      "|    n_updates            | 4200          |\n",
      "|    policy_gradient_loss | -0.0032       |\n",
      "|    value_loss           | 1.7           |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -0.315   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 14665    |\n",
      "|    total_timesteps | 807936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=808000, episode_reward=0.69 +/- 0.76\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 808000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025202509 |\n",
      "|    clip_fraction        | 0.000895     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.298        |\n",
      "|    learning_rate        | 5.78e-05     |\n",
      "|    loss                 | 0.139        |\n",
      "|    n_updates            | 4208         |\n",
      "|    policy_gradient_loss | -0.00356     |\n",
      "|    value_loss           | 0.325        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=809000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 809000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.2      |\n",
      "|    ep_rew_mean     | -0.409   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 14754    |\n",
      "|    total_timesteps | 809472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 810000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030035424 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.25         |\n",
      "|    explained_variance   | 0.144         |\n",
      "|    learning_rate        | 5.73e-05      |\n",
      "|    loss                 | 8.34          |\n",
      "|    n_updates            | 4216          |\n",
      "|    policy_gradient_loss | -0.000889     |\n",
      "|    value_loss           | 20            |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=811000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 811000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.12     |\n",
      "|    ep_rew_mean     | -0.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 14843    |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=812000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 812000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.549363e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.231       |\n",
      "|    explained_variance   | 0.231        |\n",
      "|    learning_rate        | 5.69e-05     |\n",
      "|    loss                 | 20.9         |\n",
      "|    n_updates            | 4224         |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    value_loss           | 35.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.21     |\n",
      "|    ep_rew_mean     | -0.328   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 203      |\n",
      "|    time_elapsed    | 14893    |\n",
      "|    total_timesteps | 812544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=813000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.8           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 813000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036532801 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.252        |\n",
      "|    explained_variance   | 0.301         |\n",
      "|    learning_rate        | 5.64e-05      |\n",
      "|    loss                 | 19.6          |\n",
      "|    n_updates            | 4232          |\n",
      "|    policy_gradient_loss | -0.00147      |\n",
      "|    value_loss           | 33.5          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=814000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 814000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.86     |\n",
      "|    ep_rew_mean     | -0.397   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 14945    |\n",
      "|    total_timesteps | 814080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 815000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030373165 |\n",
      "|    clip_fraction        | 0.000326      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.245        |\n",
      "|    explained_variance   | -0.593        |\n",
      "|    learning_rate        | 5.6e-05       |\n",
      "|    loss                 | 0.96          |\n",
      "|    n_updates            | 4240          |\n",
      "|    policy_gradient_loss | -0.00327      |\n",
      "|    value_loss           | 2.88          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.11     |\n",
      "|    ep_rew_mean     | -0.371   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 205      |\n",
      "|    time_elapsed    | 14995    |\n",
      "|    total_timesteps | 815616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 816000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00091409433 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.25         |\n",
      "|    explained_variance   | -0.689        |\n",
      "|    learning_rate        | 5.55e-05      |\n",
      "|    loss                 | 0.345         |\n",
      "|    n_updates            | 4248          |\n",
      "|    policy_gradient_loss | -0.00243      |\n",
      "|    value_loss           | 1.49          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=817000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.60 +/- 2.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 817000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.92     |\n",
      "|    ep_rew_mean     | -0.415   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 15046    |\n",
      "|    total_timesteps | 817152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=818000, episode_reward=1.19 +/- 0.08\n",
      "Episode length: 8.80 +/- 3.31\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 8.8           |\n",
      "|    mean_reward          | 1.19          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 818000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015073898 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.27         |\n",
      "|    explained_variance   | 0.425         |\n",
      "|    learning_rate        | 5.5e-05       |\n",
      "|    loss                 | 7.65          |\n",
      "|    n_updates            | 4256          |\n",
      "|    policy_gradient_loss | -0.00106      |\n",
      "|    value_loss           | 30.1          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.6      |\n",
      "|    ep_rew_mean     | -0.403   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 207      |\n",
      "|    time_elapsed    | 15097    |\n",
      "|    total_timesteps | 818688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=819000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 819000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003005144 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.245       |\n",
      "|    explained_variance   | 0.197        |\n",
      "|    learning_rate        | 5.46e-05     |\n",
      "|    loss                 | 0.317        |\n",
      "|    n_updates            | 4264         |\n",
      "|    policy_gradient_loss | -0.000905    |\n",
      "|    value_loss           | 0.491        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.28     |\n",
      "|    ep_rew_mean     | -0.427   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 15149    |\n",
      "|    total_timesteps | 820224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=821000, episode_reward=1.17 +/- 0.07\n",
      "Episode length: 8.20 +/- 3.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8.2          |\n",
      "|    mean_reward          | 1.17         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 821000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006133444 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.278       |\n",
      "|    explained_variance   | 0.315        |\n",
      "|    learning_rate        | 5.41e-05     |\n",
      "|    loss                 | 8.5          |\n",
      "|    n_updates            | 4272         |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    value_loss           | 34.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.3      |\n",
      "|    ep_rew_mean     | -1.18    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 15200    |\n",
      "|    total_timesteps | 821760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=822000, episode_reward=1.15 +/- 0.08\n",
      "Episode length: 7.20 +/- 3.43\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.2           |\n",
      "|    mean_reward          | 1.15          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 822000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042292025 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.278        |\n",
      "|    explained_variance   | 0.474         |\n",
      "|    learning_rate        | 5.37e-05      |\n",
      "|    loss                 | 6.73          |\n",
      "|    n_updates            | 4280          |\n",
      "|    policy_gradient_loss | -0.00162      |\n",
      "|    value_loss           | 14.8          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=823000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 823000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.28     |\n",
      "|    ep_rew_mean     | -0.347   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 15250    |\n",
      "|    total_timesteps | 823296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=824000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.20 +/- 1.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.2          |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 824000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026691183 |\n",
      "|    clip_fraction        | 0.00057      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.256       |\n",
      "|    explained_variance   | 0.24         |\n",
      "|    learning_rate        | 5.32e-05     |\n",
      "|    loss                 | 0.178        |\n",
      "|    n_updates            | 4288         |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    value_loss           | 0.389        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.63     |\n",
      "|    ep_rew_mean     | -0.462   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 211      |\n",
      "|    time_elapsed    | 15303    |\n",
      "|    total_timesteps | 824832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 825000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022197708 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.0448       |\n",
      "|    learning_rate        | 5.27e-05     |\n",
      "|    loss                 | 0.206        |\n",
      "|    n_updates            | 4296         |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    value_loss           | 0.445        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=826000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 826000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.84     |\n",
      "|    ep_rew_mean     | -0.537   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 15354    |\n",
      "|    total_timesteps | 826368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=827000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.00 +/- 3.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6            |\n",
      "|    mean_reward          | 1.12         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 827000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030916661 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.23        |\n",
      "|    explained_variance   | 0.23         |\n",
      "|    learning_rate        | 5.23e-05     |\n",
      "|    loss                 | 0.166        |\n",
      "|    n_updates            | 4304         |\n",
      "|    policy_gradient_loss | -0.00369     |\n",
      "|    value_loss           | 0.352        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.29     |\n",
      "|    ep_rew_mean     | -1.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 213      |\n",
      "|    time_elapsed    | 15440    |\n",
      "|    total_timesteps | 827904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=1.11 +/- 0.08\n",
      "Episode length: 5.60 +/- 3.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 828000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004648999 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.324        |\n",
      "|    learning_rate        | 5.18e-05     |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 4312         |\n",
      "|    policy_gradient_loss | -0.00201     |\n",
      "|    value_loss           | 34           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=829000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.60 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 829000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.2      |\n",
      "|    ep_rew_mean     | -1.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 15528    |\n",
      "|    total_timesteps | 829440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 830000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00092043326 |\n",
      "|    clip_fraction        | 0.000407      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.233        |\n",
      "|    explained_variance   | 0.427         |\n",
      "|    learning_rate        | 5.14e-05      |\n",
      "|    loss                 | 7.21          |\n",
      "|    n_updates            | 4320          |\n",
      "|    policy_gradient_loss | -0.00743      |\n",
      "|    value_loss           | 15.3          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.97     |\n",
      "|    ep_rew_mean     | -0.394   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 15615    |\n",
      "|    total_timesteps | 830976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=831000, episode_reward=0.75 +/- 0.81\n",
      "Episode length: 7.40 +/- 3.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.4          |\n",
      "|    mean_reward          | 0.752        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 831000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015795411 |\n",
      "|    clip_fraction        | 0.000407     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.275       |\n",
      "|    explained_variance   | -0.141       |\n",
      "|    learning_rate        | 5.09e-05     |\n",
      "|    loss                 | 0.179        |\n",
      "|    n_updates            | 4328         |\n",
      "|    policy_gradient_loss | -0.00482     |\n",
      "|    value_loss           | 0.762        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=0.76 +/- 0.77\n",
      "Episode length: 7.60 +/- 4.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 0.757    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 832000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.85     |\n",
      "|    ep_rew_mean     | -0.497   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 15707    |\n",
      "|    total_timesteps | 832512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=833000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 833000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025259082 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.264       |\n",
      "|    explained_variance   | 0.275        |\n",
      "|    learning_rate        | 5.04e-05     |\n",
      "|    loss                 | 0.127        |\n",
      "|    n_updates            | 4336         |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    value_loss           | 0.296        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=834000, episode_reward=1.11 +/- 0.03\n",
      "Episode length: 5.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 834000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.6      |\n",
      "|    ep_rew_mean     | -0.423   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 15794    |\n",
      "|    total_timesteps | 834048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=0.73 +/- 0.74\n",
      "Episode length: 6.60 +/- 3.32\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 0.733        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 835000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021202534 |\n",
      "|    clip_fraction        | 0.00179      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.223       |\n",
      "|    explained_variance   | 0.403        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 0.128        |\n",
      "|    n_updates            | 4344         |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    value_loss           | 0.284        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.32     |\n",
      "|    ep_rew_mean     | -0.246   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 15846    |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=836000, episode_reward=0.70 +/- 0.79\n",
      "Episode length: 5.00 +/- 1.26\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.695         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 836000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00064476114 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.242        |\n",
      "|    explained_variance   | -0.0525       |\n",
      "|    learning_rate        | 4.95e-05      |\n",
      "|    loss                 | 0.181         |\n",
      "|    n_updates            | 4352          |\n",
      "|    policy_gradient_loss | -0.00046      |\n",
      "|    value_loss           | 0.539         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=837000, episode_reward=0.74 +/- 0.76\n",
      "Episode length: 6.80 +/- 2.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 0.738    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 837000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.18     |\n",
      "|    ep_rew_mean     | -0.329   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 15897    |\n",
      "|    total_timesteps | 837120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=838000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 838000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019620291 |\n",
      "|    clip_fraction        | 0.00106      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | -0.567       |\n",
      "|    learning_rate        | 4.9e-05      |\n",
      "|    loss                 | 0.285        |\n",
      "|    n_updates            | 4360         |\n",
      "|    policy_gradient_loss | -0.00734     |\n",
      "|    value_loss           | 1.07         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.83     |\n",
      "|    ep_rew_mean     | -0.297   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 15984    |\n",
      "|    total_timesteps | 838656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=839000, episode_reward=0.69 +/- 0.77\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 0.686         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 839000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028532976 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.231        |\n",
      "|    explained_variance   | 0.0813        |\n",
      "|    learning_rate        | 4.86e-05      |\n",
      "|    loss                 | 11.9          |\n",
      "|    n_updates            | 4368          |\n",
      "|    policy_gradient_loss | -0.00054      |\n",
      "|    value_loss           | 19.9          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=1.12 +/- 0.08\n",
      "Episode length: 6.20 +/- 3.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.12     |\n",
      "|    ep_rew_mean     | -0.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 221      |\n",
      "|    time_elapsed    | 16075    |\n",
      "|    total_timesteps | 840192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=841000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.8           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 841000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023459368 |\n",
      "|    clip_fraction        | 0.000163      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.237        |\n",
      "|    explained_variance   | -0.431        |\n",
      "|    learning_rate        | 4.81e-05      |\n",
      "|    loss                 | 0.793         |\n",
      "|    n_updates            | 4376          |\n",
      "|    policy_gradient_loss | -0.00233      |\n",
      "|    value_loss           | 0.856         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.04     |\n",
      "|    ep_rew_mean     | -0.352   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 16126    |\n",
      "|    total_timesteps | 841728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=842000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 842000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012649248 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.24        |\n",
      "|    explained_variance   | 0.358        |\n",
      "|    learning_rate        | 4.77e-05     |\n",
      "|    loss                 | 0.152        |\n",
      "|    n_updates            | 4384         |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    value_loss           | 0.302        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=843000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 843000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.75     |\n",
      "|    ep_rew_mean     | -0.419   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 16218    |\n",
      "|    total_timesteps | 843264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=844000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 1.07        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 844000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002286832 |\n",
      "|    clip_fraction        | 0.000244    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 4.72e-05    |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 4392        |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    value_loss           | 0.285       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.86     |\n",
      "|    ep_rew_mean     | -0.377   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 16306    |\n",
      "|    total_timesteps | 844800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=1.15 +/- 0.05\n",
      "Episode length: 7.40 +/- 2.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.4         |\n",
      "|    mean_reward          | 1.15        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 845000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002368084 |\n",
      "|    clip_fraction        | 0.000326    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.242      |\n",
      "|    explained_variance   | -0.319      |\n",
      "|    learning_rate        | 4.67e-05    |\n",
      "|    loss                 | 0.637       |\n",
      "|    n_updates            | 4400        |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    value_loss           | 0.719       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=846000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 846000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.83     |\n",
      "|    ep_rew_mean     | -0.417   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 16396    |\n",
      "|    total_timesteps | 846336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=847000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 847000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00071987975 |\n",
      "|    clip_fraction        | 0.000326      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.212        |\n",
      "|    explained_variance   | 0.307         |\n",
      "|    learning_rate        | 4.63e-05      |\n",
      "|    loss                 | 0.284         |\n",
      "|    n_updates            | 4408          |\n",
      "|    policy_gradient_loss | -0.00444      |\n",
      "|    value_loss           | 17            |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.72     |\n",
      "|    ep_rew_mean     | -0.42    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 16484    |\n",
      "|    total_timesteps | 847872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 848000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011991084 |\n",
      "|    clip_fraction        | 8.14e-05     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.227       |\n",
      "|    explained_variance   | -0.0307      |\n",
      "|    learning_rate        | 4.58e-05     |\n",
      "|    loss                 | 0.476        |\n",
      "|    n_updates            | 4416         |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 0.556        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=849000, episode_reward=0.73 +/- 0.78\n",
      "Episode length: 6.40 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 0.729    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 849000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.72     |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 16572    |\n",
      "|    total_timesteps | 849408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 850000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028751127 |\n",
      "|    clip_fraction        | 0.00179      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.222       |\n",
      "|    explained_variance   | 0.0191       |\n",
      "|    learning_rate        | 4.54e-05     |\n",
      "|    loss                 | 0.192        |\n",
      "|    n_updates            | 4424         |\n",
      "|    policy_gradient_loss | -0.0042      |\n",
      "|    value_loss           | 0.51         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.69     |\n",
      "|    ep_rew_mean     | -0.421   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 16640    |\n",
      "|    total_timesteps | 850944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=851000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 851000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061489674 |\n",
      "|    clip_fraction        | 0.00431      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | 0.391        |\n",
      "|    learning_rate        | 4.49e-05     |\n",
      "|    loss                 | 0.138        |\n",
      "|    n_updates            | 4432         |\n",
      "|    policy_gradient_loss | -0.00396     |\n",
      "|    value_loss           | 0.294        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 852000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.8      |\n",
      "|    ep_rew_mean     | -0.458   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 229      |\n",
      "|    time_elapsed    | 16691    |\n",
      "|    total_timesteps | 852480   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=853000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 853000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020258257 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.282       |\n",
      "|    explained_variance   | 0.351        |\n",
      "|    learning_rate        | 4.44e-05     |\n",
      "|    loss                 | 0.133        |\n",
      "|    n_updates            | 4440         |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    value_loss           | 0.312        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=854000, episode_reward=0.79 +/- 0.76\n",
      "Episode length: 9.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | 0.79     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 854000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.81     |\n",
      "|    ep_rew_mean     | -1.27    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 16741    |\n",
      "|    total_timesteps | 854016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.4           |\n",
      "|    mean_reward          | 1.08          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 855000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024372998 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.256        |\n",
      "|    explained_variance   | 0.417         |\n",
      "|    learning_rate        | 4.4e-05       |\n",
      "|    loss                 | 11.6          |\n",
      "|    n_updates            | 4448          |\n",
      "|    policy_gradient_loss | -0.00033      |\n",
      "|    value_loss           | 15.5          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -1.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 231      |\n",
      "|    time_elapsed    | 16791    |\n",
      "|    total_timesteps | 855552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=856000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.20 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 856000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5009815e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.259        |\n",
      "|    explained_variance   | 0.185         |\n",
      "|    learning_rate        | 4.35e-05      |\n",
      "|    loss                 | 4.91          |\n",
      "|    n_updates            | 4456          |\n",
      "|    policy_gradient_loss | -0.000517     |\n",
      "|    value_loss           | 19.4          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=857000, episode_reward=1.14 +/- 0.09\n",
      "Episode length: 7.00 +/- 3.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 857000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -0.253   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 16843    |\n",
      "|    total_timesteps | 857088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 858000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025803407 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.244        |\n",
      "|    explained_variance   | 0.168         |\n",
      "|    learning_rate        | 4.31e-05      |\n",
      "|    loss                 | 8.36          |\n",
      "|    n_updates            | 4464          |\n",
      "|    policy_gradient_loss | -0.00117      |\n",
      "|    value_loss           | 18.5          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.81     |\n",
      "|    ep_rew_mean     | -0.378   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 16895    |\n",
      "|    total_timesteps | 858624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=859000, episode_reward=1.11 +/- 0.04\n",
      "Episode length: 5.60 +/- 1.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 859000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005076989 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.0769       |\n",
      "|    learning_rate        | 4.26e-05     |\n",
      "|    loss                 | 0.376        |\n",
      "|    n_updates            | 4472         |\n",
      "|    policy_gradient_loss | -0.00247     |\n",
      "|    value_loss           | 0.467        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=1.08 +/- 0.01\n",
      "Episode length: 4.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 1.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.99     |\n",
      "|    ep_rew_mean     | -0.334   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 16982    |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=861000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 861000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005160355 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.283       |\n",
      "|    explained_variance   | 0.334        |\n",
      "|    learning_rate        | 4.21e-05     |\n",
      "|    loss                 | 5.55         |\n",
      "|    n_updates            | 4480         |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    value_loss           | 17.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.14     |\n",
      "|    ep_rew_mean     | -0.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 17071    |\n",
      "|    total_timesteps | 861696   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=862000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 862000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010414681 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.268        |\n",
      "|    explained_variance   | 0.296         |\n",
      "|    learning_rate        | 4.17e-05      |\n",
      "|    loss                 | 27            |\n",
      "|    n_updates            | 4488          |\n",
      "|    policy_gradient_loss | -0.000945     |\n",
      "|    value_loss           | 34.2          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=863000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 863000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.12     |\n",
      "|    ep_rew_mean     | -1.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 17159    |\n",
      "|    total_timesteps | 863232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=1.13 +/- 0.07\n",
      "Episode length: 6.60 +/- 2.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 1.13         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 864000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.931981e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.244        |\n",
      "|    learning_rate        | 4.12e-05     |\n",
      "|    loss                 | 3.4          |\n",
      "|    n_updates            | 4496         |\n",
      "|    policy_gradient_loss | -0.000809    |\n",
      "|    value_loss           | 17.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.03     |\n",
      "|    ep_rew_mean     | -0.273   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 237      |\n",
      "|    time_elapsed    | 17247    |\n",
      "|    total_timesteps | 864768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=865000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.00 +/- 3.10\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6             |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 865000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00064180954 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.249        |\n",
      "|    explained_variance   | 0.304         |\n",
      "|    learning_rate        | 4.08e-05      |\n",
      "|    loss                 | 0.216         |\n",
      "|    n_updates            | 4504          |\n",
      "|    policy_gradient_loss | -0.00152      |\n",
      "|    value_loss           | 0.387         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=866000, episode_reward=0.72 +/- 0.77\n",
      "Episode length: 6.00 +/- 2.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 0.719    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 866000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.81     |\n",
      "|    ep_rew_mean     | -0.378   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 17335    |\n",
      "|    total_timesteps | 866304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=867000, episode_reward=0.74 +/- 0.76\n",
      "Episode length: 6.80 +/- 2.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.8          |\n",
      "|    mean_reward          | 0.738        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 867000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015563788 |\n",
      "|    clip_fraction        | 0.00114      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.359        |\n",
      "|    learning_rate        | 4.03e-05     |\n",
      "|    loss                 | 0.164        |\n",
      "|    n_updates            | 4512         |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    value_loss           | 0.317        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -0.315   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 239      |\n",
      "|    time_elapsed    | 17425    |\n",
      "|    total_timesteps | 867840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=868000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 868000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017680858 |\n",
      "|    clip_fraction        | 0.000326     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.265       |\n",
      "|    explained_variance   | 0.365        |\n",
      "|    learning_rate        | 3.98e-05     |\n",
      "|    loss                 | 0.129        |\n",
      "|    n_updates            | 4520         |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 0.287        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=869000, episode_reward=-19.03 +/- 40.40\n",
      "Episode length: 8.00 +/- 2.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -19      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 869000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.59     |\n",
      "|    ep_rew_mean     | -0.383   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 17515    |\n",
      "|    total_timesteps | 869376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 870000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026961372 |\n",
      "|    clip_fraction        | 0.00106      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 3.94e-05     |\n",
      "|    loss                 | 0.118        |\n",
      "|    n_updates            | 4528         |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 0.259        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.8      |\n",
      "|    ep_rew_mean     | -0.438   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 17602    |\n",
      "|    total_timesteps | 870912   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=871000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 871000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013785278 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.248       |\n",
      "|    explained_variance   | 0.425        |\n",
      "|    learning_rate        | 3.89e-05     |\n",
      "|    loss                 | 0.133        |\n",
      "|    n_updates            | 4536         |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    value_loss           | 0.287        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=872000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.60 +/- 3.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 872000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.11     |\n",
      "|    ep_rew_mean     | -0.251   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 17691    |\n",
      "|    total_timesteps | 872448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=873000, episode_reward=1.15 +/- 0.05\n",
      "Episode length: 7.20 +/- 1.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 873000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017752514 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.29        |\n",
      "|    explained_variance   | -0.2         |\n",
      "|    learning_rate        | 3.85e-05     |\n",
      "|    loss                 | 0.742        |\n",
      "|    n_updates            | 4544         |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    value_loss           | 0.9          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.91     |\n",
      "|    ep_rew_mean     | -0.395   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 243      |\n",
      "|    time_elapsed    | 17778    |\n",
      "|    total_timesteps | 873984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=874000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 874000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052211545 |\n",
      "|    clip_fraction        | 0.00358      |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.463        |\n",
      "|    learning_rate        | 3.8e-05      |\n",
      "|    loss                 | 0.121        |\n",
      "|    n_updates            | 4552         |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    value_loss           | 0.278        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=875000, episode_reward=0.73 +/- 0.76\n",
      "Episode length: 6.60 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 0.733    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 875000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.74     |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 17867    |\n",
      "|    total_timesteps | 875520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 876000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009024983 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | 0.217        |\n",
      "|    learning_rate        | 3.75e-05     |\n",
      "|    loss                 | 0.132        |\n",
      "|    n_updates            | 4560         |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    value_loss           | 0.399        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=877000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 877000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.04     |\n",
      "|    ep_rew_mean     | -0.292   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 17953    |\n",
      "|    total_timesteps | 877056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=878000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 878000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021093905 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.178        |\n",
      "|    learning_rate        | 3.71e-05     |\n",
      "|    loss                 | 0.199        |\n",
      "|    n_updates            | 4568         |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    value_loss           | 0.421        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.95     |\n",
      "|    ep_rew_mean     | -0.315   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 18007    |\n",
      "|    total_timesteps | 878592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=879000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 2.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 879000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001910842 |\n",
      "|    clip_fraction        | 0.000163    |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.258      |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 3.66e-05    |\n",
      "|    loss                 | 0.131       |\n",
      "|    n_updates            | 4576        |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    value_loss           | 0.271       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=0.75 +/- 0.77\n",
      "Episode length: 7.40 +/- 2.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 0.752    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.99     |\n",
      "|    ep_rew_mean     | -0.354   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 247      |\n",
      "|    time_elapsed    | 18061    |\n",
      "|    total_timesteps | 880128   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=881000, episode_reward=1.11 +/- 0.03\n",
      "Episode length: 5.80 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 881000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003966218 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.26        |\n",
      "|    explained_variance   | -0.453       |\n",
      "|    learning_rate        | 3.62e-05     |\n",
      "|    loss                 | 0.198        |\n",
      "|    n_updates            | 4584         |\n",
      "|    policy_gradient_loss | -0.00354     |\n",
      "|    value_loss           | 1.05         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -0.415   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 18113    |\n",
      "|    total_timesteps | 881664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=882000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.20 +/- 1.47\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 882000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014421988 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.26         |\n",
      "|    explained_variance   | 0.378         |\n",
      "|    learning_rate        | 3.57e-05      |\n",
      "|    loss                 | 5.31          |\n",
      "|    n_updates            | 4592          |\n",
      "|    policy_gradient_loss | -0.000352     |\n",
      "|    value_loss           | 15.9          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=883000, episode_reward=1.11 +/- 0.08\n",
      "Episode length: 5.60 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 883000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.87     |\n",
      "|    ep_rew_mean     | -0.296   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 249      |\n",
      "|    time_elapsed    | 18167    |\n",
      "|    total_timesteps | 883200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=884000, episode_reward=-19.04 +/- 40.36\n",
      "Episode length: 7.80 +/- 2.79\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.8           |\n",
      "|    mean_reward          | -19           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 884000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010219783 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.245        |\n",
      "|    explained_variance   | 0.381         |\n",
      "|    learning_rate        | 3.52e-05      |\n",
      "|    loss                 | 9.13          |\n",
      "|    n_updates            | 4600          |\n",
      "|    policy_gradient_loss | -0.00133      |\n",
      "|    value_loss           | 15.8          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.89     |\n",
      "|    ep_rew_mean     | -0.376   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 18219    |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=0.33 +/- 0.93\n",
      "Episode length: 6.60 +/- 2.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 0.333        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 885000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014598059 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | 0.463        |\n",
      "|    learning_rate        | 3.48e-05     |\n",
      "|    loss                 | 0.124        |\n",
      "|    n_updates            | 4608         |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 0.259        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=886000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 886000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.17     |\n",
      "|    ep_rew_mean     | -0.249   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 251      |\n",
      "|    time_elapsed    | 18272    |\n",
      "|    total_timesteps | 886272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887000, episode_reward=1.13 +/- 0.08\n",
      "Episode length: 6.60 +/- 3.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.6           |\n",
      "|    mean_reward          | 1.13          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 887000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047010466 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.257        |\n",
      "|    explained_variance   | 0.29          |\n",
      "|    learning_rate        | 3.43e-05      |\n",
      "|    loss                 | 12            |\n",
      "|    n_updates            | 4616          |\n",
      "|    policy_gradient_loss | -0.000833     |\n",
      "|    value_loss           | 17.4          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.94     |\n",
      "|    ep_rew_mean     | -0.295   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 18324    |\n",
      "|    total_timesteps | 887808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=1.16 +/- 0.08\n",
      "Episode length: 7.80 +/- 3.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.8          |\n",
      "|    mean_reward          | 1.16         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 888000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.123039e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.276       |\n",
      "|    explained_variance   | 0.454        |\n",
      "|    learning_rate        | 3.39e-05     |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 4624         |\n",
      "|    policy_gradient_loss | -0.000792    |\n",
      "|    value_loss           | 14.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=889000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 1.14     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 889000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.13     |\n",
      "|    ep_rew_mean     | -0.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 253      |\n",
      "|    time_elapsed    | 18374    |\n",
      "|    total_timesteps | 889344   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=890000, episode_reward=1.15 +/- 0.08\n",
      "Episode length: 7.40 +/- 3.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.4          |\n",
      "|    mean_reward          | 1.15         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 890000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008247804 |\n",
      "|    clip_fraction        | 0.000163     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.257       |\n",
      "|    explained_variance   | 0.429        |\n",
      "|    learning_rate        | 3.34e-05     |\n",
      "|    loss                 | 0.132        |\n",
      "|    n_updates            | 4632         |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 0.317        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.87     |\n",
      "|    ep_rew_mean     | -1.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 18463    |\n",
      "|    total_timesteps | 890880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=891000, episode_reward=1.13 +/- 0.07\n",
      "Episode length: 6.40 +/- 2.94\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.4           |\n",
      "|    mean_reward          | 1.13          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 891000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038551388 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.271        |\n",
      "|    explained_variance   | 0.0337        |\n",
      "|    learning_rate        | 3.29e-05      |\n",
      "|    loss                 | 3.7           |\n",
      "|    n_updates            | 4640          |\n",
      "|    policy_gradient_loss | -0.000809     |\n",
      "|    value_loss           | 21.1          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=892000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 892000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.92     |\n",
      "|    ep_rew_mean     | -1.41    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 255      |\n",
      "|    time_elapsed    | 18550    |\n",
      "|    total_timesteps | 892416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=893000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 893000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023467078 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.289        |\n",
      "|    explained_variance   | 0.277         |\n",
      "|    learning_rate        | 3.25e-05      |\n",
      "|    loss                 | 14.9          |\n",
      "|    n_updates            | 4648          |\n",
      "|    policy_gradient_loss | -0.00108      |\n",
      "|    value_loss           | 34.3          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.88     |\n",
      "|    ep_rew_mean     | -0.276   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 18639    |\n",
      "|    total_timesteps | 893952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=894000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 894000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022602237 |\n",
      "|    clip_fraction        | 8.14e-05      |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.285        |\n",
      "|    explained_variance   | -0.599        |\n",
      "|    learning_rate        | 3.2e-05       |\n",
      "|    loss                 | 0.729         |\n",
      "|    n_updates            | 4656          |\n",
      "|    policy_gradient_loss | -0.00216      |\n",
      "|    value_loss           | 1.4           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=895000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.20 +/- 2.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 895000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.58     |\n",
      "|    ep_rew_mean     | -0.0995  |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 257      |\n",
      "|    time_elapsed    | 18725    |\n",
      "|    total_timesteps | 895488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=-19.09 +/- 40.36\n",
      "Episode length: 5.60 +/- 2.06\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.6           |\n",
      "|    mean_reward          | -19.1         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 896000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026065577 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.269        |\n",
      "|    explained_variance   | -0.316        |\n",
      "|    learning_rate        | 3.16e-05      |\n",
      "|    loss                 | 0.479         |\n",
      "|    n_updates            | 4664          |\n",
      "|    policy_gradient_loss | -0.000901     |\n",
      "|    value_loss           | 1.63          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=897000, episode_reward=1.13 +/- 0.06\n",
      "Episode length: 6.60 +/- 2.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 897000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.16     |\n",
      "|    ep_rew_mean     | -0.27    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 18812    |\n",
      "|    total_timesteps | 897024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=898000, episode_reward=0.73 +/- 0.75\n",
      "Episode length: 6.40 +/- 3.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 0.729        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 898000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037351444 |\n",
      "|    clip_fraction        | 0.000651     |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.261       |\n",
      "|    explained_variance   | 0.402        |\n",
      "|    learning_rate        | 3.11e-05     |\n",
      "|    loss                 | 0.134        |\n",
      "|    n_updates            | 4672         |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 0.3          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.92     |\n",
      "|    ep_rew_mean     | -1.27    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 259      |\n",
      "|    time_elapsed    | 18900    |\n",
      "|    total_timesteps | 898560   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=899000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 899000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035485078 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.244        |\n",
      "|    explained_variance   | 0.443         |\n",
      "|    learning_rate        | 3.06e-05      |\n",
      "|    loss                 | 9.21          |\n",
      "|    n_updates            | 4680          |\n",
      "|    policy_gradient_loss | 0.000421      |\n",
      "|    value_loss           | 15            |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.28     |\n",
      "|    ep_rew_mean     | -0.287   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 18988    |\n",
      "|    total_timesteps | 900096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=901000, episode_reward=0.70 +/- 0.74\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 0.7           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 901000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012693259 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.279        |\n",
      "|    explained_variance   | 0.236         |\n",
      "|    learning_rate        | 3.02e-05      |\n",
      "|    loss                 | 11.6          |\n",
      "|    n_updates            | 4688          |\n",
      "|    policy_gradient_loss | -0.00101      |\n",
      "|    value_loss           | 18            |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.97     |\n",
      "|    ep_rew_mean     | -0.334   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 261      |\n",
      "|    time_elapsed    | 19078    |\n",
      "|    total_timesteps | 901632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=902000, episode_reward=0.69 +/- 0.76\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 902000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002873308 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.248       |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 2.97e-05     |\n",
      "|    loss                 | 0.138        |\n",
      "|    n_updates            | 4696         |\n",
      "|    policy_gradient_loss | -0.000748    |\n",
      "|    value_loss           | 0.276        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=903000, episode_reward=0.76 +/- 0.79\n",
      "Episode length: 7.60 +/- 3.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 0.757    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 903000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.82     |\n",
      "|    ep_rew_mean     | -1.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 19165    |\n",
      "|    total_timesteps | 903168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=904000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 904000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017422852 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.261        |\n",
      "|    explained_variance   | 0.183         |\n",
      "|    learning_rate        | 2.92e-05      |\n",
      "|    loss                 | 12            |\n",
      "|    n_updates            | 4704          |\n",
      "|    policy_gradient_loss | -0.000346     |\n",
      "|    value_loss           | 19            |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -0.355   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 263      |\n",
      "|    time_elapsed    | 19253    |\n",
      "|    total_timesteps | 904704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=905000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.53\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6             |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 905000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8878685e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.269        |\n",
      "|    explained_variance   | 0.528         |\n",
      "|    learning_rate        | 2.88e-05      |\n",
      "|    loss                 | 8.6           |\n",
      "|    n_updates            | 4712          |\n",
      "|    policy_gradient_loss | -0.000227     |\n",
      "|    value_loss           | 13.6          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=906000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 906000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.28     |\n",
      "|    ep_rew_mean     | -1.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 19342    |\n",
      "|    total_timesteps | 906240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=907000, episode_reward=0.68 +/- 0.78\n",
      "Episode length: 4.40 +/- 0.80\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 4.4            |\n",
      "|    mean_reward          | 0.681          |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 907000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.33772455e-05 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.4            |\n",
      "|    entropy_loss         | -0.261         |\n",
      "|    explained_variance   | 0.356          |\n",
      "|    learning_rate        | 2.83e-05       |\n",
      "|    loss                 | 10.9           |\n",
      "|    n_updates            | 4720           |\n",
      "|    policy_gradient_loss | -0.000413      |\n",
      "|    value_loss           | 16             |\n",
      "--------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.9      |\n",
      "|    ep_rew_mean     | -0.376   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 265      |\n",
      "|    time_elapsed    | 19430    |\n",
      "|    total_timesteps | 907776   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=908000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.00 +/- 1.55\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 908000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014753589 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.255        |\n",
      "|    explained_variance   | 0.358         |\n",
      "|    learning_rate        | 2.79e-05      |\n",
      "|    loss                 | 0.145         |\n",
      "|    n_updates            | 4728          |\n",
      "|    policy_gradient_loss | -0.000776     |\n",
      "|    value_loss           | 0.328         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=909000, episode_reward=1.17 +/- 0.09\n",
      "Episode length: 8.00 +/- 3.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | 1.17     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 909000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.23     |\n",
      "|    ep_rew_mean     | -0.408   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 19520    |\n",
      "|    total_timesteps | 909312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=0.75 +/- 0.80\n",
      "Episode length: 7.20 +/- 2.32\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 0.748        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 910000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.179395e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.26        |\n",
      "|    explained_variance   | -0.222       |\n",
      "|    learning_rate        | 2.74e-05     |\n",
      "|    loss                 | 0.426        |\n",
      "|    n_updates            | 4736         |\n",
      "|    policy_gradient_loss | -0.000678    |\n",
      "|    value_loss           | 1.02         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | -1.41    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 267      |\n",
      "|    time_elapsed    | 19608    |\n",
      "|    total_timesteps | 910848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=911000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 1.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 911000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.667172e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.245       |\n",
      "|    explained_variance   | 0.164        |\n",
      "|    learning_rate        | 2.69e-05     |\n",
      "|    loss                 | 13.6         |\n",
      "|    n_updates            | 4744         |\n",
      "|    policy_gradient_loss | -0.000197    |\n",
      "|    value_loss           | 18.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 912000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | -0.458   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 19661    |\n",
      "|    total_timesteps | 912384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=913000, episode_reward=0.74 +/- 0.76\n",
      "Episode length: 6.80 +/- 3.06\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.8           |\n",
      "|    mean_reward          | 0.738         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 913000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1612002e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.257        |\n",
      "|    explained_variance   | 0.199         |\n",
      "|    learning_rate        | 2.65e-05      |\n",
      "|    loss                 | 10.5          |\n",
      "|    n_updates            | 4752          |\n",
      "|    policy_gradient_loss | -0.000359     |\n",
      "|    value_loss           | 19.3          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.3      |\n",
      "|    ep_rew_mean     | -0.246   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 269      |\n",
      "|    time_elapsed    | 19714    |\n",
      "|    total_timesteps | 913920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=914000, episode_reward=1.16 +/- 0.06\n",
      "Episode length: 7.80 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.8           |\n",
      "|    mean_reward          | 1.16          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 914000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3077981e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.246        |\n",
      "|    explained_variance   | 0.418         |\n",
      "|    learning_rate        | 2.6e-05       |\n",
      "|    loss                 | 13.4          |\n",
      "|    n_updates            | 4760          |\n",
      "|    policy_gradient_loss | -0.000264     |\n",
      "|    value_loss           | 30.3          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.60 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 915000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.17     |\n",
      "|    ep_rew_mean     | -1.34    |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 19802    |\n",
      "|    total_timesteps | 915456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=916000, episode_reward=0.77 +/- 0.76\n",
      "Episode length: 8.20 +/- 3.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8.2          |\n",
      "|    mean_reward          | 0.771        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 916000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.334783e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.264       |\n",
      "|    explained_variance   | 0.00898      |\n",
      "|    learning_rate        | 2.56e-05     |\n",
      "|    loss                 | 18           |\n",
      "|    n_updates            | 4768         |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 21.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.83     |\n",
      "|    ep_rew_mean     | -0.317   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 271      |\n",
      "|    time_elapsed    | 19854    |\n",
      "|    total_timesteps | 916992   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=917000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.8           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 917000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1866988e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.223        |\n",
      "|    explained_variance   | 0.386         |\n",
      "|    learning_rate        | 2.51e-05      |\n",
      "|    loss                 | 0.143         |\n",
      "|    n_updates            | 4776          |\n",
      "|    policy_gradient_loss | -0.000351     |\n",
      "|    value_loss           | 0.329         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=918000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.20 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 918000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.98     |\n",
      "|    ep_rew_mean     | -0.434   |\n",
      "| time/              |          |\n",
      "|    fps             | 20       |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 19904    |\n",
      "|    total_timesteps | 918528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=919000, episode_reward=0.33 +/- 0.95\n",
      "Episode length: 6.60 +/- 2.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 0.333        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 919000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.734502e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.254       |\n",
      "|    explained_variance   | -0.376       |\n",
      "|    learning_rate        | 2.46e-05     |\n",
      "|    loss                 | 0.597        |\n",
      "|    n_updates            | 4784         |\n",
      "|    policy_gradient_loss | -0.000471    |\n",
      "|    value_loss           | 0.963        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=0.73 +/- 0.81\n",
      "Episode length: 6.40 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 0.729    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.17     |\n",
      "|    ep_rew_mean     | -1.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 273      |\n",
      "|    time_elapsed    | 19954    |\n",
      "|    total_timesteps | 920064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=921000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 921000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001979719 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.249       |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 2.42e-05     |\n",
      "|    loss                 | 9.2          |\n",
      "|    n_updates            | 4792         |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 19.4         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.26     |\n",
      "|    ep_rew_mean     | -1.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 20006    |\n",
      "|    total_timesteps | 921600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=922000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 1.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 922000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.129732e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | 0.155        |\n",
      "|    learning_rate        | 2.37e-05     |\n",
      "|    loss                 | 6.59         |\n",
      "|    n_updates            | 4800         |\n",
      "|    policy_gradient_loss | -0.00017     |\n",
      "|    value_loss           | 19.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=923000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.00 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 923000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.59     |\n",
      "|    ep_rew_mean     | -0.363   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 275      |\n",
      "|    time_elapsed    | 20057    |\n",
      "|    total_timesteps | 923136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=1.10 +/- 0.03\n",
      "Episode length: 5.00 +/- 1.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 924000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001703659 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.223       |\n",
      "|    explained_variance   | 0.317        |\n",
      "|    learning_rate        | 2.33e-05     |\n",
      "|    loss                 | 0.16         |\n",
      "|    n_updates            | 4808         |\n",
      "|    policy_gradient_loss | -0.000797    |\n",
      "|    value_loss           | 0.359        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.82     |\n",
      "|    ep_rew_mean     | -0.218   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 20109    |\n",
      "|    total_timesteps | 924672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=925000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.80 +/- 1.17\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 925000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021267838 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.205        |\n",
      "|    explained_variance   | -0.128        |\n",
      "|    learning_rate        | 2.28e-05      |\n",
      "|    loss                 | 0.327         |\n",
      "|    n_updates            | 4816          |\n",
      "|    policy_gradient_loss | -0.000816     |\n",
      "|    value_loss           | 0.8           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=926000, episode_reward=1.12 +/- 0.08\n",
      "Episode length: 6.20 +/- 3.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 926000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -0.413   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 277      |\n",
      "|    time_elapsed    | 20161    |\n",
      "|    total_timesteps | 926208   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=927000, episode_reward=1.13 +/- 0.05\n",
      "Episode length: 6.60 +/- 2.24\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.6           |\n",
      "|    mean_reward          | 1.13          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 927000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019289972 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.258        |\n",
      "|    explained_variance   | -0.226        |\n",
      "|    learning_rate        | 2.23e-05      |\n",
      "|    loss                 | 0.77          |\n",
      "|    n_updates            | 4824          |\n",
      "|    policy_gradient_loss | -0.000605     |\n",
      "|    value_loss           | 1.68          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.24     |\n",
      "|    ep_rew_mean     | -0.368   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 20249    |\n",
      "|    total_timesteps | 927744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.40 +/- 1.96\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 928000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014819014 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.247        |\n",
      "|    explained_variance   | -0.281        |\n",
      "|    learning_rate        | 2.19e-05      |\n",
      "|    loss                 | 0.445         |\n",
      "|    n_updates            | 4832          |\n",
      "|    policy_gradient_loss | -0.00121      |\n",
      "|    value_loss           | 0.843         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=929000, episode_reward=1.12 +/- 0.06\n",
      "Episode length: 6.00 +/- 2.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 929000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.47     |\n",
      "|    ep_rew_mean     | -0.122   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 279      |\n",
      "|    time_elapsed    | 20336    |\n",
      "|    total_timesteps | 929280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=0.72 +/- 0.78\n",
      "Episode length: 6.20 +/- 2.86\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.2           |\n",
      "|    mean_reward          | 0.724         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 930000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3037034e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.232        |\n",
      "|    explained_variance   | 0.0214        |\n",
      "|    learning_rate        | 2.14e-05      |\n",
      "|    loss                 | 0.204         |\n",
      "|    n_updates            | 4840          |\n",
      "|    policy_gradient_loss | -0.000701     |\n",
      "|    value_loss           | 0.66          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.13     |\n",
      "|    ep_rew_mean     | -0.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 20423    |\n",
      "|    total_timesteps | 930816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=931000, episode_reward=0.72 +/- 0.77\n",
      "Episode length: 6.20 +/- 2.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.2           |\n",
      "|    mean_reward          | 0.724         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 931000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037320986 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.249        |\n",
      "|    explained_variance   | 0.352         |\n",
      "|    learning_rate        | 2.1e-05       |\n",
      "|    loss                 | 0.154         |\n",
      "|    n_updates            | 4848          |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    value_loss           | 0.318         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=932000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 932000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.14     |\n",
      "|    ep_rew_mean     | -2.31    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 281      |\n",
      "|    time_elapsed    | 20510    |\n",
      "|    total_timesteps | 932352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=933000, episode_reward=0.70 +/- 0.74\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 0.7           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 933000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017426322 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.238        |\n",
      "|    explained_variance   | 0.26          |\n",
      "|    learning_rate        | 2.05e-05      |\n",
      "|    loss                 | 29.2          |\n",
      "|    n_updates            | 4856          |\n",
      "|    policy_gradient_loss | -0.000389     |\n",
      "|    value_loss           | 34.9          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.15     |\n",
      "|    ep_rew_mean     | -0.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 20598    |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=934000, episode_reward=0.70 +/- 0.77\n",
      "Episode length: 5.20 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 0.7          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 934000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.308213e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.265       |\n",
      "|    explained_variance   | -0.657       |\n",
      "|    learning_rate        | 2e-05        |\n",
      "|    loss                 | 0.574        |\n",
      "|    n_updates            | 4864         |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    value_loss           | 1.12         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=935000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 935000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.1      |\n",
      "|    ep_rew_mean     | -1.22    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 283      |\n",
      "|    time_elapsed    | 20686    |\n",
      "|    total_timesteps | 935424   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=936000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.40 +/- 1.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 936000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.372897e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 1.96e-05     |\n",
      "|    loss                 | 18.1         |\n",
      "|    n_updates            | 4872         |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 19.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.18     |\n",
      "|    ep_rew_mean     | -0.309   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 20738    |\n",
      "|    total_timesteps | 936960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=937000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 937000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.246345e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | -0.227       |\n",
      "|    learning_rate        | 1.91e-05     |\n",
      "|    loss                 | 0.255        |\n",
      "|    n_updates            | 4880         |\n",
      "|    policy_gradient_loss | -0.000717    |\n",
      "|    value_loss           | 0.877        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=938000, episode_reward=0.72 +/- 0.74\n",
      "Episode length: 6.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 0.719    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 938000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.04     |\n",
      "|    ep_rew_mean     | -2.25    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 285      |\n",
      "|    time_elapsed    | 20790    |\n",
      "|    total_timesteps | 938496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=939000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4             |\n",
      "|    mean_reward          | 1.07          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 939000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025752815 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.248        |\n",
      "|    explained_variance   | 0.0238        |\n",
      "|    learning_rate        | 1.87e-05      |\n",
      "|    loss                 | 9.21          |\n",
      "|    n_updates            | 4888          |\n",
      "|    policy_gradient_loss | -0.00248      |\n",
      "|    value_loss           | 39.2          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.87     |\n",
      "|    ep_rew_mean     | -0.336   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 20841    |\n",
      "|    total_timesteps | 940032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=941000, episode_reward=-19.05 +/- 40.30\n",
      "Episode length: 7.40 +/- 4.45\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.4           |\n",
      "|    mean_reward          | -19           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 941000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021093315 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.237        |\n",
      "|    explained_variance   | 0.251         |\n",
      "|    learning_rate        | 1.82e-05      |\n",
      "|    loss                 | 0.179         |\n",
      "|    n_updates            | 4896          |\n",
      "|    policy_gradient_loss | -0.00104      |\n",
      "|    value_loss           | 0.417         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.93     |\n",
      "|    ep_rew_mean     | -0.335   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 287      |\n",
      "|    time_elapsed    | 20893    |\n",
      "|    total_timesteps | 941568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=942000, episode_reward=0.35 +/- 0.93\n",
      "Episode length: 7.40 +/- 2.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.4           |\n",
      "|    mean_reward          | 0.352         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 942000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.8744174e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.235        |\n",
      "|    explained_variance   | 0.296         |\n",
      "|    learning_rate        | 1.77e-05      |\n",
      "|    loss                 | 0.137         |\n",
      "|    n_updates            | 4904          |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    value_loss           | 0.369         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=943000, episode_reward=0.70 +/- 0.75\n",
      "Episode length: 5.40 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 0.705    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 943000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.8      |\n",
      "|    ep_rew_mean     | -0.458   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 20982    |\n",
      "|    total_timesteps | 943104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=0.70 +/- 0.75\n",
      "Episode length: 5.00 +/- 2.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 944000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007587758 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | 0.386        |\n",
      "|    learning_rate        | 1.73e-05     |\n",
      "|    loss                 | 0.159        |\n",
      "|    n_updates            | 4912         |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    value_loss           | 0.312        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.69     |\n",
      "|    ep_rew_mean     | -0.321   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 289      |\n",
      "|    time_elapsed    | 21034    |\n",
      "|    total_timesteps | 944640   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=945000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 945000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024157744 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.207        |\n",
      "|    explained_variance   | 0.31          |\n",
      "|    learning_rate        | 1.68e-05      |\n",
      "|    loss                 | 0.15          |\n",
      "|    n_updates            | 4920          |\n",
      "|    policy_gradient_loss | -0.000365     |\n",
      "|    value_loss           | 0.379         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=946000, episode_reward=0.74 +/- 0.79\n",
      "Episode length: 6.80 +/- 2.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 0.738    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 946000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.05     |\n",
      "|    ep_rew_mean     | -1.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 21084    |\n",
      "|    total_timesteps | 946176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.8           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 947000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1252817e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.219        |\n",
      "|    explained_variance   | 0.0254        |\n",
      "|    learning_rate        | 1.64e-05      |\n",
      "|    loss                 | 2.45          |\n",
      "|    n_updates            | 4928          |\n",
      "|    policy_gradient_loss | -0.000391     |\n",
      "|    value_loss           | 20.3          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.03     |\n",
      "|    ep_rew_mean     | -1.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 291      |\n",
      "|    time_elapsed    | 21135    |\n",
      "|    total_timesteps | 947712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=0.76 +/- 0.80\n",
      "Episode length: 7.60 +/- 3.14\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.6           |\n",
      "|    mean_reward          | 0.757         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 948000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7875687e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.236        |\n",
      "|    explained_variance   | 0.0318        |\n",
      "|    learning_rate        | 1.59e-05      |\n",
      "|    loss                 | 10.6          |\n",
      "|    n_updates            | 4936          |\n",
      "|    policy_gradient_loss | -0.000289     |\n",
      "|    value_loss           | 20.2          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=949000, episode_reward=0.32 +/- 0.94\n",
      "Episode length: 6.00 +/- 1.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 0.319    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 949000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.82     |\n",
      "|    ep_rew_mean     | -0.278   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 21187    |\n",
      "|    total_timesteps | 949248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=0.73 +/- 0.78\n",
      "Episode length: 6.40 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.4          |\n",
      "|    mean_reward          | 0.729        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 950000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.048816e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | 0.273        |\n",
      "|    learning_rate        | 1.54e-05     |\n",
      "|    loss                 | 0.14         |\n",
      "|    n_updates            | 4944         |\n",
      "|    policy_gradient_loss | -0.000507    |\n",
      "|    value_loss           | 0.459        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8        |\n",
      "|    ep_rew_mean     | -0.313   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 21237    |\n",
      "|    total_timesteps | 950784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=951000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 951000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4507324e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.223        |\n",
      "|    explained_variance   | -0.192        |\n",
      "|    learning_rate        | 1.5e-05       |\n",
      "|    loss                 | 0.241         |\n",
      "|    n_updates            | 4952          |\n",
      "|    policy_gradient_loss | -0.000312     |\n",
      "|    value_loss           | 0.914         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=952000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 952000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.09     |\n",
      "|    ep_rew_mean     | -0.231   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 21289    |\n",
      "|    total_timesteps | 952320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=953000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 953000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018962678 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.232        |\n",
      "|    explained_variance   | -0.228        |\n",
      "|    learning_rate        | 1.45e-05      |\n",
      "|    loss                 | 0.599         |\n",
      "|    n_updates            | 4960          |\n",
      "|    policy_gradient_loss | -0.00108      |\n",
      "|    value_loss           | 0.924         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.94     |\n",
      "|    ep_rew_mean     | -1.28    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 295      |\n",
      "|    time_elapsed    | 21339    |\n",
      "|    total_timesteps | 953856   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=954000, episode_reward=-19.08 +/- 40.37\n",
      "Episode length: 6.20 +/- 1.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.2           |\n",
      "|    mean_reward          | -19.1         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 954000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.4181236e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.223        |\n",
      "|    explained_variance   | 0.00701       |\n",
      "|    learning_rate        | 1.41e-05      |\n",
      "|    loss                 | 10.6          |\n",
      "|    n_updates            | 4968          |\n",
      "|    policy_gradient_loss | -0.000746     |\n",
      "|    value_loss           | 20.9          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=955000, episode_reward=0.72 +/- 0.76\n",
      "Episode length: 6.20 +/- 2.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 0.724    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 955000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.89     |\n",
      "|    ep_rew_mean     | -0.296   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 21389    |\n",
      "|    total_timesteps | 955392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=956000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 956000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0345518e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.212        |\n",
      "|    explained_variance   | -0.0131       |\n",
      "|    learning_rate        | 1.36e-05      |\n",
      "|    loss                 | 0.447         |\n",
      "|    n_updates            | 4976          |\n",
      "|    policy_gradient_loss | -0.00047      |\n",
      "|    value_loss           | 0.708         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.79     |\n",
      "|    ep_rew_mean     | -0.378   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 297      |\n",
      "|    time_elapsed    | 21440    |\n",
      "|    total_timesteps | 956928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=957000, episode_reward=0.73 +/- 0.76\n",
      "Episode length: 6.60 +/- 3.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.6           |\n",
      "|    mean_reward          | 0.733         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 957000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013471779 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.211        |\n",
      "|    explained_variance   | 0.397         |\n",
      "|    learning_rate        | 1.31e-05      |\n",
      "|    loss                 | 0.162         |\n",
      "|    n_updates            | 4984          |\n",
      "|    policy_gradient_loss | -0.000853     |\n",
      "|    value_loss           | 0.344         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=958000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 958000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.82     |\n",
      "|    ep_rew_mean     | -0.358   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 21528    |\n",
      "|    total_timesteps | 958464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=959000, episode_reward=1.14 +/- 0.08\n",
      "Episode length: 6.80 +/- 3.49\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.8           |\n",
      "|    mean_reward          | 1.14          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 959000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9244886e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.217        |\n",
      "|    explained_variance   | 0.254         |\n",
      "|    learning_rate        | 1.27e-05      |\n",
      "|    loss                 | 2.95          |\n",
      "|    n_updates            | 4992          |\n",
      "|    policy_gradient_loss | -0.000281     |\n",
      "|    value_loss           | 17.9          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.11     |\n",
      "|    ep_rew_mean     | -0.271   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 299      |\n",
      "|    time_elapsed    | 21615    |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=961000, episode_reward=0.35 +/- 0.96\n",
      "Episode length: 7.20 +/- 1.72\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.2           |\n",
      "|    mean_reward          | 0.348         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 961000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010440557 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.232        |\n",
      "|    explained_variance   | 0.41          |\n",
      "|    learning_rate        | 1.22e-05      |\n",
      "|    loss                 | 0.146         |\n",
      "|    n_updates            | 5000          |\n",
      "|    policy_gradient_loss | -0.000485     |\n",
      "|    value_loss           | 0.315         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.41     |\n",
      "|    ep_rew_mean     | -0.104   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 21702    |\n",
      "|    total_timesteps | 961536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=962000, episode_reward=1.17 +/- 0.08\n",
      "Episode length: 8.00 +/- 3.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 8            |\n",
      "|    mean_reward          | 1.17         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 962000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.169527e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.221       |\n",
      "|    explained_variance   | 0.23         |\n",
      "|    learning_rate        | 1.18e-05     |\n",
      "|    loss                 | 0.177        |\n",
      "|    n_updates            | 5008         |\n",
      "|    policy_gradient_loss | -0.000264    |\n",
      "|    value_loss           | 0.423        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=963000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 963000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.73     |\n",
      "|    ep_rew_mean     | -0.32    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 301      |\n",
      "|    time_elapsed    | 21751    |\n",
      "|    total_timesteps | 963072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=964000, episode_reward=-19.06 +/- 40.36\n",
      "Episode length: 6.80 +/- 2.48\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.8           |\n",
      "|    mean_reward          | -19.1         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 964000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022511762 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.21         |\n",
      "|    explained_variance   | 0.353         |\n",
      "|    learning_rate        | 1.13e-05      |\n",
      "|    loss                 | 0.138         |\n",
      "|    n_updates            | 5016          |\n",
      "|    policy_gradient_loss | -0.000705     |\n",
      "|    value_loss           | 0.369         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.97     |\n",
      "|    ep_rew_mean     | -0.454   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 21802    |\n",
      "|    total_timesteps | 964608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=965000, episode_reward=1.10 +/- 0.04\n",
      "Episode length: 5.40 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 1.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 965000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.96143e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.23       |\n",
      "|    explained_variance   | 0.00415     |\n",
      "|    learning_rate        | 1.08e-05    |\n",
      "|    loss                 | 14.7        |\n",
      "|    n_updates            | 5024        |\n",
      "|    policy_gradient_loss | -0.000629   |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=966000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.00 +/- 2.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 966000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.07     |\n",
      "|    ep_rew_mean     | -0.352   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 303      |\n",
      "|    time_elapsed    | 21853    |\n",
      "|    total_timesteps | 966144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 967000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2971573e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.229        |\n",
      "|    explained_variance   | 0.314         |\n",
      "|    learning_rate        | 1.04e-05      |\n",
      "|    loss                 | 0.16          |\n",
      "|    n_updates            | 5032          |\n",
      "|    policy_gradient_loss | -0.000731     |\n",
      "|    value_loss           | 0.335         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.31     |\n",
      "|    ep_rew_mean     | -0.266   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 304      |\n",
      "|    time_elapsed    | 21903    |\n",
      "|    total_timesteps | 967680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=968000, episode_reward=0.73 +/- 0.76\n",
      "Episode length: 6.60 +/- 3.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 6.6          |\n",
      "|    mean_reward          | 0.733        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 968000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.960107e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.204       |\n",
      "|    explained_variance   | 0.456        |\n",
      "|    learning_rate        | 9.91e-06     |\n",
      "|    loss                 | 9.66         |\n",
      "|    n_updates            | 5040         |\n",
      "|    policy_gradient_loss | -0.000233    |\n",
      "|    value_loss           | 14.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=969000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 969000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.92     |\n",
      "|    ep_rew_mean     | -0.275   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 305      |\n",
      "|    time_elapsed    | 21991    |\n",
      "|    total_timesteps | 969216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 970000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8627303e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.199        |\n",
      "|    explained_variance   | 0.29          |\n",
      "|    learning_rate        | 9.45e-06      |\n",
      "|    loss                 | 14.5          |\n",
      "|    n_updates            | 5048          |\n",
      "|    policy_gradient_loss | -0.000257     |\n",
      "|    value_loss           | 17.3          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.75     |\n",
      "|    ep_rew_mean     | -0.399   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 22078    |\n",
      "|    total_timesteps | 970752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=971000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 2.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 971000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.45925e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.215      |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 8.99e-06    |\n",
      "|    loss                 | 0.223       |\n",
      "|    n_updates            | 5056        |\n",
      "|    policy_gradient_loss | -0.000141   |\n",
      "|    value_loss           | 0.454       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 972000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.51     |\n",
      "|    ep_rew_mean     | -0.325   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 307      |\n",
      "|    time_elapsed    | 22167    |\n",
      "|    total_timesteps | 972288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=973000, episode_reward=1.14 +/- 0.06\n",
      "Episode length: 6.80 +/- 2.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.8           |\n",
      "|    mean_reward          | 1.14          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 973000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013435404 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.213        |\n",
      "|    explained_variance   | 0.484         |\n",
      "|    learning_rate        | 8.53e-06      |\n",
      "|    loss                 | 0.133         |\n",
      "|    n_updates            | 5064          |\n",
      "|    policy_gradient_loss | -0.000609     |\n",
      "|    value_loss           | 0.257         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.86     |\n",
      "|    ep_rew_mean     | -0.437   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 22255    |\n",
      "|    total_timesteps | 973824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=974000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 974000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.939615e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | 0.222        |\n",
      "|    learning_rate        | 8.07e-06     |\n",
      "|    loss                 | 3.87         |\n",
      "|    n_updates            | 5072         |\n",
      "|    policy_gradient_loss | -0.000205    |\n",
      "|    value_loss           | 35.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=1.10 +/- 0.05\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 975000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.76     |\n",
      "|    ep_rew_mean     | -0.459   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 309      |\n",
      "|    time_elapsed    | 22343    |\n",
      "|    total_timesteps | 975360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.6          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 976000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.147998e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.202       |\n",
      "|    explained_variance   | 0.452        |\n",
      "|    learning_rate        | 7.61e-06     |\n",
      "|    loss                 | 0.149        |\n",
      "|    n_updates            | 5080         |\n",
      "|    policy_gradient_loss | -0.000315    |\n",
      "|    value_loss           | 0.278        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.87     |\n",
      "|    ep_rew_mean     | -0.336   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 22431    |\n",
      "|    total_timesteps | 976896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=977000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.80 +/- 2.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | 1.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 977000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003239389 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.354        |\n",
      "|    learning_rate        | 7.15e-06     |\n",
      "|    loss                 | 0.17         |\n",
      "|    n_updates            | 5088         |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 0.329        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=978000, episode_reward=0.70 +/- 0.75\n",
      "Episode length: 5.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.695    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 978000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.72     |\n",
      "|    ep_rew_mean     | -0.38    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 311      |\n",
      "|    time_elapsed    | 22519    |\n",
      "|    total_timesteps | 978432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=979000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.4           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 979000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0487306e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.237        |\n",
      "|    explained_variance   | -0.525        |\n",
      "|    learning_rate        | 6.69e-06      |\n",
      "|    loss                 | 0.593         |\n",
      "|    n_updates            | 5096          |\n",
      "|    policy_gradient_loss | -0.00026      |\n",
      "|    value_loss           | 1.48          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.98     |\n",
      "|    ep_rew_mean     | -1.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 22604    |\n",
      "|    total_timesteps | 979968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=1.16 +/- 0.08\n",
      "Episode length: 7.60 +/- 3.50\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.6           |\n",
      "|    mean_reward          | 1.16          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 980000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3506622e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.211        |\n",
      "|    explained_variance   | 0.414         |\n",
      "|    learning_rate        | 6.23e-06      |\n",
      "|    loss                 | 4.97          |\n",
      "|    n_updates            | 5104          |\n",
      "|    policy_gradient_loss | -0.000809     |\n",
      "|    value_loss           | 15.5          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=981000, episode_reward=1.11 +/- 0.05\n",
      "Episode length: 5.60 +/- 2.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 1.11     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 981000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.27     |\n",
      "|    ep_rew_mean     | -1.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 22693    |\n",
      "|    total_timesteps | 981504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=982000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 4.6           |\n",
      "|    mean_reward          | 1.09          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 982000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8616904e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.215        |\n",
      "|    explained_variance   | 0.399         |\n",
      "|    learning_rate        | 5.77e-06      |\n",
      "|    loss                 | 8.74          |\n",
      "|    n_updates            | 5112          |\n",
      "|    policy_gradient_loss | -9.07e-05     |\n",
      "|    value_loss           | 15.7          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=983000, episode_reward=0.74 +/- 0.79\n",
      "Episode length: 7.00 +/- 2.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 0.743    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 983000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.01     |\n",
      "|    ep_rew_mean     | -0.293   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 22781    |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=0.75 +/- 0.82\n",
      "Episode length: 7.40 +/- 2.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.4          |\n",
      "|    mean_reward          | 0.752        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 984000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.034777e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.221       |\n",
      "|    explained_variance   | 0.33         |\n",
      "|    learning_rate        | 5.3e-06      |\n",
      "|    loss                 | 8.62         |\n",
      "|    n_updates            | 5120         |\n",
      "|    policy_gradient_loss | -0.000189    |\n",
      "|    value_loss           | 17.1         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.88     |\n",
      "|    ep_rew_mean     | -0.316   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 315      |\n",
      "|    time_elapsed    | 22834    |\n",
      "|    total_timesteps | 984576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=985000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.2          |\n",
      "|    mean_reward          | 1.1          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 985000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.996886e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.214       |\n",
      "|    explained_variance   | 0.423        |\n",
      "|    learning_rate        | 4.84e-06     |\n",
      "|    loss                 | 0.144        |\n",
      "|    n_updates            | 5128         |\n",
      "|    policy_gradient_loss | -0.00026     |\n",
      "|    value_loss           | 0.307        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=986000, episode_reward=0.69 +/- 0.76\n",
      "Episode length: 4.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 0.69     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 986000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.27     |\n",
      "|    ep_rew_mean     | -1.26    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 316      |\n",
      "|    time_elapsed    | 22922    |\n",
      "|    total_timesteps | 986112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987000, episode_reward=0.68 +/- 0.78\n",
      "Episode length: 4.40 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 987000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.436355e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.226       |\n",
      "|    explained_variance   | 0.111        |\n",
      "|    learning_rate        | 4.38e-06     |\n",
      "|    loss                 | 4.31         |\n",
      "|    n_updates            | 5136         |\n",
      "|    policy_gradient_loss | -9.37e-05    |\n",
      "|    value_loss           | 19.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.83     |\n",
      "|    ep_rew_mean     | -0.377   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 317      |\n",
      "|    time_elapsed    | 23009    |\n",
      "|    total_timesteps | 987648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=988000, episode_reward=1.12 +/- 0.04\n",
      "Episode length: 6.00 +/- 1.67\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6             |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 988000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9042636e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.24         |\n",
      "|    explained_variance   | 0.334         |\n",
      "|    learning_rate        | 3.92e-06      |\n",
      "|    loss                 | 0.148         |\n",
      "|    n_updates            | 5144          |\n",
      "|    policy_gradient_loss | -0.000267     |\n",
      "|    value_loss           | 0.32          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=989000, episode_reward=1.07 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 1.07     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 989000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.03     |\n",
      "|    ep_rew_mean     | -0.253   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 23099    |\n",
      "|    total_timesteps | 989184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 990000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.765685e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.215       |\n",
      "|    explained_variance   | -0.263       |\n",
      "|    learning_rate        | 3.46e-06     |\n",
      "|    loss                 | 0.222        |\n",
      "|    n_updates            | 5152         |\n",
      "|    policy_gradient_loss | -0.000247    |\n",
      "|    value_loss           | 0.714        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.29     |\n",
      "|    ep_rew_mean     | -2.35    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 319      |\n",
      "|    time_elapsed    | 23185    |\n",
      "|    total_timesteps | 990720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=991000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 991000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.036142e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.218       |\n",
      "|    explained_variance   | 0.17         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 35.3         |\n",
      "|    n_updates            | 5160         |\n",
      "|    policy_gradient_loss | -9.16e-06    |\n",
      "|    value_loss           | 37.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=1.13 +/- 0.04\n",
      "Episode length: 6.40 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 1.13     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 992000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.83     |\n",
      "|    ep_rew_mean     | -0.337   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 23272    |\n",
      "|    total_timesteps | 992256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=993000, episode_reward=1.08 +/- 0.02\n",
      "Episode length: 4.40 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 1.08         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 993000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.012068e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.22        |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 2.54e-06     |\n",
      "|    loss                 | 0.162        |\n",
      "|    n_updates            | 5168         |\n",
      "|    policy_gradient_loss | -4.19e-05    |\n",
      "|    value_loss           | 0.303        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.23     |\n",
      "|    ep_rew_mean     | -0.388   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 321      |\n",
      "|    time_elapsed    | 23361    |\n",
      "|    total_timesteps | 993792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=994000, episode_reward=1.12 +/- 0.05\n",
      "Episode length: 6.20 +/- 2.04\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 6.2           |\n",
      "|    mean_reward          | 1.12          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 994000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3338868e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.221        |\n",
      "|    explained_variance   | 0.0283        |\n",
      "|    learning_rate        | 2.08e-06      |\n",
      "|    loss                 | 0.485         |\n",
      "|    n_updates            | 5176          |\n",
      "|    policy_gradient_loss | -0.000148     |\n",
      "|    value_loss           | 19.9          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=995000, episode_reward=1.12 +/- 0.07\n",
      "Episode length: 6.20 +/- 3.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 1.12     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 995000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.77     |\n",
      "|    ep_rew_mean     | -0.359   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 322      |\n",
      "|    time_elapsed    | 23413    |\n",
      "|    total_timesteps | 995328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=1.11 +/- 0.06\n",
      "Episode length: 5.80 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.8           |\n",
      "|    mean_reward          | 1.11          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 996000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4054434e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.237        |\n",
      "|    explained_variance   | -0.322        |\n",
      "|    learning_rate        | 1.62e-06      |\n",
      "|    loss                 | 1.09          |\n",
      "|    n_updates            | 5184          |\n",
      "|    policy_gradient_loss | -5.56e-05     |\n",
      "|    value_loss           | 0.965         |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.97     |\n",
      "|    ep_rew_mean     | -0.314   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 23463    |\n",
      "|    total_timesteps | 996864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=997000, episode_reward=1.09 +/- 0.04\n",
      "Episode length: 4.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 1.09         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 997000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.324053e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.4          |\n",
      "|    entropy_loss         | -0.223       |\n",
      "|    explained_variance   | 0.428        |\n",
      "|    learning_rate        | 1.16e-06     |\n",
      "|    loss                 | 0.186        |\n",
      "|    n_updates            | 5192         |\n",
      "|    policy_gradient_loss | -2.15e-05    |\n",
      "|    value_loss           | 0.321        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=998000, episode_reward=1.09 +/- 0.03\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 1.09     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 998000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.05     |\n",
      "|    ep_rew_mean     | -0.252   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 23514    |\n",
      "|    total_timesteps | 998400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999000, episode_reward=1.10 +/- 0.06\n",
      "Episode length: 5.20 +/- 2.40\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5.2           |\n",
      "|    mean_reward          | 1.1           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 999000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3194043e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.214        |\n",
      "|    explained_variance   | 0.452         |\n",
      "|    learning_rate        | 7e-07         |\n",
      "|    loss                 | 0.145         |\n",
      "|    n_updates            | 5200          |\n",
      "|    policy_gradient_loss | -2.86e-05     |\n",
      "|    value_loss           | 0.3           |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.37     |\n",
      "|    ep_rew_mean     | -1.15    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 325      |\n",
      "|    time_elapsed    | 23564    |\n",
      "|    total_timesteps | 999936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=1.16 +/- 0.08\n",
      "Episode length: 7.60 +/- 3.38\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 7.6           |\n",
      "|    mean_reward          | 1.16          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 1000000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0904235e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.4           |\n",
      "|    entropy_loss         | -0.232        |\n",
      "|    explained_variance   | 0.427         |\n",
      "|    learning_rate        | 2.4e-07       |\n",
      "|    loss                 | 12.5          |\n",
      "|    n_updates            | 5208          |\n",
      "|    policy_gradient_loss | -2.5e-06      |\n",
      "|    value_loss           | 15.3          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=1001000, episode_reward=1.10 +/- 0.07\n",
      "Episode length: 5.40 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 1.1      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1001000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.08     |\n",
      "|    ep_rew_mean     | -0.171   |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 23615    |\n",
      "|    total_timesteps | 1001472  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fa898cc1520>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.set_env(env2)\n",
    "# Train the model for a large number of timesteps\n",
    "agent.learn(total_timesteps=500000,\n",
    "            reset_num_timesteps = False,\n",
    "           callback = eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e01ea1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7549/1689855701.py:5: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(evaluation_log2_df['timesteps'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_log2 = np.load(eval_log_path + '/evaluations.npz')\n",
    "evaluation_log2_df = pd.DataFrame({item: [np.mean(ep) for ep in evaluation_log2[item]] for item in evaluation_log2.files})\n",
    "ax = evaluation_log2_df.loc[0:len(evaluation_log2_df), 'results'].plot(color = 'lightgray', xlim = [-5, len(evaluation_log2_df)], figsize = (10,5))\n",
    "evaluation_log2_df['results'].rolling(5).mean().plot(color = 'black', xlim = [-5, len(evaluation_log2_df)])\n",
    "ax.set_xticklabels(evaluation_log2_df['timesteps'])\n",
    "ax.set_xlabel(\"Eval Episode\")\n",
    "plt.ylabel(\"Rolling Mean Cumulative Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e043a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 0.921428833333333 +/- 0.5821790464617728\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the agent in the environment for 30 episodes\n",
    "agent.set_env(eval_env2)\n",
    "mean_reward, std_reward = sb3.common.evaluation.evaluate_policy(agent, \n",
    "                                                                agent.get_env(), \n",
    "                                                                n_eval_episodes=30)\n",
    "\n",
    "print(\"Mean Reward: {} +/- {}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c88749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"./ppo_connectx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9443035",
   "metadata": {},
   "source": [
    "## Evaluating my agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a00a8dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.predict(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36734f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testagent(obs, config):\n",
    "    import numpy as np\n",
    "    obs = np.array(obs['board']).reshape(1, config.rows, config.columns)\n",
    "    action, _ = agent.predict(obs)\n",
    "    return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "089e2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca93c89b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.18\n",
      "Agent 2 Win Percentage: 0.81\n",
      "Number of Invalid Plays by Agent 1: 1\n",
      "Number of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(agent1=testagent, agent2=\"negamax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2fff878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.82\n",
      "Agent 2 Win Percentage: 0.17\n",
      "Number of Invalid Plays by Agent 1: 1\n",
      "Number of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(agent1=testagent, agent2=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b58c49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: true,\n",
       "  &quot;playing&quot;: true,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: true,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;2a2358b6-2002-11ee-95da-7c214ac9f68a&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;columns&quot;: 7,\n",
       "      &quot;rows&quot;: 6,\n",
       "      &quot;inarow&quot;: 4,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 1,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 2,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 5,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 3,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 4,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 6,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 5,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 6,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 7,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 8,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 9,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 10,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 5,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 11,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 12,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 13,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 14,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 6,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 15,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 16,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 17,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 18,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 6,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 19,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: -1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 20,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 6,\n",
       "          &quot;reward&quot;: 1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      -1,\n",
       "      1\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;DONE&quot;,\n",
       "      &quot;DONE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000859,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.065616,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.00031,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.052102,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000223,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.007595,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000189,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.042733,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000195,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.043537,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000361,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.008116,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000195,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.027552,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000189,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000248,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.00017,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 7.1e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000165,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 2.4e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ]\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"300\" height=\"300\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = make(\"connectx\", debug=True)\n",
    "\n",
    "# Two random agents play one game round\n",
    "env.run([testagent, \"negamax\"])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c95ca22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: true,\n",
       "  &quot;playing&quot;: false,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: false,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;4c391f3a-2002-11ee-95da-7c214ac9f68a&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;columns&quot;: 7,\n",
       "      &quot;rows&quot;: 6,\n",
       "      &quot;inarow&quot;: 4,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      0,\n",
       "      0\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;ACTIVE&quot;,\n",
       "      &quot;INACTIVE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    []\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;,\n",
       "  &quot;interactive&quot;: true,\n",
       "  &quot;width&quot;: 500,\n",
       "  &quot;height&quot;: 450\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"500\" height=\"450\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.play([None, testagent ], width=500, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e2f60",
   "metadata": {},
   "source": [
    "## Writing submission file and Validating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d650b1",
   "metadata": {},
   "source": [
    "I found  [\"Connect4: make submission with stable-baselines3\"](https://www.kaggle.com/code/toshikazuwatanabe/connect4-make-submission-with-stable-baselines3) to be extremely helpful in learning how to write a submission file. I want to express my gratitude to [noypeban](https://www.kaggle.com/toshikazuwatanabe)  for sharing this notebook. Thank you very much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4a13f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission.py\n",
    "def agent(obs, config):\n",
    "    import numpy as np\n",
    "    import torch as th\n",
    "    from torch import nn as nn\n",
    "    from torch import tensor\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net,self).__init__()\n",
    "            n_inputs= 42\n",
    "            n_actions=7\n",
    "            self.policy1 = nn.Linear(in_features=42, out_features=64, bias=True)\n",
    "            self.policy2 = nn.Linear(in_features=64, out_features=64, bias=True)\n",
    "            self.action = nn.Linear(in_features=64, out_features=7, bias=True)\n",
    "            self.out_activ = nn.Softmax(dim=1)\n",
    "        def forward(self,x):\n",
    "            x = nn.Flatten(start_dim = 1, end_dim = -1)(x)\n",
    "            x = nn.Flatten(start_dim = 1, end_dim = -1)(x)\n",
    "            x = th.tanh(self.policy1(x))\n",
    "            x = th.tanh(self.policy2(x))\n",
    "            x = self.action(x)\n",
    "            x = self.out_activ(x)\n",
    "            x = x.argmax()\n",
    "            return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "529f557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = sb3.PPO.load(\"./ppo_connectx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69494680",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['mlp_extractor.policy_net.0.weight', 'mlp_extractor.policy_net.0.bias', 'mlp_extractor.policy_net.2.weight', 'mlp_extractor.policy_net.2.bias', 'mlp_extractor.value_net.0.weight', 'mlp_extractor.value_net.0.bias', 'mlp_extractor.value_net.2.weight', 'mlp_extractor.value_net.2.bias', 'action_net.weight', 'action_net.bias', 'value_net.weight', 'value_net.bias'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41fef946",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_printoptions(profile=\"full\")\n",
    "\n",
    "agent_path = 'submission.py'\n",
    "\n",
    "state_dict = agent.policy.to('cpu').state_dict()\n",
    "state_dict ={\n",
    "    'policy1.weight':state_dict['mlp_extractor.policy_net.0.weight'],\n",
    "    'policy1.bias':state_dict['mlp_extractor.policy_net.0.bias'],\n",
    "    'policy2.weight':state_dict['mlp_extractor.policy_net.2.weight'],\n",
    "    'policy2.bias':state_dict['mlp_extractor.policy_net.2.bias'],\n",
    "    'action.weight':state_dict['action_net.weight'],\n",
    "    'action.bias':state_dict['action_net.bias'],\n",
    "}\n",
    "\n",
    "with open(agent_path, mode='a') as file:\n",
    "    file.write(f'    state_dict = {state_dict}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a86e3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to submission.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a submission.py\n",
    "    model = Net()\n",
    "    model = model.float()\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to('cpu')\n",
    "    model = model.eval()\n",
    "    obs = tensor(obs['board']).reshape(1, 1, config.rows, config.columns).float()\n",
    "    action = model(obs)\n",
    "    return int(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb4f43",
   "metadata": {},
   "source": [
    "### Validate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0954382b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent=None\n",
    "f = open(agent_path)\n",
    "source = f.read()\n",
    "exec(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f98d6cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(env.reset()[0]['observation'], env.configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d57ac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.12\n",
      "Agent 2 Win Percentage: 0.87\n",
      "Number of Invalid Plays by Agent 1: 1\n",
      "Number of Invalid Plays by Agent 2: 0\n"
     ]
    }
   ],
   "source": [
    "get_win_percentages(agent1=agent, agent2=\"negamax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0f54eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: true,\n",
       "  &quot;playing&quot;: true,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: true,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;994ce9d2-2002-11ee-95da-7c214ac9f68a&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;columns&quot;: 7,\n",
       "      &quot;rows&quot;: 6,\n",
       "      &quot;inarow&quot;: 4,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 1,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 2,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 5,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 3,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 4,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 5,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 6,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 7,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 8,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 9,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 10,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 11,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 12,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 13,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 14,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 15,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: -1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      1,\n",
       "      -1\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;DONE&quot;,\n",
       "      &quot;DONE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001355,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001429,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.002047,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001107,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001072,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001063,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001205,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001068,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001072,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001062,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001175,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001053,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001056,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001068,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.001178,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ]\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"300\" height=\"300\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = make(\"connectx\", debug=True)\n",
    "\n",
    "# Two random agents play one game round\n",
    "env.run([agent, agent])\n",
    "\n",
    "# Show the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df7fa75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "env = make(\"connectx\", debug=True)\n",
    "env.run([agent, agent])\n",
    "print(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ae5de",
   "metadata": {},
   "source": [
    "## Result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4a0255",
   "metadata": {},
   "source": [
    "<div style=\" background-color: rgb(218, 255, 215);\n",
    "             border-style: outset;\n",
    "             padding: 10px;\n",
    "             margin: 10px;\n",
    "             text-indent: 10px;\n",
    "             text-align: justify;\n",
    "             letter-spacing: 1px;\n",
    "             line-height: 200%\">\n",
    "    <h4> &#x1F913; This is my personal reflections and I welcome any corrections or suggestions for improvement. &#x1F4BB; </h4>\n",
    "    <ul>\n",
    "        <li style = \"margin: 10px\"> I implemented linear learning rate decay, as shown in the <a href=\"https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#learning-rate-schedule\">documentation</a>  example. However, it may be beneficial to introduce adeptive decay to aid in model convergence. </li> \n",
    "        <li style = \"margin: 10px\"> To ensure that my agent did not make invalid moves, I heavily penalized such actions. However, this hinder exploration. Therefore, it may be worth exploring PPO with invalid action masking.  </li> \n",
    "        <li style = \"margin: 10px\"> The performance of my agent is resonably bad but I think my model is still undertrained and needs more training. However, I need to to pause the experiment until I figure out how to parallelize the training process in Kaggle environments. The current training process takes up too much time and is overwhelming.  </li> \n",
    "</ul>\n",
    "\n",
    "<h4> I would appreciate your guidance on the tasks I attempted but struggled with or found perplexing.:</h4>\n",
    "    <ul>\n",
    "        <li style = \"margin: 10px\"> I'm facing a challenge with converting the Kaggle environment into a vectorized gymnasium environment to speed up my training process by trainning in parallel environments. </li> \n",
    "        <li style = \"margin: 10px\"> During my initial testing, I found that my model performed much worse when orthogonal weight initialization was set to true. Therefore, I opted to set it to false for the actual training stage. However, I have come across papers and forum posts suggesting that using orthogonal weight initialization with PPO can aid in model convergence so I am confused. </li> \n",
    "               <li style = \"margin: 10px\"> The default CNN policy of SB3 had a minimum image size requirement that was not met by the input values. To address this, I opted to use a vector agent instead of creating a custom CNN policy. I stumbled upon some notebooks that utilized a customized CNN policy. I'm curious about the reasoning behind choosing a CNN policy with PPO for this particular environment. From what I observed, the notebooks didn't demonstrate superior performance, likely due to insufficient training. Would anyone who has experimented with PPO with CNN policy for a significant amount of time be willing to share performance results? </li> \n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advancedml",
   "language": "python",
   "name": "advancedml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
