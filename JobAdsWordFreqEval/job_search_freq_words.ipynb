{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e218815",
   "metadata": {},
   "source": [
    "### Simple frequencies at data_analytics job adds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603148e",
   "metadata": {},
   "source": [
    "    Small-scale text analytics study to assess word frequencies across data_analytics job add at linkedin to gain insight about structure of current demand. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6406714",
   "metadata": {},
   "source": [
    "#### Reading html, stripping content, creating corpus(dictionary of title(key),content(value) pair.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c66817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import subprocess\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a3c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jobadds012023.txt', 'r') as f:\n",
    "    urls = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9329cf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of adds in current corpus\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ce7569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mycorpus={}\n",
    "for page in urls:\n",
    "    rawhtml=urllib.request.urlopen(page).read()\n",
    "    soup=BeautifulSoup(rawhtml)\n",
    "    key=soup.title.string\n",
    "    value=soup.body.get_text(strip=True)\n",
    "    mykey=\"\".join(key.split(\", \")[0].split())\n",
    "    myvalue = re.search('companyReportReportBackSubmit(.*)Show moreShow', value).group(1)\n",
    "    mycorpus[mykey]=myvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51267f",
   "metadata": {},
   "source": [
    "#### Preprocessing methods (creating relevant tokens from corpus)\n",
    "    common steps in preprocessing pipelines:\n",
    "    1. stop_word removal\n",
    "    2. tokanisation\n",
    "    3. pos-tagging\n",
    "    4. lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07230ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkeskin/anaconda3/envs/COMP47750/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a1adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag map for lemmatizer\n",
    "tag_map = {\n",
    "    'CC':None, # coordin. conjunction (and, but, or)  \n",
    "    'CD':wn.NOUN, # cardinal number (one, two)             \n",
    "    'DT':None, # determiner (a, the)                    \n",
    "    'EX':wn.ADV, # existential ‘there’ (there)           \n",
    "    'FW':None, # foreign word (mea culpa)             \n",
    "    'IN':wn.ADV, # preposition/sub-conj (of, in, by)   \n",
    "    'JJ':wn.ADJ, # adjective (yellow)                  \n",
    "    'JJR':wn.ADJ, # adj., comparative (bigger)          \n",
    "    'JJS':wn.ADJ, # adj., superlative (wildest)           \n",
    "    'LS':None, # list item marker (1, 2, One)          \n",
    "    'MD':None, # modal (can, should)                    \n",
    "    'NN':wn.NOUN, # noun, sing. or mass (llama)          \n",
    "    'NNS':wn.NOUN, # noun, plural (llamas)                  \n",
    "    'NNP':wn.NOUN, # proper noun, sing. (IBM)              \n",
    "    'NNPS':wn.NOUN, # proper noun, plural (Carolinas)\n",
    "    'PDT':wn.ADJ, # predeterminer (all, both)            \n",
    "    'POS':None, # possessive ending (’s )               \n",
    "    'PRP':None, # personal pronoun (I, you, he)     \n",
    "    'PRP$':None, # possessive pronoun (your, one’s)    \n",
    "    'RB':wn.ADV, # adverb (quickly, never)            \n",
    "    'RBR':wn.ADV, # adverb, comparative (faster)        \n",
    "    'RBS':wn.ADV, # adverb, superlative (fastest)     \n",
    "    'RP':wn.ADJ, # particle (up, off)\n",
    "    'SYM':None, # symbol (+,%, &)\n",
    "    'TO':None, # “to” (to)\n",
    "    'UH':None, # interjection (ah, oops)\n",
    "    'VB':wn.VERB, # verb base form (eat)\n",
    "    'VBD':wn.VERB, # verb past tense (ate)\n",
    "    'VBG':wn.VERB, # verb gerund (eating)\n",
    "    'VBN':wn.VERB, # verb past participle (eaten)\n",
    "    'VBP':wn.VERB, # verb non-3sg pres (eat)\n",
    "    'VBZ':wn.VERB, # verb 3sg pres (eats)\n",
    "    'WDT':None, # wh-determiner (which, that)\n",
    "    'WP':None, # wh-pronoun (what, who)\n",
    "    'WP$':None, # possessive (wh- whose)\n",
    "    'WRB':None, # wh-adverb (how, where)\n",
    "    '$':None, #  dollar sign ($)\n",
    "    '#':None, # pound sign (#)\n",
    "    '“':None, # left quote (‘ or “)\n",
    "    '”':None, # right quote (’ or ”)\n",
    "    '(':None, # left parenthesis ([, (, {, <)\n",
    "    ')':None, # right parenthesis (], ), }, >)\n",
    "    ',':None, # comma (,)\n",
    "    '.':None, # sentence-final punc (. ! ?)\n",
    "    ':':None # mid-sentence punc (: ; ... – -)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343a9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_all(text,tag_map):\n",
    "    \"\"\"function to create tokens list from all preprocessed words\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = nltk.word_tokenize(text.lower())\n",
    "    text_tokens_wo_stop_words=[w for w in text_tokens if not w in stop_words and w.isalnum()]    \n",
    "    text_with_pos=nltk.pos_tag(text_tokens_wo_stop_words)\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    def convert_tags(tag):\n",
    "        return tag_map.get(tag)\n",
    "    lemmatized_tag_list_wo_stop_word=[]\n",
    "    for item in text_with_pos:\n",
    "        new_tag=convert_tags(item[1])\n",
    "        if new_tag== None:\n",
    "            out= item[0]\n",
    "        else:\n",
    "            out=wnl.lemmatize(item[0],new_tag)\n",
    "        lemmatized_tag_list_wo_stop_word.append(out)\n",
    "    return lemmatized_tag_list_wo_stop_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf861d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_adj(text,tag_map):\n",
    "    \"\"\"function to create tokens list from preprocessed adjectives\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = nltk.word_tokenize(text.lower())\n",
    "    text_tokens_wo_stop_words=[w for w in text_tokens if not w in stop_words and w.isalnum()]    \n",
    "    text_with_pos=nltk.pos_tag(text_tokens_wo_stop_words)\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    def convert_tags(tag):\n",
    "        return tag_map.get(tag)\n",
    "    lemmatized_tag_list_wo_stop_word=[]\n",
    "    for item in text_with_pos:\n",
    "        new_tag=convert_tags(item[1])\n",
    "        if new_tag==wn.ADJ:\n",
    "            out=wnl.lemmatize(item[0],new_tag)\n",
    "            lemmatized_tag_list_wo_stop_word.append(out)\n",
    "    return lemmatized_tag_list_wo_stop_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da7ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_adj_postadj(text,tag_map):\n",
    "    \"\"\"function to create tokens list from preprocessed adjectives and words came after an adjective\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = nltk.word_tokenize(text.lower())\n",
    "    text_tokens_wo_stop_words=[w for w in text_tokens if not w in stop_words and w.isalnum()]    \n",
    "    text_with_pos=nltk.pos_tag(text_tokens_wo_stop_words)\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    def convert_tags(tag):\n",
    "        return tag_map.get(tag)\n",
    "    lemmatized_tag_list_wo_stop_word=[]\n",
    "    past_tag=None\n",
    "    for item in text_with_pos:\n",
    "        new_tag=convert_tags(item[1])\n",
    "        if new_tag==wn.ADJ:\n",
    "            out=wnl.lemmatize(item[0],new_tag)\n",
    "            lemmatized_tag_list_wo_stop_word.append(out)\n",
    "        elif past_tag==wn.ADJ:\n",
    "            if new_tag== None:\n",
    "                out= item[0]\n",
    "            else:\n",
    "                out=wnl.lemmatize(item[0],new_tag)\n",
    "            lemmatized_tag_list_wo_stop_word.append(out)\n",
    "        past_tag=new_tag\n",
    "    return lemmatized_tag_list_wo_stop_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c6305",
   "metadata": {},
   "source": [
    "#### Extracting insight in the form of simple word and bi-gram frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9761539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 197), ('work', 90), ('team', 56), ('experience', 51), ('business', 50), ('learn', 44), ('company', 39), ('customer', 39), ('opportunity', 36), ('new', 33)]\n"
     ]
    }
   ],
   "source": [
    "#top 10 word frequencies\n",
    "my_tokens={}\n",
    "for key, value in mycorpus.items():\n",
    "    text_tokens=preprocessing_all(value,tag_map)\n",
    "    my_tokens[key]=text_tokens\n",
    "all_tokens= list(itertools.chain.from_iterable(my_tokens.values()))\n",
    "cummulative_freqdist_all = nltk.FreqDist(all_tokens)\n",
    "print(cummulative_freqdist_all.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c93d5b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 33), ('technical', 18), ('big', 16), ('global', 12), ('analytical', 11), ('large', 10), ('strong', 10), ('sexual', 10), ('equal', 9), ('national', 9)]\n"
     ]
    }
   ],
   "source": [
    "#top 10 adjective frequencies\n",
    "my_adjtokens={}\n",
    "for key, value in mycorpus.items():\n",
    "    text_tokens=preprocessing_adj(value,tag_map)\n",
    "    my_adjtokens[key]=text_tokens\n",
    "all_adj_tokens= list(itertools.chain.from_iterable(my_adjtokens.values()))\n",
    "cummulative_freqdist_adj = nltk.FreqDist(all_adj_tokens)\n",
    "print(cummulative_freqdist_adj.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd4592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "513616d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('machine', 'learn'), 27),\n",
       " (('data', 'science'), 18),\n",
       " (('big', 'data'), 11),\n",
       " (('data', 'engineer'), 10),\n",
       " (('computer', 'science'), 9),\n",
       " (('data', 'analytics'), 9),\n",
       " (('data', 'scientist'), 9),\n",
       " (('sexual', 'orientation'), 9),\n",
       " (('national', 'origin'), 8),\n",
       " (('office', '365'), 8)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding frequencies of all bi-grams while ignoring bigrams co-occur less then 2 times in corpus\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "bigrams_all = BigramCollocationFinder.from_words(all_tokens)\n",
    "bigrams_all.apply_freq_filter(2)\n",
    "sorted(bigrams_all.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c283945f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('big', 'data'), 11),\n",
       " (('sexual', 'orientation'), 9),\n",
       " (('national', 'origin'), 8),\n",
       " (('equal', 'opportunity'), 7),\n",
       " (('marital', 'status'), 5),\n",
       " (('mental', 'disability'), 4),\n",
       " (('physical', 'mental'), 4),\n",
       " (('verbal', 'write'), 4),\n",
       " (('ethnic', 'national'), 3),\n",
       " (('military', 'veteran'), 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding frequencies of adjective paired bi-grams while ignoring bigrams co-occur less then 2 times in corpus\n",
    "my_adj_postadj_tokens={}\n",
    "for key, value in mycorpus.items():\n",
    "    text_tokens=preprocessing_adj_postadj(value,tag_map)\n",
    "    my_adj_postadj_tokens[key]=text_tokens\n",
    "all_adj_postadj_tokens= list(itertools.chain.from_iterable(my_adj_postadj_tokens.values()))\n",
    "bigrams_adj_postadj = BigramCollocationFinder.from_words(all_adj_postadj_tokens)\n",
    "bigrams_adj_postadj.apply_freq_filter(2)\n",
    "sorted(bigrams_adj_postadj.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:COMP47750]",
   "language": "python",
   "name": "conda-env-COMP47750-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
