{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqYqaWjA1yLW",
    "outputId": "c67dec5d-cf93-47b1-d223-daf3aea0a218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvvDy68-U_kL"
   },
   "source": [
    "### Install Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bChIaQp4RgfB",
    "outputId": "10dca787-95b8-4a72-c429-a9a33381186b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Starting OpenBSD Secure Shell server sshd\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "# Installing openssh-server\n",
    "!apt-get install openssh-server -qq > /dev/null\n",
    "\n",
    "# Starting our server\n",
    "!service ssh start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KAtUQJjR8f-",
    "outputId": "762590ce-c824-4722-dc59-666a802135b9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating public/private rsa key pair.\n",
      "Created directory '/root/.ssh'.\n",
      "Your identification has been saved in /root/.ssh/id_rsa.\n",
      "Your public key has been saved in /root/.ssh/id_rsa.pub.\n",
      "The key fingerprint is:\n",
      "SHA256:VQFKd8+XPtruK434I1ESBWeAdgtyI/SDtODOLwducu0 root@6155e2551bb9\n",
      "The key's randomart image is:\n",
      "+---[RSA 2048]----+\n",
      "|     ..o. ++B=   |\n",
      "|    . o++O =oo  .|\n",
      "|     . o*o= o o..|\n",
      "|    o    ..o ... |\n",
      "|     +  S   o  o |\n",
      "|    . +    .  o .|\n",
      "|   . = +    o.o. |\n",
      "|    + +    o +.. |\n",
      "|       E    o.++.|\n",
      "+----[SHA256]-----+\n"
     ]
    }
   ],
   "source": [
    "# Creating a new rsa key pair with empty password\n",
    "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zYITXjZ0SG5J"
   },
   "outputs": [],
   "source": [
    "# Copying the public key we just generated to autorized keys\n",
    "!cat $HOME/.ssh/id_rsa.pub>>$HOME/.ssh/authorized_keys\n",
    "\n",
    "# Changing the permissions on the key\n",
    "!chmod 0600 ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifYrMgH8SK-R",
    "outputId": "c3cc4dd1-78de-402a-be5b-cde0dae913c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\r\n",
      " 20:42:41 up 2 min,  0 users,  load average: 1.03, 0.49, 0.19\n"
     ]
    }
   ],
   "source": [
    "# Conneting with our local machine\n",
    "!ssh -o StrictHostKeyChecking=no localhost uptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "a_PPILDLUm7Q"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PF40OFaEU6C5",
    "outputId": "0dbd339a-9452-4131-e7d3-964b33a7bb10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-03 20:42:41--  https://dlcdn.apache.org/hadoop/common/hadoop-3.3.3/hadoop-3.3.3.tar.gz\n",
      "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 645040598 (615M) [application/x-gzip]\n",
      "Saving to: ‘hadoop-3.3.3.tar.gz’\n",
      "\n",
      "hadoop-3.3.3.tar.gz 100%[===================>] 615.16M   226MB/s    in 2.7s    \n",
      "\n",
      "2022-12-03 20:42:44 (226 MB/s) - ‘hadoop-3.3.3.tar.gz’ saved [645040598/645040598]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Installing Hadoop and configuring JAVA_HOME: \n",
    "!if [ ! -d /usr/local/hadoop-3.3.3/ ]; then \\\n",
    "wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.3/hadoop-3.3.3.tar.gz; \\\n",
    "tar -xzf hadoop-3.3.3.tar.gz; \\\n",
    "cp -r hadoop-3.3.3/ /usr/local/; \\\n",
    "rm -rf hadoop-3.3.3/; \\\n",
    "rm hadoop-3.3.3.tar.gz; \\\n",
    "echo \"export JAVA_HOME=$(dirname $(dirname $(realpath $(which java))))\" >> /usr/local/hadoop-3.3.3/etc/hadoop/hadoop-env.sh; \\\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Scb-Xsb6VKCR"
   },
   "outputs": [],
   "source": [
    "# Setting up some of our environmental variables: \n",
    "os.environ['PATH'] = \"/usr/local/hadoop-3.3.3/bin/:\" + os.environ['PATH']\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gtviGruZVCh4"
   },
   "outputs": [],
   "source": [
    "# Running our config script\n",
    "# Note: remember to check you have the correct filepath\n",
    "!bash lab4_config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "RP7Oxg88QUqd",
    "outputId": "1c87929b-0268-4d3d-cf0c-14267eaae335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: /usr/local/hadoop-3.3.3/logs does not exist. Creating.\n",
      "2022-12-03 20:43:02,893 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = 6155e2551bb9/172.28.0.12\n",
      "STARTUP_MSG:   args = [-format]\n",
      "STARTUP_MSG:   version = 3.3.3\n",
      "STARTUP_MSG:   classpath = /usr/local/hadoop-3.3.3/etc/hadoop:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jetty-security-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/reload4j-1.2.18.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jetty-server-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jetty-servlet-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jetty-http-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jackson-databind-2.13.2.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/curator-client-4.2.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/curator-recipes-4.2.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jetty-util-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/gson-2.8.9.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jetty-webapp-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/hadoop-annotations-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/hadoop-auth-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jetty-xml-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/zookeeper-jute-3.5.6.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/zookeeper-3.5.6.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jackson-annotations-2.13.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jetty-util-ajax-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/curator-framework-4.2.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jackson-core-2.13.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jetty-io-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/hadoop-kms-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/hadoop-registry-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/hadoop-nfs-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/hadoop-common-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/common/hadoop-common-3.3.3-tests.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jetty-security-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/reload4j-1.2.18.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jetty-server-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jetty-http-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jackson-databind-2.13.2.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/curator-client-4.2.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/curator-recipes-4.2.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jetty-util-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/gson-2.8.9.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/hadoop-annotations-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/hadoop-auth-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jetty-xml-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/zookeeper-jute-3.5.6.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/zookeeper-3.5.6.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jackson-annotations-2.13.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/curator-framework-4.2.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jackson-core-2.13.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jetty-io-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/hadoop-hdfs-client-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/hadoop-hdfs-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/hadoop-hdfs-3.3.3-tests.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/hadoop-hdfs-client-3.3.3-tests.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.3-tests.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.3-tests.jar:/usr/local/hadoop-3.3.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.3-tests.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/asm-tree-9.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jetty-plus-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/asm-commons-9.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.13.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/websocket-common-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/websocket-server-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jetty-annotations-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.13.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/asm-analysis-9.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/websocket-servlet-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/websocket-api-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jetty-jndi-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/javax.ws.rs-api-2.1.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/websocket-client-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.13.2.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/lib/jetty-client-9.4.43.v20210629.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-services-core-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-client-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-api-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-registry-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-server-common-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-server-router-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-common-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.3.jar:/usr/local/hadoop-3.3.3/share/hadoop/yarn/hadoop-yarn-services-api-3.3.3.jar\n",
      "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r d37586cbda38c338d9fe481addda5a05fb516f71; compiled by 'stevel' on 2022-05-09T16:36Z\n",
      "STARTUP_MSG:   java = 11.0.17\n",
      "************************************************************/\n",
      "2022-12-03 20:43:02,962 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "2022-12-03 20:43:03,114 INFO namenode.NameNode: createNameNode [-format]\n",
      "2022-12-03 20:43:04,031 INFO namenode.NameNode: Formatting using clusterid: CID-a57cb715-de76-4cf9-b9d1-fbda419f11ac\n",
      "2022-12-03 20:43:04,094 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "2022-12-03 20:43:04,187 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "2022-12-03 20:43:04,191 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "2022-12-03 20:43:04,191 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "2022-12-03 20:43:04,248 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
      "2022-12-03 20:43:04,248 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
      "2022-12-03 20:43:04,249 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
      "2022-12-03 20:43:04,249 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
      "2022-12-03 20:43:04,249 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "2022-12-03 20:43:04,314 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "2022-12-03 20:43:04,329 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "2022-12-03 20:43:04,329 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "2022-12-03 20:43:04,334 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "2022-12-03 20:43:04,334 INFO blockmanagement.BlockManager: The block deletion will start around 2022 Dec 03 20:43:04\n",
      "2022-12-03 20:43:04,336 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "2022-12-03 20:43:04,336 INFO util.GSet: VM type       = 64-bit\n",
      "2022-12-03 20:43:04,338 INFO util.GSet: 2.0% max memory 3.2 GB = 65.0 MB\n",
      "2022-12-03 20:43:04,338 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
      "2022-12-03 20:43:04,360 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
      "2022-12-03 20:43:04,360 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
      "2022-12-03 20:43:04,367 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
      "2022-12-03 20:43:04,367 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "2022-12-03 20:43:04,367 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "2022-12-03 20:43:04,368 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
      "2022-12-03 20:43:04,368 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "2022-12-03 20:43:04,368 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "2022-12-03 20:43:04,368 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "2022-12-03 20:43:04,368 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
      "2022-12-03 20:43:04,368 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "2022-12-03 20:43:04,368 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "2022-12-03 20:43:04,397 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
      "2022-12-03 20:43:04,397 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
      "2022-12-03 20:43:04,397 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
      "2022-12-03 20:43:04,398 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
      "2022-12-03 20:43:04,408 INFO util.GSet: Computing capacity for map INodeMap\n",
      "2022-12-03 20:43:04,408 INFO util.GSet: VM type       = 64-bit\n",
      "2022-12-03 20:43:04,408 INFO util.GSet: 1.0% max memory 3.2 GB = 32.5 MB\n",
      "2022-12-03 20:43:04,408 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
      "2022-12-03 20:43:04,414 INFO namenode.FSDirectory: ACLs enabled? true\n",
      "2022-12-03 20:43:04,414 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
      "2022-12-03 20:43:04,414 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "2022-12-03 20:43:04,414 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "2022-12-03 20:43:04,421 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
      "2022-12-03 20:43:04,423 INFO snapshot.SnapshotManager: SkipList is disabled\n",
      "2022-12-03 20:43:04,428 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "2022-12-03 20:43:04,428 INFO util.GSet: VM type       = 64-bit\n",
      "2022-12-03 20:43:04,428 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n",
      "2022-12-03 20:43:04,428 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "2022-12-03 20:43:04,437 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "2022-12-03 20:43:04,437 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "2022-12-03 20:43:04,437 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "2022-12-03 20:43:04,442 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "2022-12-03 20:43:04,442 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "2022-12-03 20:43:04,444 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "2022-12-03 20:43:04,444 INFO util.GSet: VM type       = 64-bit\n",
      "2022-12-03 20:43:04,445 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 997.8 KB\n",
      "2022-12-03 20:43:04,445 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
      "2022-12-03 20:43:04,482 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1084860161-172.28.0.12-1670100184467\n",
      "2022-12-03 20:43:04,542 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
      "2022-12-03 20:43:04,608 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "2022-12-03 20:43:04,735 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
      "2022-12-03 20:43:04,766 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "2022-12-03 20:43:04,802 INFO namenode.FSNamesystem: Stopping services started for active state\n",
      "2022-12-03 20:43:04,803 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
      "2022-12-03 20:43:04,811 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
      "2022-12-03 20:43:04,812 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at 6155e2551bb9/172.28.0.12\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hdfs namenode -format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0XqLvC7BRNMt"
   },
   "outputs": [],
   "source": [
    "# Creating our HDFS environmental variables: \n",
    "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
    "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
    "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
    "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
    "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Ydm_FwbRQHd",
    "outputId": "9ab391d1-5db8-4c05-8f3b-db84f580af7b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [6155e2551bb9]\n",
      "6155e2551bb9: Warning: Permanently added '6155e2551bb9,172.28.0.12' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "# Launching hdfs daemons\n",
    "!$HADOOP_HOME/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XqjhHSBRSF1",
    "outputId": "7fcb6a85-ca49-4999-a9fe-ae3b80771d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504 SecondaryNameNode\n",
      "1682 Jps\n",
      "1286 DataNode\n",
      "1150 NameNode\n"
     ]
    }
   ],
   "source": [
    "# Listing the running daemons\n",
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvsrIN1ASZZM",
    "outputId": "7c77abbd-bdf2-419c-f4e6-809259eadc23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "# Launching our yarn daemons\n",
    "# nohup causes a process to ignore a \"hang-up\" signal\n",
    "!nohup $HADOOP_HOME/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1wYsh04SeKN",
    "outputId": "2eea8a8d-acc6-451f-cb8a-6fcb840fa242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504 SecondaryNameNode\n",
      "1953 NodeManager\n",
      "1826 ResourceManager\n",
      "1286 DataNode\n",
      "2092 Jps\n",
      "1150 NameNode\n"
     ]
    }
   ],
   "source": [
    "#Listing the running daemons\n",
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "M97p1FpHShvP",
    "outputId": "4046be7e-702a-4552-d4bc-87c209acb1de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 115658190848 (107.72 GB)\n",
      "Present Capacity: 89913307136 (83.74 GB)\n",
      "DFS Remaining: 89913282560 (83.74 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "DFS Used%: 0.00%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 127.0.0.1:9866 (localhost)\n",
      "Hostname: 6155e2551bb9\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 115658190848 (107.72 GB)\n",
      "DFS Used: 24576 (24 KB)\n",
      "Non DFS Used: 25728106496 (23.96 GB)\n",
      "DFS Remaining: 89913282560 (83.74 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 77.74%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Sat Dec 03 20:43:38 UTC 2022\n",
      "Last Block Report: Sat Dec 03 20:43:17 UTC 2022\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report the basic file system information and statistics to make sure everything is set up as it should be: \n",
    "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psDmw_kgUTps"
   },
   "source": [
    "### You may complete the following tasks using Java or Python scripts. The output for each task in this section should be just one line. Upload the following three books to your HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "hfzkwp1xiVR8",
    "outputId": "94d6b298-8764-485d-8235-52984f371a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-03 20:43:39--  http://www.gutenberg.org/cache/epub/1524/pg1524.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.gutenberg.org/cache/epub/1524/pg1524.txt [following]\n",
      "--2022-12-03 20:43:39--  https://www.gutenberg.org/cache/epub/1524/pg1524.txt\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 204364 (200K) [text/plain]\n",
      "Saving to: ‘pg1524.txt’\n",
      "\n",
      "\r",
      "pg1524.txt            0%[                    ]       0  --.-KB/s               \r",
      "pg1524.txt          100%[===================>] 199.57K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-12-03 20:43:39 (3.26 MB/s) - ‘pg1524.txt’ saved [204364/204364]\n",
      "\n",
      "--2022-12-03 20:43:39--  http://www.gutenberg.org/cache/epub/1112/pg1112.txt\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.gutenberg.org/cache/epub/1112/pg1112.txt [following]\n",
      "--2022-12-03 20:43:39--  https://www.gutenberg.org/cache/epub/1112/pg1112.txt\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 179382 (175K) [text/plain]\n",
      "Saving to: ‘pg1112.txt’\n",
      "\n",
      "pg1112.txt          100%[===================>] 175.18K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-12-03 20:43:39 (2.71 MB/s) - ‘pg1112.txt’ saved [179382/179382]\n",
      "\n",
      "--2022-12-03 20:43:39--  http://www.gutenberg.org/cache/epub/2267/pg2267.txt\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.gutenberg.org/cache/epub/2267/pg2267.txt [following]\n",
      "--2022-12-03 20:43:39--  https://www.gutenberg.org/cache/epub/2267/pg2267.txt\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 172298 (168K) [text/plain]\n",
      "Saving to: ‘pg2267.txt’\n",
      "\n",
      "pg2267.txt          100%[===================>] 168.26K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-12-03 20:43:40 (2.64 MB/s) - ‘pg2267.txt’ saved [172298/172298]\n",
      "\n",
      "FINISHED --2022-12-03 20:43:40--\n",
      "Total wall clock time: 0.5s\n",
      "Downloaded: 3 files, 543K in 0.2s (2.87 MB/s)\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "\n",
    "# Solution:\n",
    "!wget http://www.gutenberg.org/cache/epub/1524/pg1524.txt http://www.gutenberg.org/cache/epub/1112/pg1112.txt http://www.gutenberg.org/cache/epub/2267/pg2267.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "b3a_8_FGjAn7"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "# Solution: \n",
    "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count\n",
    "!$HADOOP_HOME/bin/hdfs dfs -put *.txt /word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBzk0_5rpzcu",
    "outputId": "99acef27-74ab-41cb-a0f1-94513256fd13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 root supergroup     179382 2022-12-03 20:43 /word_count/pg1112.txt\n",
      "-rw-r--r--   1 root supergroup     204364 2022-12-03 20:43 /word_count/pg1524.txt\n",
      "-rw-r--r--   1 root supergroup     172298 2022-12-03 20:43 /word_count/pg2267.txt\n"
     ]
    }
   ],
   "source": [
    "# We can see our files now on the HDFS:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3HLcnFUVaKv"
   },
   "source": [
    "### 1. How many words in the corpus begin with the letter Y/y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RuK_tT4XtcWJ",
    "outputId": "1e851868-1d6a-451b-e7c6-ed2e515b6dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper1.py\n"
     ]
    }
   ],
   "source": [
    "# Writing our mapper.py script:\n",
    "# The executable will read our input and transform them into key-value \n",
    "# pairs which are then passed to our reducer script. \n",
    "# Reference: https://github.com/LMAPcoder/Hadoop-on-Colab/blob/main/Hadoop_on_Colab.ipynb\n",
    "\n",
    "%%writefile mapper1.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "  \n",
    "for line in sys.stdin:\n",
    "  line = line.strip()  # removes whitespace either side of our line\n",
    "  words = line.split()  # splitting our line into a list of words\n",
    "    \n",
    "  for word in words:\n",
    "    big_y = re.findall(\"^Y\", word)\n",
    "    small_y=re.findall(\"^y\", word)\n",
    "    if len(big_y)==1:\n",
    "      print('%s\\t%s' % (word, 1))  # writing our results to STDOUT (this is the input for reducer.py)\n",
    "    elif len(small_y)==1:\n",
    "      print('%s\\t%s' % (word, 1))  # writing our results to STDOUT (this is the input for reducer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a66f4O6svDx7",
    "outputId": "54ce973f-9ed2-4dc0-e354-ac67dc4b2e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer1.py\n"
     ]
    }
   ],
   "source": [
    "# Writing our reducer.py script: \n",
    "# This executable reads all intermediate key-value pairs generated by our mapper.py\n",
    "# And aggregates these into our final output result\n",
    "# Reference: https://github.com/LMAPcoder/Hadoop-on-Colab/blob/main/Hadoop_on_Colab.ipynb\n",
    "\n",
    "%%writefile reducer1.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "  \n",
    "import sys\n",
    "from operator import itemgetter\n",
    "  \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "y_Y_count=0  \n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  word, count = line.split('\\t', 1)  # splitting the data on the basis of tab (see mapper.py)\n",
    "  \n",
    "  try:\n",
    "    count = int(count)  # convert count (currently a string) to int\n",
    "  except ValueError:\n",
    "    continue  # silently ignore line if count is not a number\n",
    "  \n",
    "  # this IF-switch only works because Hadoop sorts map output\n",
    "  # by key (here: word) before it is passed to the reducer\n",
    "  if current_word == word:\n",
    "    current_count += count\n",
    "  else:\n",
    "    current_count = count\n",
    "    current_word = word\n",
    "  y_Y_count+=current_count\n",
    "\n",
    "\n",
    "print(f\"{y_Y_count} words in the corpus begin with the letter Y/y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZG4i6nuLzo4e"
   },
   "outputs": [],
   "source": [
    "# Giving these new files permissions:\n",
    "!chmod u+x /content/mapper1.py /content/reducer1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxPUksiHmrQV",
    "outputId": "5f4dfd34-e68a-4cca-9784-e457de274411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105521 words in the corpus begin with the letter Y/y\n"
     ]
    }
   ],
   "source": [
    "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
    "!cat pg2267.txt | python mapper1.py | sort -k1,1 | python reducer1.py | head -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Fs9cX0O8zzGW",
    "outputId": "a3815ac3-9ebb-412f-d9c0-2cccddfdc62c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar9086846026193386767/] [] /tmp/streamjob8219554400497305437.jar tmpDir=null\n",
      "2022-12-03 20:17:30,426 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2022-12-03 20:17:30,727 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2022-12-03 20:17:31,156 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1670098633286_0001\n",
      "2022-12-03 20:17:31,541 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2022-12-03 20:17:31,683 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2022-12-03 20:17:32,162 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1670098633286_0001\n",
      "2022-12-03 20:17:32,163 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-12-03 20:17:32,469 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-12-03 20:17:32,470 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-12-03 20:17:32,891 INFO impl.YarnClientImpl: Submitted application application_1670098633286_0001\n",
      "2022-12-03 20:17:33,004 INFO mapreduce.Job: The url to track the job: http://b2fddb901fd0:8088/proxy/application_1670098633286_0001/\n",
      "2022-12-03 20:17:33,006 INFO mapreduce.Job: Running job: job_1670098633286_0001\n",
      "2022-12-03 20:17:44,369 INFO mapreduce.Job: Job job_1670098633286_0001 running in uber mode : false\n",
      "2022-12-03 20:17:44,371 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-12-03 20:17:59,724 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-12-03 20:18:00,739 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-12-03 20:18:06,825 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-12-03 20:18:07,843 INFO mapreduce.Job: Job job_1670098633286_0001 completed successfully\n",
      "2022-12-03 20:18:07,983 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=22049\n",
      "\t\tFILE: Number of bytes written=1155447\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=556329\n",
      "\t\tHDFS: Number of bytes written=54\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=41003\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4578\n",
      "\t\tTotal time spent by all map tasks (ms)=41003\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4578\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=41003\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4578\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=41987072\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4687872\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=14980\n",
      "\t\tMap output records=2557\n",
      "\t\tMap output bytes=16929\n",
      "\t\tMap output materialized bytes=22061\n",
      "\t\tInput split bytes=285\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=123\n",
      "\t\tReduce shuffle bytes=22061\n",
      "\t\tReduce input records=2557\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=5114\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=372\n",
      "\t\tCPU time spent (ms)=4530\n",
      "\t\tPhysical memory (bytes) snapshot=1117036544\n",
      "\t\tVirtual memory (bytes) snapshot=11116306432\n",
      "\t\tTotal committed heap usage (bytes)=1065353216\n",
      "\t\tPeak Map Physical memory (bytes)=321499136\n",
      "\t\tPeak Map Virtual memory (bytes)=2780741632\n",
      "\t\tPeak Reduce Physical memory (bytes)=210157568\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2784055296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=556044\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=54\n",
      "2022-12-03 20:18:07,983 INFO streaming.StreamJob: Output directory: /word_count/python_output1\n"
     ]
    }
   ],
   "source": [
    "#Running MapReduce programs\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.3.jar \\\n",
    "  -input /word_count/*.txt \\\n",
    "  -output /word_count/python_output1 \\\n",
    "  -mapper \"python /content/mapper1.py\" \\\n",
    "  -reducer \"python /content/reducer1.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6pLpDvj0PEn",
    "outputId": "47153893-4131-439f-ed18-9a7530181031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2022-12-03 20:18 /word_count/python_output1/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         54 2022-12-03 20:18 /word_count/python_output1/part-00000\n"
     ]
    }
   ],
   "source": [
    "# Checking out our new python_output directory:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/python_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqxwcP2B1Roz",
    "outputId": "616c1628-8f30-4366-ded0-de9037e1c267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757759 words in the corpus begin with the letter Y/y\t\n"
     ]
    }
   ],
   "source": [
    "# part-00000 contains the ouput this time:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/python_output1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFk2IiYGdiRh"
   },
   "source": [
    "### 2. What is the total number of unique words in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7jutSIOdrYx",
    "outputId": "4649c6ed-dba9-4d65-a441-9d34d6807ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper2.py\n"
     ]
    }
   ],
   "source": [
    "# Writing our mapper.py script:\n",
    "# The executable will read our input and transform them into key-value \n",
    "# pairs which are then passed to our reducer script. \n",
    "# Reference: https://github.com/LMAPcoder/Hadoop-on-Colab/blob/main/Hadoop_on_Colab.ipynb\n",
    "\n",
    "%%writefile mapper2.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "  \n",
    "for line in sys.stdin:\n",
    "  line = line.strip()  # removes whitespace either side of our line\n",
    "  words = line.split()  # splitting our line into a list of words\n",
    "    \n",
    "  for word in words:\n",
    "    print('%s\\t%s' % (word, 1))  # writing our results to STDOUT (this is the input for reducer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "048xKp28d3BH",
    "outputId": "21c75768-6d35-4a80-b399-63e897a8f316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer2.py\n"
     ]
    }
   ],
   "source": [
    "# Writing our reducer.py script: \n",
    "# This executable reads all intermediate key-value pairs generated by our mapper.py\n",
    "# And aggregates these into our final output result\n",
    "# Reference: https://github.com/LMAPcoder/Hadoop-on-Colab/blob/main/Hadoop_on_Colab.ipynb\n",
    "\n",
    "%%writefile reducer2.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "  \n",
    "import sys\n",
    "from operator import itemgetter\n",
    "  \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "unique_counter=0  \n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  word, count = line.split('\\t', 1)  # splitting the data on the basis of tab (see mapper.py)\n",
    "  \n",
    "  try:\n",
    "    count = int(count)  # convert count (currently a string) to int\n",
    "  except ValueError:\n",
    "    continue  # silently ignore line if count is not a number\n",
    "  \n",
    "  # this IF-switch only works because Hadoop sorts map output\n",
    "  # by key (here: word) before it is passed to the reducer\n",
    "  if current_word == word:\n",
    "    current_count += count\n",
    "  else:\n",
    "    if current_word: # to avoid None values\n",
    "      unique_counter+=1\n",
    "    current_count = count\n",
    "    current_word = word\n",
    "  \n",
    "print(f\"total number of unique words in the corpus is {unique_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nEsH-fRxd3BI"
   },
   "outputs": [],
   "source": [
    "# Giving these new files permissions:\n",
    "!chmod u+x /content/mapper2.py /content/reducer2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "EIoobBodd3BI",
    "outputId": "7de4759e-c656-4694-dd0a-2ea58facd5b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar16131529443164896695/] [] /tmp/streamjob14515511339128030120.jar tmpDir=null\n",
      "2022-12-03 20:18:17,975 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2022-12-03 20:18:18,283 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2022-12-03 20:18:18,656 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1670098633286_0002\n",
      "2022-12-03 20:18:19,066 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2022-12-03 20:18:19,588 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2022-12-03 20:18:20,379 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1670098633286_0002\n",
      "2022-12-03 20:18:20,380 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-12-03 20:18:20,596 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-12-03 20:18:20,596 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-12-03 20:18:20,684 INFO impl.YarnClientImpl: Submitted application application_1670098633286_0002\n",
      "2022-12-03 20:18:20,730 INFO mapreduce.Job: The url to track the job: http://b2fddb901fd0:8088/proxy/application_1670098633286_0002/\n",
      "2022-12-03 20:18:20,731 INFO mapreduce.Job: Running job: job_1670098633286_0002\n",
      "2022-12-03 20:18:29,998 INFO mapreduce.Job: Job job_1670098633286_0002 running in uber mode : false\n",
      "2022-12-03 20:18:30,000 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-12-03 20:18:45,307 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-12-03 20:18:46,315 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-12-03 20:18:52,394 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-12-03 20:18:53,414 INFO mapreduce.Job: Job job_1670098633286_0002 completed successfully\n",
      "2022-12-03 20:18:53,549 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=894717\n",
      "\t\tFILE: Number of bytes written=2900787\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=556329\n",
      "\t\tHDFS: Number of bytes written=53\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=40387\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4866\n",
      "\t\tTotal time spent by all map tasks (ms)=40387\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4866\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=40387\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4866\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=41356288\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4982784\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=14980\n",
      "\t\tMap output records=94075\n",
      "\t\tMap output bytes=706561\n",
      "\t\tMap output materialized bytes=894729\n",
      "\t\tInput split bytes=285\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=17745\n",
      "\t\tReduce shuffle bytes=894729\n",
      "\t\tReduce input records=94075\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=188150\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=366\n",
      "\t\tCPU time spent (ms)=6000\n",
      "\t\tPhysical memory (bytes) snapshot=1041580032\n",
      "\t\tVirtual memory (bytes) snapshot=11098386432\n",
      "\t\tTotal committed heap usage (bytes)=929038336\n",
      "\t\tPeak Map Physical memory (bytes)=289632256\n",
      "\t\tPeak Map Virtual memory (bytes)=2775392256\n",
      "\t\tPeak Reduce Physical memory (bytes)=195256320\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2779414528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=556044\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=53\n",
      "2022-12-03 20:18:53,549 INFO streaming.StreamJob: Output directory: /word_count/python_output2\n"
     ]
    }
   ],
   "source": [
    "#Running MapReduce programs\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.3.jar \\\n",
    "  -input /word_count/*.txt \\\n",
    "  -output /word_count/python_output2 \\\n",
    "  -mapper \"python /content/mapper2.py\" \\\n",
    "  -reducer \"python /content/reducer2.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jdXsgaPd3BI",
    "outputId": "6faa9c76-3a97-4266-ca96-53dea61e26ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2022-12-03 20:18 /word_count/python_output2/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         53 2022-12-03 20:18 /word_count/python_output2/part-00000\n"
     ]
    }
   ],
   "source": [
    "# Checking out our new python_output directory:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/python_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3ewqaQ0d3BI",
    "outputId": "bfd7948f-7575-4ad1-f04f-d33830514b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words in the corpus is 17744\t\n"
     ]
    }
   ],
   "source": [
    "# part-00000 contains the ouput this time:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/python_output2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCq4Bh9kksMS"
   },
   "source": [
    "### 3. Which word occurs most frequently in the corpus? How many times does it occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZUw8mbsKr_z",
    "outputId": "d0a578c9-3200-42be-ee4b-f9adf427239e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper3.py\n"
     ]
    }
   ],
   "source": [
    "# Writing our mapper.py script:\n",
    "# The executable will read our input and transform them into key-value \n",
    "# pairs which are then passed to our reducer script. \n",
    "# Reference: https://github.com/LMAPcoder/Hadoop-on-Colab/blob/main/Hadoop_on_Colab.ipynb\n",
    "\n",
    "%%writefile mapper3.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "  \n",
    "for line in sys.stdin:\n",
    "  line = line.strip()  # removes whitespace either side of our line\n",
    "  words = line.split()  # splitting our line into a list of words\n",
    "    \n",
    "  for word in words:\n",
    "    print('%s\\t%s' % (word, 1))  # writing our results to STDOUT (this is the input for reducer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6J7EVd0Kr_z",
    "outputId": "8790f5c9-d706-4053-c5cb-106f94b21998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.py\n"
     ]
    }
   ],
   "source": [
    "# Writing our reducer.py script: \n",
    "# This executable reads all intermediate key-value pairs generated by our mapper.py\n",
    "# And aggregates these into our final output result\n",
    "# Reference: https://github.com/LMAPcoder/Hadoop-on-Colab/blob/main/Hadoop_on_Colab.ipynb\n",
    "\n",
    "%%writefile reducer3.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "  \n",
    "import sys\n",
    "from operator import itemgetter\n",
    "  \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "words_counts={}  \n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  word, count = line.split('\\t', 1)  # splitting the data on the basis of tab (see mapper.py)\n",
    "  \n",
    "  try:\n",
    "    count = int(count)  # convert count (currently a string) to int\n",
    "  except ValueError:\n",
    "    continue  # silently ignore line if count is not a number\n",
    "  \n",
    "  # this IF-switch only works because Hadoop sorts map output\n",
    "  # by key (here: word) before it is passed to the reducer\n",
    "  if current_word == word:\n",
    "    current_count += count\n",
    "  else:\n",
    "    if current_word: # to avoid None values\n",
    "      words_counts[current_word]= current_count\n",
    "    current_count = count\n",
    "    current_word = word\n",
    "  \n",
    "m_freq_item=max(words_counts.items(), key=itemgetter(1))\n",
    "m_freq_word=m_freq_item[0]\n",
    "m_freq_word_occur=m_freq_item[1]\n",
    "print(f\"most frequently in the corpus is {m_freq_word}, it occured {m_freq_word_occur} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "lKWAwEHIKr_0"
   },
   "outputs": [],
   "source": [
    "# Giving these new files permissions:\n",
    "!chmod u+x /content/mapper3.py /content/reducer3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itGv0ZaBnsH8",
    "outputId": "c00f5e10-e768-486f-b4b8-b2a3b1029b1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most frequently in the corpus is I, it occured 830 times.\n"
     ]
    }
   ],
   "source": [
    "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
    "!cat pg2267.txt | python mapper3.py | sort -k1,1 | python reducer3.py | head -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "jWlK4Pr_Kr_0",
    "outputId": "d5f8a73c-1ec6-4977-c370-79409dee87a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar14679185848601467052/] [] /tmp/streamjob8689544642227814113.jar tmpDir=null\n",
      "2022-12-03 20:20:32,069 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2022-12-03 20:20:32,321 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2022-12-03 20:20:32,749 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1670098633286_0003\n",
      "2022-12-03 20:20:33,160 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2022-12-03 20:20:33,287 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2022-12-03 20:20:33,638 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1670098633286_0003\n",
      "2022-12-03 20:20:33,638 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-12-03 20:20:33,864 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-12-03 20:20:33,865 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-12-03 20:20:34,027 INFO impl.YarnClientImpl: Submitted application application_1670098633286_0003\n",
      "2022-12-03 20:20:34,139 INFO mapreduce.Job: The url to track the job: http://b2fddb901fd0:8088/proxy/application_1670098633286_0003/\n",
      "2022-12-03 20:20:34,145 INFO mapreduce.Job: Running job: job_1670098633286_0003\n",
      "2022-12-03 20:20:43,343 INFO mapreduce.Job: Job job_1670098633286_0003 running in uber mode : false\n",
      "2022-12-03 20:20:43,344 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-12-03 20:20:58,624 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2022-12-03 20:20:59,640 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-12-03 20:21:05,713 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-12-03 20:21:06,733 INFO mapreduce.Job: Job job_1670098633286_0003 completed successfully\n",
      "2022-12-03 20:21:06,860 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=894717\n",
      "\t\tFILE: Number of bytes written=2900783\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=556329\n",
      "\t\tHDFS: Number of bytes written=62\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=41081\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4720\n",
      "\t\tTotal time spent by all map tasks (ms)=41081\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4720\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=41081\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4720\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=42066944\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4833280\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=14980\n",
      "\t\tMap output records=94075\n",
      "\t\tMap output bytes=706561\n",
      "\t\tMap output materialized bytes=894729\n",
      "\t\tInput split bytes=285\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=17745\n",
      "\t\tReduce shuffle bytes=894729\n",
      "\t\tReduce input records=94075\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=188150\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=377\n",
      "\t\tCPU time spent (ms)=6340\n",
      "\t\tPhysical memory (bytes) snapshot=1055080448\n",
      "\t\tVirtual memory (bytes) snapshot=11109158912\n",
      "\t\tTotal committed heap usage (bytes)=864026624\n",
      "\t\tPeak Map Physical memory (bytes)=286515200\n",
      "\t\tPeak Map Virtual memory (bytes)=2777092096\n",
      "\t\tPeak Reduce Physical memory (bytes)=213135360\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2781609984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=556044\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=62\n",
      "2022-12-03 20:21:06,860 INFO streaming.StreamJob: Output directory: /word_count/python_output3\n"
     ]
    }
   ],
   "source": [
    "#Running MapReduce programs\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.3.jar \\\n",
    "  -input /word_count/*.txt \\\n",
    "  -output /word_count/python_output3 \\\n",
    "  -mapper \"python /content/mapper3.py\" \\\n",
    "  -reducer \"python /content/reducer3.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-iXZEisKr_0",
    "outputId": "5caa439b-35e5-4600-951b-53585274c3a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2022-12-03 20:21 /word_count/python_output3/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         62 2022-12-03 20:21 /word_count/python_output3/part-00000\n"
     ]
    }
   ],
   "source": [
    "# Checking out our new python_output directory:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/python_output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EfEzd_9Kr_0",
    "outputId": "d74e25c1-cc26-4d49-bb3b-b8dc1291a2c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most frequently in the corpus is the, it occured 2576 times.\t\n"
     ]
    }
   ],
   "source": [
    "# part-00000 contains the ouput this time:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/python_output3/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAShE6RbKyZ4"
   },
   "source": [
    "### 4. For that most commonly occurring word, what word most frequently follows it in a line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPsvlHSzLDkj",
    "outputId": "2e98dfeb-3160-47a8-e4dd-19c51741c422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper4.py\n"
     ]
    }
   ],
   "source": [
    "# Writing our mapper.py script:\n",
    "# The executable will read our input and transform them into key-value \n",
    "# pairs which are then passed to our reducer script. \n",
    "# Reference: https://github.com/LMAPcoder/Hadoop-on-Colab/blob/main/Hadoop_on_Colab.ipynb\n",
    "\n",
    "%%writefile mapper4.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "  \n",
    "for line in sys.stdin:\n",
    "  line = line.strip()  # removes whitespace either side of our line\n",
    "  words = line.split()  # splitting our line into a list of words\n",
    "  p_word=None    \n",
    "  for word in words:\n",
    "    if p_word==\"the\":\n",
    "      print('%s\\t%s' % (word, 1))  # writing our results to STDOUT (this is the input for reducer.py)\n",
    "    p_word=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYlQSyGGLDkk",
    "outputId": "78e9e9ce-8698-4a47-bb6c-8d327299e5c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer4.py\n"
     ]
    }
   ],
   "source": [
    "# Writing our reducer.py script: \n",
    "# This executable reads all intermediate key-value pairs generated by our mapper.py\n",
    "# And aggregates these into our final output result\n",
    "# Reference: https://github.com/LMAPcoder/Hadoop-on-Colab/blob/main/Hadoop_on_Colab.ipynb\n",
    "\n",
    "%%writefile reducer4.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "  \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "words_counts={}  \n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  word, count = line.split('\\t', 1)  # splitting the data on the basis of tab (see mapper.py)\n",
    "  \n",
    "  try:\n",
    "    count = int(count)  # convert count (currently a string) to int\n",
    "  except ValueError:\n",
    "    continue  # silently ignore line if count is not a number\n",
    "  \n",
    "  # this IF-switch only works because Hadoop sorts map output\n",
    "  # by key (here: word) before it is passed to the reducer\n",
    "  if current_word == word:\n",
    "    current_count += count\n",
    "  else:\n",
    "    if current_word: # to avoid None values\n",
    "      words_counts[current_word]= current_count\n",
    "    current_count = count\n",
    "    current_word = word\n",
    "  \n",
    "m_freq_item=max(words_counts.items(), key=itemgetter(1))\n",
    "m_freq_word=m_freq_item[0]\n",
    "m_freq_word_occur=m_freq_item[1]\n",
    "print(f\"most commonly occurring word the followed most frequently by {m_freq_word}, {m_freq_word} followed the {m_freq_word_occur} times in a line.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Ay_TWx3DLDkk"
   },
   "outputs": [],
   "source": [
    "# Giving these new files permissions:\n",
    "!chmod u+x /content/mapper4.py /content/reducer4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chS-1WiLu4zk",
    "outputId": "1c2dc71c-81d7-43f3-ebd3-ffbe34673e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most commonly occurring word the followed most frequently by Moore, Moore followed the 21 times in a line.\n"
     ]
    }
   ],
   "source": [
    "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
    "!cat pg2267.txt | python mapper4.py | sort -k1,1 | python reducer4.py | head -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "WvHsS-EsLDkk",
    "outputId": "b672ba2f-38b0-4ed7-bd2e-7fc0bb1d23db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar18214177651360519311/] [] /tmp/streamjob5948849691694841749.jar tmpDir=null\n",
      "2022-12-03 20:50:11,875 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2022-12-03 20:50:12,109 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
      "2022-12-03 20:50:12,452 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1670100213453_0001\n",
      "2022-12-03 20:50:13,207 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2022-12-03 20:50:13,340 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2022-12-03 20:50:13,696 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1670100213453_0001\n",
      "2022-12-03 20:50:13,697 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-12-03 20:50:13,909 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-12-03 20:50:13,909 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-12-03 20:50:14,361 INFO impl.YarnClientImpl: Submitted application application_1670100213453_0001\n",
      "2022-12-03 20:50:14,419 INFO mapreduce.Job: The url to track the job: http://6155e2551bb9:8088/proxy/application_1670100213453_0001/\n",
      "2022-12-03 20:50:14,421 INFO mapreduce.Job: Running job: job_1670100213453_0001\n",
      "2022-12-03 20:50:23,776 INFO mapreduce.Job: Job job_1670100213453_0001 running in uber mode : false\n",
      "2022-12-03 20:50:23,777 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-12-03 20:50:36,995 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-12-03 20:50:44,078 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-12-03 20:50:45,096 INFO mapreduce.Job: Job job_1670100213453_0001 completed successfully\n",
      "2022-12-03 20:50:45,222 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=27007\n",
      "\t\tFILE: Number of bytes written=1165363\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=556329\n",
      "\t\tHDFS: Number of bytes written=112\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=32347\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4295\n",
      "\t\tTotal time spent by all map tasks (ms)=32347\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4295\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=32347\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4295\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=33123328\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4398080\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=14980\n",
      "\t\tMap output records=2424\n",
      "\t\tMap output bytes=22153\n",
      "\t\tMap output materialized bytes=27019\n",
      "\t\tInput split bytes=285\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1507\n",
      "\t\tReduce shuffle bytes=27019\n",
      "\t\tReduce input records=2424\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=4848\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=287\n",
      "\t\tCPU time spent (ms)=4150\n",
      "\t\tPhysical memory (bytes) snapshot=1049763840\n",
      "\t\tVirtual memory (bytes) snapshot=11101261824\n",
      "\t\tTotal committed heap usage (bytes)=907018240\n",
      "\t\tPeak Map Physical memory (bytes)=311136256\n",
      "\t\tPeak Map Virtual memory (bytes)=2776379392\n",
      "\t\tPeak Reduce Physical memory (bytes)=186036224\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2779095040\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=556044\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=112\n",
      "2022-12-03 20:50:45,223 INFO streaming.StreamJob: Output directory: /word_count/python_output4\n"
     ]
    }
   ],
   "source": [
    "#Running MapReduce programs\n",
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.3.jar \\\n",
    "  -input /word_count/*.txt \\\n",
    "  -output /word_count/python_output4 \\\n",
    "  -mapper \"python /content/mapper4.py\" \\\n",
    "  -reducer \"python /content/reducer4.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be6TtZH6LDkk",
    "outputId": "1e7d6f9c-3035-46a2-b5bd-dcd943efebf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2022-12-03 20:50 /word_count/python_output4/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup        112 2022-12-03 20:50 /word_count/python_output4/part-00000\n"
     ]
    }
   ],
   "source": [
    "# Checking out our new python_output directory:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/python_output4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ISLaCcVLDkk",
    "outputId": "804ea158-a6ca-467d-878e-fe770d61f3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most commonly occurring word the followed most frequently by Project, Project followed the 58 times in a line.\t\n"
     ]
    }
   ],
   "source": [
    "# part-00000 contains the ouput this time:\n",
    "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/python_output4/part-00000"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
